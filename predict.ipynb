{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b4e4614-6333-4564-b23b-d59ca6a4a8d2",
   "metadata": {},
   "source": [
    "### 오디오 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9320139f-722c-4682-93f8-5e6da877e9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from predict_stock_outlook import extract_video_audio\n",
    "\n",
    "df = pd.read_csv(\"data_kr/video/자료 수집 최종본.csv\")\n",
    "\n",
    "for row in df.itertuples():\n",
    "    if pd.isna(row.url) or row.url == '' or row.category != \"video\":\n",
    "        continue\n",
    "\n",
    "    code = str(row.code).zfill(6)\n",
    "    audio_dir = f'data_kr/video/audio/{row.sector}/{code}/'\n",
    "    text_dir = f'data_kr/video/text/{row.sector}/{code}/'\n",
    "    os.makedirs(audio_dir, exist_ok=True)\n",
    "\n",
    "    if audio_dir + f'{row.year}-{row.quarter}' != 'data_kr/video/audio/산업재/003490/2016-Q2':\n",
    "        continue\n",
    "    \n",
    "    if extract_video_audio(\"link\", row.url, audio_dir + f'{row.year}-{row.quarter}'):\n",
    "        with open('data_kr/video/log.txt', \"a\", encoding=\"utf-8\") as log_file:\n",
    "            timestamp = datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S]\")\n",
    "            log_file.write(f\"{timestamp} audio download completed: {audio_dir + f'{row.year}-{row.quarter}'}\\n\")\n",
    "    else:\n",
    "        with open('data_kr/video/log.txt', \"a\", encoding=\"utf-8\") as log_file:\n",
    "            timestamp = datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S]\")\n",
    "            log_file.write(f\"{timestamp} audio download error: {audio_dir + f'{row.year}-{row.quarter}'}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1dc206-96f3-4bf2-9b25-09b38ab4529c",
   "metadata": {},
   "source": [
    "### 텍스트로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4448eb2f-9bec-4513-8714-ad550cccffdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from predict_stock_outlook import audio2text\n",
    "\n",
    "df = pd.read_csv(\"data_kr/video/자료 수집 최종본.csv\")\n",
    "\n",
    "for row in df.itertuples():\n",
    "    if pd.isna(row.url) or row.url == '' or row.category != \"video\":\n",
    "        continue\n",
    "\n",
    "    code = str(row.code).zfill(6)\n",
    "    audio_dir = f'data_kr/video/audio/{row.sector}/{code}/'\n",
    "    text_dir = f'data_kr/video/text/{row.sector}/{code}/'\n",
    "    os.makedirs(text_dir, exist_ok=True)\n",
    "    \n",
    "    if audio_dir + f'{row.year}-{row.quarter}' != 'data_kr/video/audio/산업재/003490/2016-Q2':\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        text = audio2text(audio_dir + f'{row.year}-{row.quarter}')\n",
    "        with open(text_dir + f'{row.year}-{row.quarter}.txt', \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    "        with open('data_kr/video/log.txt', \"a\", encoding=\"utf-8\") as log_file:\n",
    "            timestamp = datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S]\")\n",
    "            log_file.write(f\"{timestamp} whisper completed: {text_dir + f'{row.year}-{row.quarter}'}\\n\")\n",
    "    except Exception as e:\n",
    "        with open('data_kr/video/log.txt', \"a\", encoding=\"utf-8\") as log_file:\n",
    "            timestamp = datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S]\")\n",
    "            log_file.write(f\"{timestamp} whisper error: {text_dir + f'{row.year}-{row.quarter}'}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84c7ccf-1dea-4696-aa6d-d36f5e4dee1b",
   "metadata": {},
   "source": [
    "### 텍스트 token 수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3802c4-52df-4dde-a9f5-d7b9eccfcb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data_kr/video/자료 수집 최종본.csv\")\n",
    "\n",
    "import tiktoken\n",
    "# 예: GPT-4용 인코더 불러오기\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "total_tokens = 0\n",
    "\n",
    "for row in tqdm(df.itertuples(), total=len(df), desc=\"checking tokens\"):\n",
    "\tif pd.isna(row.url) or row.url == '':\n",
    "\t\tcontinue\n",
    "\n",
    "\tcode = str(row.code).zfill(6)\n",
    "\ttext_dir = f'data_kr/video/text/{row.sector}/{code}/'\n",
    "\tos.makedirs(text_dir, exist_ok=True)\n",
    "    \n",
    "\ttry:\n",
    "\t\tfilename = f'{row.year}-{row.quarter}-{str(row.month).zfill(2)}-{row.week}.txt'\n",
    "\t\twith open(text_dir + filename, \"r\", encoding=\"utf-8\") as file:\n",
    "\t\t\ttext = file.read()\n",
    "\t\t\tsystem_prompt = \"\"\"\n",
    "\t\t너는 경제 전문 뉴스 분석 AI야. 사용자가 지정한 종목(회사명)과 직접적으로 관련된 정보만 선택해 핵심적으로 요약해.\n",
    "\t\t사실 기반으로 요약하고, 감성이나 추론이 필요한 경우에는 중립적으로 표현해.\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\t\tuser_prompt = f\"\"\"\n",
    "\t\t다음은 경제 뉴스 기사입니다.\n",
    "\n",
    "\t\t이 기사에서 **한국 상장 기업 \"{code}\"**과 관련된 내용만 골라 요약해 주세요.\n",
    "\n",
    "\t\t요약 기준:\n",
    "\t\t- \"{code}\"이 언급된 부분 중심\n",
    "\t\t- 관련 사업, 실적, 주가, 시장 반응, 경쟁사와의 연관성\n",
    "\t\t- 정부 정책, 산업 트렌드 등 외부 요인 중 관련 있는 부분\n",
    "\t\t- 부정적/긍정적 논조도 간단히 언급 (있는 경우)\n",
    "\n",
    "\t\t형식은 간결한 문장 또는 Bullet Point 형식으로 작성해 주세요.\n",
    "\n",
    "\t\t기사 전문:\n",
    "\t\t{text}\n",
    "\t\t\"\"\"\n",
    "\t\t\ttotal_prompt = system_prompt + user_prompt\n",
    "\t\t\ttokens = encoding.encode(total_prompt)\n",
    "\t\t\ttotal_tokens += len(tokens)\n",
    "\t\twith open('data_kr/video/num_token.txt', \"a\", encoding=\"utf-8\") as log_file:\n",
    "\t\t\ttimestamp = datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S]\")\n",
    "\t\t\tlog_file.write(f\"{text_dir}{filename} len: {len(text)}, token: {len(tokens)}\\n\")\n",
    "\texcept Exception as e:\n",
    "\t\twith open('data_kr/video/num_token.txt', \"a\", encoding=\"utf-8\") as log_file:\n",
    "\t\t\ttimestamp = datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S]\")\n",
    "\t\t\tlog_file.write(f\"{text_dir}{filename} error: \" + e + \"\\n\")\n",
    "\n",
    "with open('data_kr/video/num_token.txt', \"a\", encoding=\"utf-8\") as log_file:\n",
    "\tlog_file.write(f\"total tokens: {total_tokens}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270d9d4c-6adf-4f55-b874-ecfb5d6b58a7",
   "metadata": {},
   "source": [
    "### LLM으로 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87e4c23-ba5a-4768-8ded-6e69e51f8783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from predict_stock_outlook import summarize_text\n",
    "\n",
    "df = pd.read_csv(\"data_kr/video/자료 수집 최종본.csv\")\n",
    "\n",
    "for row in tqdm(df.itertuples(), total=len(df), desc=\"LLM summarizing\"):\n",
    "    if pd.isna(row.url) or row.url == '':\n",
    "        continue\n",
    "\n",
    "    code = str(row.code).zfill(6)\n",
    "    name = row.name\n",
    "    text_dir = f'data_kr/video/text/{row.sector}/{code}/'\n",
    "    summary_dir = f'preprocessed_data/llm/summary/{row.sector}/{code}/'\n",
    "    os.makedirs(summary_dir, exist_ok=True)\n",
    "        \n",
    "    try:\n",
    "        filename = f'{row.year}-{row.quarter}-{str(row.month).zfill(2)}-{row.week}.txt'\n",
    "        stock = f'{name}({code})'\n",
    "        with open(text_dir + filename, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "        summary = summarize_text(text, stock)\n",
    "        with open(summary_dir + filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(summary)\n",
    "        with open('preprocessed_data/llm/summary/log.txt', \"a\", encoding=\"utf-8\") as log_file:\n",
    "            timestamp = datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S]\")\n",
    "            log_file.write(f\"{timestamp} summary completed: {summary_dir + filename}\\n\")\n",
    "    except Exception as e:\n",
    "        with open('preprocessed_data/llm/summary/log.txt', \"a\", encoding=\"utf-8\") as log_file:\n",
    "            timestamp = datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S]\")\n",
    "            log_file.write(f\"{timestamp} summary error: {summary_dir + filename}\\t error: {e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f08c86-f82f-476a-b5a9-a9c47e242a4d",
   "metadata": {},
   "source": [
    "### LLM으로 기사 자막요약을 통해 등락 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea05f01-d708-4647-bce7-2cdbad1dcf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from predict_stock_outlook import predict_market_from_summary\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data_kr/video/뉴스 기사 수집본.csv', encoding='utf-8')\n",
    "\n",
    "for code in df[\"code\"].unique():\n",
    "    df_ = df[df[\"code\"] == code].reset_index(drop=True)\n",
    "\n",
    "    for row in tqdm(df_.itertuples(), total=len(df_), desc=f\"{code}LLM predicting\"):\n",
    "        if pd.isna(row.url) or row.url == '':\n",
    "            continue\n",
    "        \n",
    "        code = str(row.code).zfill(6)\t\n",
    "        name = row.name\n",
    "        article_dir = f'data_kr/video/text/{row.sector}/{code}/'\n",
    "        predict_dir = f'preprocessed_data/llm/predict_text/{row.sector}/{code}/'\n",
    "        os.makedirs(predict_dir, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            filename = f'{row.year}-{row.quarter}-{str(row.month).zfill(2)}-{row.week}.txt'\n",
    "            stock = f'{name}({code})'\n",
    "            with open(f'{article_dir}{filename}', \"r\", encoding=\"utf-8\") as file:\n",
    "                article = file.read()\n",
    "            data = predict_market_from_summary(article, f'{name}({code})')\n",
    "            \n",
    "            with open(f'{predict_dir}{filename}', \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(data)\n",
    "            \n",
    "            with open('preprocessed_data/llm/predict_text/log.txt', \"a\", encoding=\"utf-8\") as log_file:\n",
    "                timestamp = datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S]\")\n",
    "                log_file.write(f\"{timestamp} predict completed: {predict_dir}{filename}\\n\")\n",
    "        except Exception as e:\n",
    "            with open('preprocessed_data/llm/predict_text/log.txt', \"a\", encoding=\"utf-8\") as log_file:\n",
    "                timestamp = datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S]\")\n",
    "                log_file.write(f\"{timestamp} predict error: {predict_dir}{filename}\\n\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf24048-7d41-471e-855b-d400cca54424",
   "metadata": {},
   "source": [
    "### LLM으로 영상 자막요약을 통해 등락 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b5f5a3-0b98-4438-8975-64355fd06d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from predict_stock_outlook import predict_market_from_summary\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data_kr/video/뉴스 영상 수집본.csv', encoding='utf-8')\n",
    "\n",
    "for code in df[\"code\"].unique():\n",
    "    df_ = df[df[\"code\"] == code].reset_index(drop=True)\n",
    "\n",
    "    for row in tqdm(df_.itertuples(), total=len(df_), desc=f\"{code}LLM predicting\"):\n",
    "        if pd.isna(row.url) or row.url == '':\n",
    "            continue\n",
    "        \n",
    "        code = str(row.code).zfill(6)\t\n",
    "        name = row.name\n",
    "        script_dir = f'data_kr/video/script/{row.sector}/{code}/'\n",
    "        predict_dir = f'preprocessed_data/llm/predict_video/{row.sector}/{code}/'\n",
    "        os.makedirs(predict_dir, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            filename = f'{row.year}-{row.quarter}-{str(row.month).zfill(2)}-{row.week}.txt'\n",
    "            stock = f'{name}({code})'\n",
    "            with open(f'{script_dir}{filename}', \"r\", encoding=\"utf-8\") as file:\n",
    "                script = file.read()\n",
    "            prediction = predict_market_from_summary(script, f'{name}({code})')\n",
    "            \n",
    "            with open(f'{predict_dir}{filename}', \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(prediction)\n",
    "            \n",
    "            with open('preprocessed_data/llm/predict_video/log.txt', \"a\", encoding=\"utf-8\") as log_file:\n",
    "                timestamp = datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S]\")\n",
    "                log_file.write(f\"{timestamp} predict completed: {predict_dir}{filename}\\n\")\n",
    "        except Exception as e:\n",
    "            with open('preprocessed_data/llm/predict_video/log.txt', \"a\", encoding=\"utf-8\") as log_file:\n",
    "                timestamp = datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S]\")\n",
    "                log_file.write(f\"{timestamp} predict error: {predict_dir}{filename}\\n\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9684e553-90a5-401e-bbd4-ff1bde25fb2b",
   "metadata": {},
   "source": [
    "### LLM으로 영상자막요약 + 기사자막요약을 통해 등락 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05e394fd-4a0c-4b3e-9976-76fcd09e4996",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kk200\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\kk200\\S3CE\\predict_stock_outlook.py:33: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  gpt_model = ChatOpenAI(temperature=0, model=\"gpt-4o\", openai_api_key=OPENAI_API_KEY)\n",
      "C:\\Users\\kk200\\S3CE\\predict_stock_outlook.py:419: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = gpt_model([\n",
      "120LLM predicting: 100%|█████████████████████████████████████████████████████████████| 204/204 [05:53<00:00,  1.73s/it]\n",
      "150LLM predicting: 100%|█████████████████████████████████████████████████████████████| 204/204 [04:41<00:00,  1.38s/it]\n",
      "660LLM predicting: 100%|█████████████████████████████████████████████████████████████| 204/204 [04:38<00:00,  1.37s/it]\n",
      "1120LLM predicting: 100%|████████████████████████████████████████████████████████████| 204/204 [04:10<00:00,  1.23s/it]\n",
      "3490LLM predicting: 100%|████████████████████████████████████████████████████████████| 204/204 [06:12<00:00,  1.82s/it]\n",
      "3550LLM predicting: 100%|████████████████████████████████████████████████████████████| 204/204 [05:19<00:00,  1.57s/it]\n",
      "3570LLM predicting: 100%|████████████████████████████████████████████████████████████| 204/204 [01:19<00:00,  2.56it/s]\n",
      "4710LLM predicting: 100%|████████████████████████████████████████████████████████████| 204/204 [00:33<00:00,  6.12it/s]\n",
      "5930LLM predicting: 100%|████████████████████████████████████████████████████████████| 204/204 [04:36<00:00,  1.35s/it]\n",
      "6260LLM predicting: 100%|████████████████████████████████████████████████████████████| 204/204 [03:02<00:00,  1.12it/s]\n",
      "6400LLM predicting: 100%|████████████████████████████████████████████████████████████| 204/204 [05:56<00:00,  1.75s/it]\n",
      "8060LLM predicting: 100%|████████████████████████████████████████████████████████████| 204/204 [06:26<00:00,  1.90s/it]\n",
      "9150LLM predicting: 100%|████████████████████████████████████████████████████████████| 204/204 [05:27<00:00,  1.61s/it]\n",
      "10120LLM predicting: 100%|███████████████████████████████████████████████████████████| 204/204 [03:57<00:00,  1.16s/it]\n",
      "11070LLM predicting: 100%|███████████████████████████████████████████████████████████| 204/204 [05:34<00:00,  1.64s/it]\n",
      "11200LLM predicting: 100%|███████████████████████████████████████████████████████████| 204/204 [04:48<00:00,  1.41s/it]\n",
      "18260LLM predicting: 100%|███████████████████████████████████████████████████████████| 204/204 [04:43<00:00,  1.39s/it]\n",
      "20150LLM predicting: 100%|███████████████████████████████████████████████████████████| 204/204 [05:01<00:00,  1.48s/it]\n",
      "25540LLM predicting: 100%|███████████████████████████████████████████████████████████| 204/204 [00:34<00:00,  5.85it/s]\n",
      "29530LLM predicting: 100%|███████████████████████████████████████████████████████████| 204/204 [00:07<00:00, 27.99it/s]\n",
      "34220LLM predicting: 100%|███████████████████████████████████████████████████████████| 204/204 [04:02<00:00,  1.19s/it]\n",
      "34730LLM predicting: 100%|███████████████████████████████████████████████████████████| 204/204 [05:07<00:00,  1.51s/it]\n",
      "42700LLM predicting: 100%|███████████████████████████████████████████████████████████| 204/204 [04:15<00:00,  1.25s/it]\n",
      "47050LLM predicting: 100%|███████████████████████████████████████████████████████████| 204/204 [05:28<00:00,  1.61s/it]\n",
      "47810LLM predicting: 100%|███████████████████████████████████████████████████████████| 204/204 [04:49<00:00,  1.42s/it]\n",
      "51600LLM predicting: 100%|███████████████████████████████████████████████████████████| 204/204 [03:25<00:00,  1.01s/it]\n",
      "66570LLM predicting: 100%|███████████████████████████████████████████████████████████| 204/204 [04:24<00:00,  1.30s/it]\n",
      "86280LLM predicting: 100%|███████████████████████████████████████████████████████████| 204/204 [03:11<00:00,  1.06it/s]\n"
     ]
    }
   ],
   "source": [
    "from predict_stock_outlook import predict_market_from_mix\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "df_v = pd.read_csv('data_kr/video/뉴스 영상 수집본.csv', encoding='utf-8')\n",
    "df_v.rename(columns={'url':'v_url', 'upload_dt':'v_upload_dt'}, inplace=True)\n",
    "df_v = df_v[[\"year\",\"quarter\",\"month\",\"week\",\"code\",\"name\",\"sector\",\"after\",\"before\",\"v_url\",\"v_upload_dt\"]]\n",
    "\n",
    "df_a = pd.read_csv('data_kr/video/뉴스 기사 수집본.csv', encoding='utf-8')\n",
    "df_a.rename(columns={'url':'a_url', 'upload_dt':'a_upload_dt'}, inplace=True)\n",
    "df_a = df_a[[\"a_url\",\"a_upload_dt\"]]\n",
    "\n",
    "df = pd.concat([df_v, df_a], axis=1)\n",
    "\n",
    "for code in df[\"code\"].unique():\n",
    "    df_ = df[df[\"code\"] == code].reset_index(drop=True)\n",
    "\n",
    "    for row in tqdm(df_.itertuples(), total=len(df_), desc=f\"{code}LLM predicting\"):\n",
    "        if pd.isna(row.v_url) or pd.isna(row.a_url) :\n",
    "            continue\n",
    "        \n",
    "        code = str(row.code).zfill(6)\n",
    "        name = row.name\n",
    "        v_summary_dir = f'preprocessed_data/llm/summary_video/{row.sector}/{code}/'\n",
    "        t_summary_dir = f'preprocessed_data/llm/summary_text/{row.sector}/{code}/'\n",
    "        predict_dir = f'preprocessed_data/llm/predict_mix/{row.sector}/{code}/'\n",
    "        os.makedirs(predict_dir, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            filename = f'{row.year}-{row.quarter}-{str(row.month).zfill(2)}-{row.week}.txt'\n",
    "            stock = f'{name}({code})'\n",
    "            with open(f'{v_summary_dir}{filename}', \"r\", encoding=\"utf-8\") as file:\n",
    "                script = file.read()\n",
    "            with open(f'{t_summary_dir}{filename}', \"r\", encoding=\"utf-8\") as file:\n",
    "                article = file.read()\n",
    " \n",
    "            data = predict_market_from_mix(article, script, f'{name}({code})')\n",
    "            \n",
    "            with open(f'{predict_dir}{filename}', \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(data)\n",
    "            \n",
    "            with open('preprocessed_data/llm/predict_mix/log.txt', \"a\", encoding=\"utf-8\") as log_file:\n",
    "                timestamp = datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S]\")\n",
    "                log_file.write(f\"{timestamp} predict completed: {predict_dir}{filename}\\n\")\n",
    "        except Exception as e:\n",
    "            with open('preprocessed_data/llm/predict_mix/log.txt', \"a\", encoding=\"utf-8\") as log_file:\n",
    "                timestamp = datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S]\")\n",
    "                log_file.write(f\"{timestamp} predict error: {predict_dir}{filename}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3a9a1b-c4ca-473d-917e-86af4d5e011c",
   "metadata": {},
   "source": [
    "### 기사 예측 결과 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81c5e488-346e-4161-8b26-ee4ab9d49bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120LLM predicting: 100%|█████████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 4276.86it/s]\n",
      "150LLM predicting: 100%|█████████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 8417.08it/s]\n",
      "660LLM predicting: 100%|█████████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 8680.07it/s]\n",
      "1120LLM predicting: 100%|████████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 5602.92it/s]\n",
      "3490LLM predicting: 100%|████████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 6120.49it/s]\n",
      "3550LLM predicting: 100%|████████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 7768.92it/s]\n",
      "3570LLM predicting: 100%|███████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 11452.64it/s]\n",
      "4710LLM predicting: 100%|███████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 12311.69it/s]\n",
      "5930LLM predicting: 100%|████████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 8684.56it/s]\n",
      "6260LLM predicting: 100%|████████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 7507.11it/s]\n",
      "6400LLM predicting: 100%|████████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 5907.43it/s]\n",
      "8060LLM predicting: 100%|████████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 5621.95it/s]\n",
      "9150LLM predicting: 100%|████████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 5950.82it/s]\n",
      "10120LLM predicting: 100%|███████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 6353.64it/s]\n",
      "11070LLM predicting: 100%|███████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 5704.90it/s]\n",
      "11200LLM predicting: 100%|███████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 5686.17it/s]\n",
      "18260LLM predicting: 100%|███████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 7583.36it/s]\n",
      "20150LLM predicting: 100%|███████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 5720.46it/s]\n",
      "25540LLM predicting: 100%|██████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 10363.96it/s]\n",
      "29530LLM predicting: 100%|██████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 11600.14it/s]\n",
      "34220LLM predicting: 100%|███████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 6458.77it/s]\n",
      "34730LLM predicting: 100%|███████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 6370.10it/s]\n",
      "42700LLM predicting: 100%|███████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 6764.31it/s]\n",
      "47050LLM predicting: 100%|███████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 5498.50it/s]\n",
      "47810LLM predicting: 100%|███████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 6485.30it/s]\n",
      "51600LLM predicting: 100%|███████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 5655.17it/s]\n",
      "66570LLM predicting: 100%|███████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 5697.08it/s]\n",
      "86280LLM predicting: 100%|███████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 6334.87it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('./data_kr/video/뉴스 기사 수집본.csv', encoding='utf-8')\n",
    "\n",
    "for code in df[\"code\"].unique():\n",
    "    df_ = df[df[\"code\"] == code].reset_index(drop=True)\n",
    "    predict_list = []\n",
    "    reason_list = []\n",
    "    score_list = []\n",
    "\n",
    "    for row in tqdm(df_.itertuples(), total=len(df_), desc=f\"{code}LLM predicting\"):\n",
    "        if pd.isna(row.url) or row.url == '':\n",
    "            predict_list.append(None)\n",
    "            reason_list.append(None)\n",
    "            score_list.append(None)\n",
    "            continue\n",
    "        \n",
    "        code = str(row.code).zfill(6)\t\n",
    "        name = row.name\n",
    "        predict_dir = f'preprocessed_data/llm/predict_text/{row.sector}/{code}/'\n",
    "        os.makedirs(predict_dir, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            filename = f'{row.year}-{row.quarter}-{str(row.month).zfill(2)}-{row.week}.txt'\n",
    "\n",
    "            with open(f'{predict_dir}{filename}', 'r', encoding='utf-8') as f:\n",
    "                # json.load() 함수를 사용하여 파일의 내용을 파이썬 딕셔너리로 불러옵니다.\n",
    "                data_dict = json.load(f)\n",
    "\n",
    "            predict_list.append(data_dict[\"sentiment\"])\n",
    "            reason_list.append(data_dict[\"reasoning\"])\n",
    "            score_list.append(data_dict[\"score\"])\n",
    "            \n",
    "        except Exception as e:\n",
    "            predict_list.append(\"중립\")\n",
    "            reason_list.append(\"관련 없음\")\n",
    "            score_list.append(0)\n",
    "            \n",
    "    df_predict = df_.copy()\n",
    "    df_predict[\"prediction\"] = predict_list\n",
    "    df_predict[\"reason\"] = reason_list\n",
    "    df_predict[\"score\"] = score_list\n",
    "    df_predict = df_predict[[\"year\", \"quarter\", \"month\", \"week\", \"code\", \"name\", \"sector\", \"upload_dt\", \"prediction\", \"reason\", \"score\"]]\n",
    "    df_predict.to_csv(f\"{predict_dir}{code}.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95054901-f78d-4671-87aa-9e6931a13628",
   "metadata": {},
   "source": [
    "### 영상 예측 결과 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0b1cdce-09e6-4828-8434-465f7dff3442",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120LLM predicting: 100%|██████████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 340.54it/s]\n",
      "150LLM predicting: 100%|██████████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 277.73it/s]\n",
      "660LLM predicting: 100%|██████████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 275.75it/s]\n",
      "1120LLM predicting: 100%|█████████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 445.71it/s]\n",
      "3490LLM predicting: 100%|█████████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 274.55it/s]\n",
      "3550LLM predicting: 100%|█████████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 259.05it/s]\n",
      "3570LLM predicting: 100%|████████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 1202.52it/s]\n",
      "4710LLM predicting: 100%|████████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 1991.83it/s]\n",
      "5930LLM predicting: 100%|█████████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 236.35it/s]\n",
      "6260LLM predicting: 100%|█████████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 469.29it/s]\n",
      "6400LLM predicting: 100%|█████████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 308.11it/s]\n",
      "8060LLM predicting: 100%|█████████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 223.94it/s]\n",
      "9150LLM predicting: 100%|█████████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 336.64it/s]\n",
      "10120LLM predicting: 100%|████████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 351.54it/s]\n",
      "11070LLM predicting: 100%|████████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 361.82it/s]\n",
      "11200LLM predicting: 100%|████████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 449.51it/s]\n",
      "18260LLM predicting: 100%|████████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 334.97it/s]\n",
      "20150LLM predicting: 100%|████████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 364.63it/s]\n",
      "25540LLM predicting: 100%|███████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 2023.31it/s]\n",
      "29530LLM predicting: 100%|███████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 6637.74it/s]\n",
      "34220LLM predicting: 100%|████████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 257.79it/s]\n",
      "34730LLM predicting: 100%|████████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 211.48it/s]\n",
      "42700LLM predicting: 100%|████████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 383.64it/s]\n",
      "47050LLM predicting: 100%|████████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 289.34it/s]\n",
      "47810LLM predicting: 100%|████████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 291.04it/s]\n",
      "51600LLM predicting: 100%|████████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 408.09it/s]\n",
      "66570LLM predicting: 100%|████████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 267.56it/s]\n",
      "86280LLM predicting: 100%|████████████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 523.75it/s]\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('./data_kr/video/뉴스 영상 수집본.csv', encoding='utf-8')\n",
    "for code in df[\"code\"].unique():\n",
    "    df_ = df[df[\"code\"] == code].reset_index(drop=True)\n",
    "    predict_list = []\n",
    "    reason_list = []\n",
    "    score_list = []\n",
    "\n",
    "    for row in tqdm(df_.itertuples(), total=len(df_), desc=f\"{code}LLM predicting\"):\n",
    "        if pd.isna(row.url) or row.url == '':\n",
    "            predict_list.append(None)\n",
    "            reason_list.append(None)\n",
    "            score_list.append(None)\n",
    "            continue\n",
    "        \n",
    "        code = str(row.code).zfill(6)\t\n",
    "        name = row.name\n",
    "        predict_dir = f'preprocessed_data/llm/predict_video/{row.sector}/{code}/'\n",
    "        os.makedirs(predict_dir, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            filename = f'{row.year}-{row.quarter}-{str(row.month).zfill(2)}-{row.week}.txt'\n",
    "            \n",
    "            with open(f'{predict_dir}{filename}', 'r', encoding='utf-8') as f:\n",
    "                # json.load() 함수를 사용하여 파일의 내용을 파이썬 딕셔너리로 불러옵니다.\n",
    "                data_dict = json.load(f)\n",
    "\n",
    "            predict_list.append(data_dict[\"sentiment\"])\n",
    "            reason_list.append(data_dict[\"reasoning\"])\n",
    "            score_list.append(data_dict[\"score\"])\n",
    "            \n",
    "        except Exception as e:\n",
    "            predict_list.append(\"불가능\")\n",
    "            reason_list.append(\"관련 없음\")\n",
    "            score_list.append(0)\n",
    "            \n",
    "    df_predict = df_.copy()\n",
    "    df_predict[\"prediction\"] = predict_list\n",
    "    df_predict[\"reason\"] = reason_list\n",
    "    df_predict[\"score\"] = score_list\n",
    "    df_predict = df_predict[[\"year\", \"quarter\", \"month\", \"week\", \"code\", \"name\", \"sector\", \"upload_dt\", \"prediction\", \"reason\", \"score\"]]\n",
    "    df_predict.to_csv(f\"{predict_dir}{code}.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62be3001-6d4b-4a19-850f-604b6a9662a6",
   "metadata": {},
   "source": [
    "### 영상+기사 예측 결과 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79d33f16-ee3b-4595-b61d-de7db9e728c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120LLM predicting: 100%|███████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 6474.31it/s]\n",
      "150LLM predicting: 100%|███████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 8219.23it/s]\n",
      "660LLM predicting: 100%|███████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 8650.85it/s]\n",
      "1120LLM predicting: 100%|██████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 8634.52it/s]\n",
      "3490LLM predicting: 100%|██████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 6579.81it/s]\n",
      "3550LLM predicting: 100%|██████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 8340.28it/s]\n",
      "3570LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 36493.99it/s]\n",
      "4710LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 68015.74it/s]\n",
      "5930LLM predicting: 100%|██████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 8894.00it/s]\n",
      "6260LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 12388.70it/s]\n",
      "6400LLM predicting: 100%|██████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 6863.77it/s]\n",
      "8060LLM predicting: 100%|██████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 5863.55it/s]\n",
      "9150LLM predicting: 100%|██████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 7051.81it/s]\n",
      "10120LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 8020.30it/s]\n",
      "11070LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 8217.96it/s]\n",
      "11200LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 6638.77it/s]\n",
      "18260LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 7640.24it/s]\n",
      "20150LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 7231.74it/s]\n",
      "25540LLM predicting: 100%|████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 49276.55it/s]\n",
      "29530LLM predicting: 100%|███████████████████████████████████████████████████████| 204/204 [00:00<00:00, 124474.54it/s]\n",
      "34220LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 7795.46it/s]\n",
      "34730LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 6908.61it/s]\n",
      "42700LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 8839.97it/s]\n",
      "47050LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 7457.78it/s]\n",
      "47810LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 7626.35it/s]\n",
      "51600LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 9105.44it/s]\n",
      "66570LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 6542.53it/s]\n",
      "86280LLM predicting: 100%|████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 10766.40it/s]\n"
     ]
    }
   ],
   "source": [
    "from predict_stock_outlook import predict_market_from_mix\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_v = pd.read_csv('data_kr/video/뉴스 영상 수집본.csv', encoding='utf-8')\n",
    "df_v.rename(columns={'url':'v_url', 'upload_dt':'v_upload_dt'}, inplace=True)\n",
    "df_v = df_v[[\"year\",\"quarter\",\"month\",\"week\",\"code\",\"name\",\"sector\",\"after\",\"before\",\"v_url\",\"v_upload_dt\"]]\n",
    "\n",
    "df_a = pd.read_csv('data_kr/video/뉴스 기사 수집본.csv', encoding='utf-8')\n",
    "df_a.rename(columns={'url':'a_url', 'upload_dt':'a_upload_dt'}, inplace=True)\n",
    "df_a = df_a[[\"a_url\",\"a_upload_dt\"]]\n",
    "\n",
    "df = pd.concat([df_v, df_a], axis=1)\n",
    "\n",
    "for code in df[\"code\"].unique():\n",
    "    df_ = df[df[\"code\"] == code].reset_index(drop=True)\n",
    "    predict_list = []\n",
    "    reason_list = []\n",
    "    score_list = []\n",
    "    uploaddt_list = []\n",
    "\n",
    "    for row in tqdm(df_.itertuples(), total=len(df_), desc=f\"{code}LLM predicting\"):\n",
    "        if pd.isna(row.v_url) or pd.isna(row.a_url) :\n",
    "            predict_list.append(None)\n",
    "            reason_list.append(None)\n",
    "            score_list.append(None)\n",
    "            uploaddt_list.append(None)\n",
    "            continue\n",
    "        \n",
    "        code = str(row.code).zfill(6)\n",
    "        name = row.name\n",
    "        predict_dir = f'preprocessed_data/llm/predict_mix/{row.sector}/{code}/'\n",
    "        os.makedirs(predict_dir, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            filename = f'{row.year}-{row.quarter}-{str(row.month).zfill(2)}-{row.week}.txt'\n",
    "            \n",
    "            with open(f'{predict_dir}{filename}', 'r', encoding='utf-8') as f:\n",
    "                # json.load() 함수를 사용하여 파일의 내용을 파이썬 딕셔너리로 불러옵니다.\n",
    "                data_dict = json.load(f)\n",
    "\n",
    "            predict_list.append(data_dict[\"sentiment\"])\n",
    "            reason_list.append(data_dict[\"reasoning\"])\n",
    "            score_list.append(data_dict[\"score\"])\n",
    "            if not pd.isna(row.v_upload_dt) and not pd.isna(row.a_upload_dt):\n",
    "                uploaddt = row.v_upload_dt if row.v_upload_dt > row.a_upload_dt else row.a_upload_dt\n",
    "                uploaddt_list.append(uploaddt)\n",
    "            else:\n",
    "                uploaddt_list.append(None)\n",
    "                \n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'error: {e}, file:{filename}')\n",
    "            predict_list.append(\"불가능\")\n",
    "            reason_list.append(\"관련 없음\")\n",
    "            score_list.append(0)\n",
    "            \n",
    "            \n",
    "    df_predict = df_.copy()\n",
    "    df_predict[\"prediction\"] = predict_list\n",
    "    df_predict[\"reason\"] = reason_list\n",
    "    df_predict[\"score\"] = score_list\n",
    "    df_predict['upload_dt'] = uploaddt_list\n",
    "    df_predict = df_predict[[\"year\", \"quarter\", \"month\", \"week\", \"code\", \"name\", \"sector\", \"upload_dt\", \"prediction\", \"reason\", \"score\"]]\n",
    "    df_predict.to_csv(f\"{predict_dir}{code}.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08186f40-9496-4e66-9322-c5098757571d",
   "metadata": {},
   "source": [
    "## 예측 결과 정리 concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8db379c-233c-48ec-a740-d938881d3694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# 현재 폴더를 Path 객체로 만듭니다.\n",
    "datas=['video', 'text', 'mix']\n",
    "\n",
    "for data in datas:\n",
    "    path_list = []\n",
    "    for sector in [\"산업재\", \"정보기술\"]:\n",
    "        root_path = Path(f'./preprocessed_data/llm/predict_{data}/{sector}')\n",
    "        \n",
    "        # rglob('*')는 재귀적으로 모든(*) 파일과 폴더를 찾습니다.\n",
    "        for path in root_path.rglob('*.csv'):\n",
    "            # path는 파일 또는 폴더에 대한 Path 객체입니다.\n",
    "            path_list.append(path)\n",
    "    df_list = [pd.read_csv(f'{path}', encoding='utf-8') for path in path_list]\n",
    "    df = pd.concat(df_list, axis=0)\n",
    "    df = df.sort_values(by=['code', 'year', 'quarter', 'month', 'week'], ascending=True)\n",
    "    df.to_csv(f'./preprocessed_data/llm/predict_{data}/predict.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebf8014-2226-421d-b3e7-552eb0776904",
   "metadata": {},
   "source": [
    "## mix에 video or text 단독예측도 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54019e35-233a-4065-898d-8f5244976108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_v = pd.read_csv('preprocessed_data/llm/predict_total/predict_video.csv', encoding='utf-8')\n",
    "df_t = pd.read_csv('preprocessed_data/llm/predict_total/predict_text.csv', encoding='utf-8')\n",
    "df_m = pd.read_csv('preprocessed_data/llm/predict_total/predict_mix.csv', encoding='utf-8')\n",
    "\n",
    "df_total = df_m.copy()\n",
    "df_total[pd.isna(df_total)] = df_v[pd.isna(df_total)]\n",
    "df_total[pd.isna(df_total)] = df_t[pd.isna(df_total)]\n",
    "\n",
    "df_total.to_csv('preprocessed_data/llm/predict_total/predict_total.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a07fb5d-bd32-4a5e-bc31-4a1fc0de3100",
   "metadata": {},
   "source": [
    "## LLM 정확도 측정 (업로드일과 일주일 마감일 가격 비교)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "33a9ceff-bdc7-4ffb-9cfb-7ee6f19325f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 5712/5712 [00:25<00:00, 220.68it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 5712/5712 [00:25<00:00, 221.91it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 5712/5712 [00:27<00:00, 207.40it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 5712/5712 [00:26<00:00, 218.60it/s]\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta, datetime\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def get_pred_accuracy():\n",
    "    plt.figure(figsize=(24, 5))\n",
    "    \n",
    "    datas = ['video', 'text', 'mix', 'total']\n",
    "    \n",
    "    for idx, data in enumerate(datas):\n",
    "        df = pd.read_csv(f'preprocessed_data/llm/predict_total/predict_{data}.csv', encoding='utf-8')\n",
    "        df_d = pd.read_csv(f'data_kr/video/뉴스 영상 수집본.csv', encoding='utf-8')[['after', 'before']]\n",
    "        df = pd.concat([df, df_d], axis=1)\n",
    "        df[['upload_dt', 'after', 'before']] = df[['upload_dt', 'after', 'before']].apply(pd.to_datetime)\n",
    "        \n",
    "        ### LLM 예측에 실제 등락 라벨 추가 ###\n",
    "        df[\"code\"] = df[\"code\"].astype(str).str.zfill(6)\n",
    "        \n",
    "        price_upload = [] # 업로드 당일 종가\n",
    "        price_end = [] # 공시 당일 종가\n",
    "        for row in tqdm(df.itertuples(), total=len(df)):\n",
    "            df_price = pd.read_csv(f\"data_kr/price/{row.code}.csv\")\n",
    "            df_price['날짜'] = pd.to_datetime(df_price['날짜'])\n",
    "            ### 업로드 날짜 직전 종가\n",
    "            price_upload.append(df_price.loc[df_price[\"날짜\"] < row.upload_dt, \"종가\"].iloc[-1] if not pd.isna(row.upload_dt) else None)\n",
    "            ### 데이터 업로드 주간 마지막 날 종가\n",
    "            price_end.append(df_price.loc[df_price[\"날짜\"] < row.before, \"종가\"].iloc[-1] if not pd.isna(row.before) else None)\n",
    "        \n",
    "        df[\"price_upload\"] = price_upload\n",
    "        df[\"price_end\"] = price_end\n",
    "    \n",
    "        def check_change(row):\n",
    "            rate = (row[\"price_end\"] / row[\"price_upload\"] - 1) * 100\n",
    "            if rate > 0:\n",
    "                return \"상승\"\n",
    "            else:\n",
    "                return \"하락\"\n",
    "        df[\"label\"] = df.apply(check_change, axis=1)\n",
    "\n",
    "        ###############################################################\n",
    "        ### 유효한 데이터만 남기기 ###\n",
    "        valid_bool = ~(pd.isna(df['prediction']) | (df['prediction'] == '중립'))\n",
    "        prediction = df[valid_bool]['prediction']\n",
    "        label = df[valid_bool]['label']\n",
    "        \n",
    "        mapping = {'매우 긍정': '상승', '긍정': '상승', '중립': '횡보', '부정': '하락', '매우 부정': '하락'}\n",
    "        prediction = [mapping[l] for l in prediction]\n",
    "        \n",
    "        # 혼동 행렬\n",
    "        labels = [\"상승\", \"하락\"]\n",
    "        cm = confusion_matrix(label, prediction, labels=labels)\n",
    "        \n",
    "        # 한글 폰트 설정 (Windows, Mac, Linux 환경에 맞게 설정)\n",
    "        # 윈도우\n",
    "        plt.rc('font', family='Malgun Gothic')\n",
    "    \n",
    "        # 시각화\n",
    "        plt.subplot(1, 4, idx + 1)  # 1행 3열 중 i번째 위치에 subplot을 생성\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "        plt.xlabel('Prediction')\n",
    "        plt.ylabel('Label')\n",
    "        plt.title(f'{data} Accuracy Confusion Matrix')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"preprocessed_data/llm/predict_total/predict_accuracy.png\")\n",
    "    plt.close()\n",
    "\n",
    "get_pred_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7e8058-b0f2-402f-bcce-96bab5d24e97",
   "metadata": {},
   "source": [
    "## LLM 예측 정확도 지표 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "116f7d77-4450-4152-ad53-9cdd5ffe3e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 5712/5712 [00:26<00:00, 219.38it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 5712/5712 [00:25<00:00, 224.15it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 5712/5712 [00:24<00:00, 229.09it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 5712/5712 [00:25<00:00, 222.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video\n",
      "accuracy: 0.4735002912055912\n",
      "precision: 0.4031413612565445\n",
      "recall: 0.6774193548387096\n",
      "f1: 0.5054704595185996\n",
      "\n",
      "text\n",
      "accuracy: 0.49854227405247814\n",
      "precision: 0.46802721088435373\n",
      "recall: 0.8981723237597912\n",
      "f1: 0.6153846153846154\n",
      "\n",
      "mix\n",
      "accuracy: 0.45758661887694146\n",
      "precision: 0.34488734835355284\n",
      "recall: 0.7236363636363636\n",
      "f1: 0.4671361502347418\n",
      "\n",
      "total\n",
      "accuracy: 0.46270213875847677\n",
      "precision: 0.3754538852578068\n",
      "recall: 0.75254730713246\n",
      "f1: 0.500968992248062\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta, datetime\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "def get_pred_accuracy():\n",
    "    scores = dict()\n",
    "    \n",
    "    datas = ['video', 'text', 'mix', 'total']\n",
    "    \n",
    "    for idx, data in enumerate(datas):\n",
    "        df = pd.read_csv(f'preprocessed_data/llm/predict_total/predict_{data}.csv', encoding='utf-8')\n",
    "        df_d = pd.read_csv(f'data_kr/video/뉴스 영상 수집본.csv', encoding='utf-8')[['after', 'before']]\n",
    "        df = pd.concat([df, df_d], axis=1)\n",
    "        df[['upload_dt', 'after', 'before']] = df[['upload_dt', 'after', 'before']].apply(pd.to_datetime)\n",
    "        \n",
    "        ### LLM 예측에 실제 등락 라벨 추가 ###\n",
    "        df[\"code\"] = df[\"code\"].astype(str).str.zfill(6)\n",
    "        \n",
    "        price_upload = [] # 업로드 당일 종가\n",
    "        price_end = [] # 공시 당일 종가\n",
    "        for row in tqdm(df.itertuples(), total=len(df)):\n",
    "            df_price = pd.read_csv(f\"data_kr/price/{row.code}.csv\")\n",
    "            df_price['날짜'] = pd.to_datetime(df_price['날짜'])\n",
    "            ### 업로드 날짜 직전 종가\n",
    "            price_upload.append(df_price.loc[df_price[\"날짜\"] < row.upload_dt, \"종가\"].iloc[-1] if not pd.isna(row.upload_dt) else None)\n",
    "            ### 데이터 업로드 주간 마지막 날 종가\n",
    "            price_end.append(df_price.loc[df_price[\"날짜\"] < row.before, \"종가\"].iloc[-1] if not pd.isna(row.before) else None)\n",
    "        \n",
    "        df[\"price_upload\"] = price_upload\n",
    "        df[\"price_end\"] = price_end\n",
    "    \n",
    "        def check_change(row):\n",
    "            rate = (row[\"price_end\"] / row[\"price_upload\"] - 1) * 100\n",
    "            if rate > 0:\n",
    "                return \"상승\"\n",
    "            else:\n",
    "                return \"하락\"\n",
    "        df[\"label\"] = df.apply(check_change, axis=1)\n",
    "\n",
    "        ###############################################################\n",
    "        ### 유효한 데이터만 남기기 ###\n",
    "        valid_bool = ~(pd.isna(df['prediction']) | (df['prediction'] == '중립'))\n",
    "        prediction = df[valid_bool]['prediction']\n",
    "        label = df[valid_bool]['label']\n",
    "        \n",
    "        mapping = {'매우 긍정': '상승', '긍정': '상승', '중립': '횡보', '부정': '하락', '매우 부정': '하락'}\n",
    "        prediction = [mapping[l] for l in prediction]\n",
    "        \n",
    "\n",
    "        accuracy = accuracy_score(label, prediction)\n",
    "        precision = precision_score(label, prediction, pos_label='상승', zero_division=0)\n",
    "        recall = recall_score(label, prediction, pos_label='상승', zero_division=0)\n",
    "        f1 = f1_score(label, prediction, pos_label='상승', zero_division=0)\n",
    "\n",
    "        score_dict = {'accuracy': accuracy, 'precision':precision, 'recall':recall, 'f1':f1}\n",
    "        scores[data] = score_dict\n",
    "\n",
    "    for data in datas:\n",
    "        print(f'{data}\\naccuracy: {scores[data]['accuracy']}\\nprecision: {scores[data]['precision']}\\nrecall: {scores[data]['recall']}\\nf1: {scores[data]['f1']}\\n')\n",
    "get_pred_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f3bb2a-0846-41db-8d01-4bbaf9d80d27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s3ce_kernel",
   "language": "python",
   "name": "s3ce"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
