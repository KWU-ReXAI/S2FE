{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b4e4614-6333-4564-b23b-d59ca6a4a8d2",
   "metadata": {},
   "source": [
    "### 오디오 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9320139f-722c-4682-93f8-5e6da877e9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from predict_stock_outlook import extract_video_audio, predict_market_from_summary_self_consistency\n",
    "\n",
    "df = pd.read_csv(\"data_kr/video/자료 수집 최종본.csv\")\n",
    "\n",
    "for row in df.itertuples():\n",
    "    if pd.isna(row.url) or row.url == '' or row.category != \"video\":\n",
    "        continue\n",
    "\n",
    "    code = str(row.code).zfill(6)\n",
    "    audio_dir = f'data_kr/video/audio/{row.sector}/{code}/'\n",
    "    text_dir = f'data_kr/video/text/{row.sector}/{code}/'\n",
    "    os.makedirs(audio_dir, exist_ok=True)\n",
    "\n",
    "    if audio_dir + f'{row.year}-{row.quarter}' != 'data_kr/video/audio/산업재/003490/2016-Q2':\n",
    "        continue\n",
    "    \n",
    "    if extract_video_audio(\"link\", row.url, audio_dir + f'{row.year}-{row.quarter}'):\n",
    "        with open('data_kr/video/log.txt', \"a\", encoding=\"utf-8\") as log_file:\n",
    "            timestamp = datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S]\")\n",
    "            log_file.write(f\"{timestamp} audio download completed: {audio_dir + f'{row.year}-{row.quarter}'}\\n\")\n",
    "    else:\n",
    "        with open('data_kr/video/log.txt', \"a\", encoding=\"utf-8\") as log_file:\n",
    "            timestamp = datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S]\")\n",
    "            log_file.write(f\"{timestamp} audio download error: {audio_dir + f'{row.year}-{row.quarter}'}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1dc206-96f3-4bf2-9b25-09b38ab4529c",
   "metadata": {},
   "source": [
    "### 텍스트로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4448eb2f-9bec-4513-8714-ad550cccffdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from predict_stock_outlook import audio2text\n",
    "\n",
    "df = pd.read_csv(\"data_kr/video/자료 수집 최종본.csv\")\n",
    "\n",
    "for row in df.itertuples():\n",
    "    if pd.isna(row.url) or row.url == '' or row.category != \"video\":\n",
    "        continue\n",
    "\n",
    "    code = str(row.code).zfill(6)\n",
    "    audio_dir = f'data_kr/video/audio/{row.sector}/{code}/'\n",
    "    text_dir = f'data_kr/video/text/{row.sector}/{code}/'\n",
    "    os.makedirs(text_dir, exist_ok=True)\n",
    "    \n",
    "    if audio_dir + f'{row.year}-{row.quarter}' != 'data_kr/video/audio/산업재/003490/2016-Q2':\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        text = audio2text(audio_dir + f'{row.year}-{row.quarter}')\n",
    "        with open(text_dir + f'{row.year}-{row.quarter}.txt', \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    "        with open('data_kr/video/log.txt', \"a\", encoding=\"utf-8\") as log_file:\n",
    "            timestamp = datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S]\")\n",
    "            log_file.write(f\"{timestamp} whisper completed: {text_dir + f'{row.year}-{row.quarter}'}\\n\")\n",
    "    except Exception as e:\n",
    "        with open('data_kr/video/log.txt', \"a\", encoding=\"utf-8\") as log_file:\n",
    "            timestamp = datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S]\")\n",
    "            log_file.write(f\"{timestamp} whisper error: {text_dir + f'{row.year}-{row.quarter}'}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84c7ccf-1dea-4696-aa6d-d36f5e4dee1b",
   "metadata": {},
   "source": [
    "### 텍스트 token 수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3802c4-52df-4dde-a9f5-d7b9eccfcb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data_kr/video/자료 수집 최종본.csv\")\n",
    "\n",
    "import tiktoken\n",
    "# 예: GPT-4용 인코더 불러오기\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "total_tokens = 0\n",
    "\n",
    "for row in tqdm(df.itertuples(), total=len(df), desc=\"checking tokens\"):\n",
    "\tif pd.isna(row.url) or row.url == '':\n",
    "\t\tcontinue\n",
    "\n",
    "\tcode = str(row.code).zfill(6)\n",
    "\ttext_dir = f'data_kr/video/text/{row.sector}/{code}/'\n",
    "\tos.makedirs(text_dir, exist_ok=True)\n",
    "    \n",
    "\ttry:\n",
    "\t\tfilename = f'{row.year}-{row.quarter}-{str(row.month).zfill(2)}-{row.week}.txt'\n",
    "\t\twith open(text_dir + filename, \"r\", encoding=\"utf-8\") as file:\n",
    "\t\t\ttext = file.read()\n",
    "\t\t\tsystem_prompt = \"\"\"\n",
    "\t\t너는 경제 전문 뉴스 분석 AI야. 사용자가 지정한 종목(회사명)과 직접적으로 관련된 정보만 선택해 핵심적으로 요약해.\n",
    "\t\t사실 기반으로 요약하고, 감성이나 추론이 필요한 경우에는 중립적으로 표현해.\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\t\tuser_prompt = f\"\"\"\n",
    "\t\t다음은 경제 뉴스 기사입니다.\n",
    "\n",
    "\t\t이 기사에서 **한국 상장 기업 \"{code}\"**과 관련된 내용만 골라 요약해 주세요.\n",
    "\n",
    "\t\t요약 기준:\n",
    "\t\t- \"{code}\"이 언급된 부분 중심\n",
    "\t\t- 관련 사업, 실적, 주가, 시장 반응, 경쟁사와의 연관성\n",
    "\t\t- 정부 정책, 산업 트렌드 등 외부 요인 중 관련 있는 부분\n",
    "\t\t- 부정적/긍정적 논조도 간단히 언급 (있는 경우)\n",
    "\n",
    "\t\t형식은 간결한 문장 또는 Bullet Point 형식으로 작성해 주세요.\n",
    "\n",
    "\t\t기사 전문:\n",
    "\t\t{text}\n",
    "\t\t\"\"\"\n",
    "\t\t\ttotal_prompt = system_prompt + user_prompt\n",
    "\t\t\ttokens = encoding.encode(total_prompt)\n",
    "\t\t\ttotal_tokens += len(tokens)\n",
    "\t\twith open('data_kr/video/num_token.txt', \"a\", encoding=\"utf-8\") as log_file:\n",
    "\t\t\ttimestamp = datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S]\")\n",
    "\t\t\tlog_file.write(f\"{text_dir}{filename} len: {len(text)}, token: {len(tokens)}\\n\")\n",
    "\texcept Exception as e:\n",
    "\t\twith open('data_kr/video/num_token.txt', \"a\", encoding=\"utf-8\") as log_file:\n",
    "\t\t\ttimestamp = datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S]\")\n",
    "\t\t\tlog_file.write(f\"{text_dir}{filename} error: \" + e + \"\\n\")\n",
    "\n",
    "with open('data_kr/video/num_token.txt', \"a\", encoding=\"utf-8\") as log_file:\n",
    "\tlog_file.write(f\"total tokens: {total_tokens}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270d9d4c-6adf-4f55-b874-ecfb5d6b58a7",
   "metadata": {},
   "source": [
    "### LLM으로 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87e4c23-ba5a-4768-8ded-6e69e51f8783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from predict_stock_outlook import summarize_text\n",
    "\n",
    "df = pd.read_csv(\"data_kr/video/자료 수집 최종본.csv\")\n",
    "\n",
    "for row in tqdm(df.itertuples(), total=len(df), desc=\"LLM summarizing\"):\n",
    "    if pd.isna(row.url) or row.url == '':\n",
    "        continue\n",
    "\n",
    "    code = str(row.code).zfill(6)\n",
    "    name = row.name\n",
    "    text_dir = f'data_kr/video/text/{row.sector}/{code}/'\n",
    "    summary_dir = f'preprocessed_data/llm/summary/{row.sector}/{code}/'\n",
    "    os.makedirs(summary_dir, exist_ok=True)\n",
    "        \n",
    "    try:\n",
    "        filename = f'{row.year}-{row.quarter}-{str(row.month).zfill(2)}-{row.week}.txt'\n",
    "        stock = f'{name}({code})'\n",
    "        with open(text_dir + filename, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "        summary = summarize_text(text, stock)\n",
    "        with open(summary_dir + filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(summary)\n",
    "        with open('preprocessed_data/llm/summary/log.txt', \"a\", encoding=\"utf-8\") as log_file:\n",
    "            timestamp = datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S]\")\n",
    "            log_file.write(f\"{timestamp} summary completed: {summary_dir + filename}\\n\")\n",
    "    except Exception as e:\n",
    "        with open('preprocessed_data/llm/summary/log.txt', \"a\", encoding=\"utf-8\") as log_file:\n",
    "            timestamp = datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S]\")\n",
    "            log_file.write(f\"{timestamp} summary error: {summary_dir + filename}\\t error: {e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f08c86-f82f-476a-b5a9-a9c47e242a4d",
   "metadata": {},
   "source": [
    "### LLM으로 기사 자막요약을 통해 등락 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ea05f01-d708-4647-bce7-2cdbad1dcf91",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-07-11T12:56:17.373445Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120LLM predicting: 100%|██████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 11619.04it/s]\n",
      "150LLM predicting: 100%|██████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 17436.38it/s]\n",
      "660LLM predicting: 100%|██████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 18546.80it/s]\n",
      "1120LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 11562.52it/s]\n",
      "3490LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 13143.44it/s]\n",
      "3550LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 17636.93it/s]\n",
      "3570LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 22440.61it/s]\n",
      "4710LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 24788.17it/s]\n",
      "5930LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 18899.52it/s]\n",
      "6260LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 15527.98it/s]\n",
      "6400LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 11824.41it/s]\n",
      "8060LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 12652.50it/s]\n",
      "9150LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 12589.58it/s]\n",
      "10120LLM predicting: 100%|███████████████████████████████████████████████████████████| 204/204 [00:07<00:00, 28.37it/s]\n",
      "11070LLM predicting: 100%|████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 12696.43it/s]\n",
      "11200LLM predicting: 100%|████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 11425.42it/s]\n",
      "18260LLM predicting: 100%|████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 15428.30it/s]\n",
      "20150LLM predicting: 100%|████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 12148.25it/s]\n",
      "25540LLM predicting: 100%|████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 24695.16it/s]\n",
      "29530LLM predicting: 100%|████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 34335.39it/s]\n",
      "34220LLM predicting: 100%|████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 14165.96it/s]\n",
      "34730LLM predicting: 100%|████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 14180.28it/s]\n",
      "42700LLM predicting: 100%|████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 14563.31it/s]\n",
      "47050LLM predicting: 100%|████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 11761.19it/s]\n",
      "47810LLM predicting: 100%|████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 13763.76it/s]\n",
      "51600LLM predicting: 100%|████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 12047.00it/s]\n",
      "66570LLM predicting: 100%|████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 12211.36it/s]\n",
      "86280LLM predicting: 100%|████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 13316.08it/s]\n"
     ]
    }
   ],
   "source": [
    "from predict_stock_outlook import predict_market_from_summary\n",
    "from predict_stock_outlook import predict_market_from_summary_self_consistency\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data_kr/video/뉴스 기사 수집본.csv', encoding='utf-8')\n",
    "llm_dir = 'predict_text_gem' # preprocessed_data/llm/ 하위폴더명 (llm predict 저장될 곳)\n",
    "\n",
    "for code in df[\"code\"].unique():\n",
    "    df_ = df[df[\"code\"] == code].reset_index(drop=True)\n",
    "    \n",
    "    for row in tqdm(df_.itertuples(), total=len(df_), desc=f\"{code}LLM predicting\"):\n",
    "        if pd.isna(row.url) or row.url == '':\n",
    "            continue\n",
    "        \n",
    "        code = str(row.code).zfill(6)\n",
    "        name = row.name\n",
    "        article_dir = f'data_kr/video/text/{row.sector}/{code}/'\n",
    "        predict_dir = f'preprocessed_data/llm/{llm_dir}/{row.sector}/{code}/'\n",
    "        os.makedirs(predict_dir, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            filename = f'{row.year}-{row.quarter}-{str(row.month).zfill(2)}-{row.week}.txt'\n",
    "            stock = f'{name}({code})'\n",
    "            with open(f'{article_dir}{filename}', \"r\", encoding=\"utf-8\") as file:\n",
    "                article = file.read()\n",
    "            data = predict_market_from_summary_self_consistency(article, f'{name}({code})')\n",
    "            \n",
    "            with open(f'{predict_dir}{filename}', \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(data)\n",
    "            \n",
    "            with open(f'preprocessed_data/llm/{llm_dir}/log.txt', \"a\", encoding=\"utf-8\") as log_file:\n",
    "                timestamp = datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S]\")\n",
    "                log_file.write(f\"{timestamp} predict completed: {predict_dir}{filename}\\n\")\n",
    "        except Exception as e:\n",
    "            with open(f'preprocessed_data/llm/{llm_dir}/log.txt', \"a\", encoding=\"utf-8\") as log_file:\n",
    "                timestamp = datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S]\")\n",
    "                log_file.write(f\"{timestamp} predict error: {predict_dir}{filename}\\n\")\n",
    "                print(e)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120LLM predicting:   0%|          | 1/204 [00:06<23:29,  6.94s/it]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ebf24048-7d41-471e-855b-d400cca54424",
   "metadata": {},
   "source": [
    "### LLM으로 영상 자막요약을 통해 등락 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4b5f5a3-0b98-4438-8975-64355fd06d4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T15:15:57.758196Z",
     "start_time": "2025-07-11T15:15:37.596026Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120LLM predicting: 100%|██████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 11363.96it/s]\n",
      "150LLM predicting: 100%|███████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 9955.18it/s]\n",
      "660LLM predicting: 100%|███████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 9596.44it/s]\n",
      "1120LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 14479.76it/s]\n",
      "3490LLM predicting: 100%|██████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 9962.72it/s]\n",
      "3550LLM predicting: 100%|██████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 9697.04it/s]\n",
      "3570LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 37468.82it/s]\n",
      "4710LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 55463.67it/s]\n",
      "5930LLM predicting: 100%|██████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 9183.33it/s]\n",
      "6260LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 16449.52it/s]\n",
      "6400LLM predicting: 100%|██████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 9870.66it/s]\n",
      "8060LLM predicting: 100%|██████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 7586.45it/s]\n",
      "9150LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 11878.25it/s]\n",
      "10120LLM predicting: 100%|████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 11823.76it/s]\n",
      "11070LLM predicting: 100%|████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 13929.57it/s]\n",
      "11200LLM predicting: 100%|████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 14383.13it/s]\n",
      "18260LLM predicting: 100%|████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 11643.87it/s]\n",
      "20150LLM predicting: 100%|████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 13739.45it/s]\n",
      "25540LLM predicting: 100%|████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 61450.59it/s]\n",
      "29530LLM predicting: 100%|███████████████████████████████████████████████████████| 204/204 [00:00<00:00, 141381.03it/s]\n",
      "34220LLM predicting: 100%|████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 10771.68it/s]\n",
      "34730LLM predicting: 100%|████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 10010.74it/s]\n",
      "42700LLM predicting: 100%|████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 14546.22it/s]\n",
      "47050LLM predicting: 100%|████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 13046.05it/s]\n",
      "47810LLM predicting: 100%|████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 10597.71it/s]\n",
      "51600LLM predicting: 100%|████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 14345.27it/s]\n",
      "66570LLM predicting: 100%|███████████████████████████████████████████████████████████| 204/204 [00:05<00:00, 38.52it/s]\n",
      "86280LLM predicting: 100%|████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 17997.14it/s]\n"
     ]
    }
   ],
   "source": [
    "from predict_stock_outlook import predict_market_from_summary\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data_kr/video/뉴스 영상 수집본.csv', encoding='utf-8')\n",
    "llm_dir = 'predict_video_gem' # preprocessed_data/llm/ 하위폴더명 (llm predict 저장될 곳)\n",
    "\n",
    "for code in df[\"code\"].unique():\n",
    "    df_ = df[df[\"code\"] == code].reset_index(drop=True)\n",
    "\n",
    "    for row in tqdm(df_.itertuples(), total=len(df_), desc=f\"{code}LLM predicting\"):\n",
    "        if pd.isna(row.url) or row.url == '':\n",
    "            continue\n",
    "        \n",
    "        code = str(row.code).zfill(6)\t\n",
    "        name = row.name\n",
    "        script_dir = f'data_kr/video/script/{row.sector}/{code}/'\n",
    "        predict_dir = f'preprocessed_data/llm/{llm_dir}/{row.sector}/{code}/'\n",
    "        os.makedirs(predict_dir, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            filename = f'{row.year}-{row.quarter}-{str(row.month).zfill(2)}-{row.week}.txt'\n",
    "            stock = f'{name}({code})'\n",
    "            with open(f'{script_dir}{filename}', \"r\", encoding=\"utf-8\") as file:\n",
    "                script = file.read()\n",
    "            prediction = predict_market_from_summary(script, f'{name}({code})')\n",
    "            \n",
    "            with open(f'{predict_dir}{filename}', \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(prediction)\n",
    "            \n",
    "            with open(f'preprocessed_data/llm/{llm_dir}/log.txt', \"a\", encoding=\"utf-8\") as log_file:\n",
    "                timestamp = datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S]\")\n",
    "                log_file.write(f\"{timestamp} predict completed: {predict_dir}{filename}\\n\")\n",
    "        except Exception as e:\n",
    "            with open(f'preprocessed_data/llm/{llm_dir}/log.txt', \"a\", encoding=\"utf-8\") as log_file:\n",
    "                timestamp = datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S]\")\n",
    "                log_file.write(f\"{timestamp} predict error: {predict_dir}{filename}\\n\")  "
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120LLM predicting:  14%|█▎        | 28/204 [00:05<00:34,  5.10it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 28\u001B[39m\n\u001B[32m     26\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mscript_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mfilename\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mr\u001B[39m\u001B[33m\"\u001B[39m, encoding=\u001B[33m\"\u001B[39m\u001B[33mutf-8\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m file:\n\u001B[32m     27\u001B[39m     script = file.read()\n\u001B[32m---> \u001B[39m\u001B[32m28\u001B[39m prediction = \u001B[43mpredict_market_from_summary\u001B[49m\u001B[43m(\u001B[49m\u001B[43mscript\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mname\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m(\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mcode\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m)\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     30\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpredict_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mfilename\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mw\u001B[39m\u001B[33m\"\u001B[39m, encoding=\u001B[33m\"\u001B[39m\u001B[33mutf-8\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m file:\n\u001B[32m     31\u001B[39m     file.write(prediction)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Graduation_Project\\Code_Of_DL\\S3CE\\predict_stock_outlook.py:73\u001B[39m, in \u001B[36mpredict_market_from_summary\u001B[39m\u001B[34m(summary, stock)\u001B[39m\n\u001B[32m     30\u001B[39m \tsystem_prompt = \u001B[33m\"\"\"\u001B[39m\n\u001B[32m     31\u001B[39m \u001B[33m당신은 주어진 기업 소식을 종합적으로 분석하여, 특정 주식 종목의 단기 등락 가능성을 판단하는 정보 분석 전문가입니다. 제시되는 분석 단계를 따라 논리적으로 추론한 후, 최종 판단을 단 하나의 정수로만 내려야 합니다.\u001B[39m\n\u001B[32m     32\u001B[39m \u001B[33m\"\"\"\u001B[39m\n\u001B[32m     34\u001B[39m \tuser_prompt = \u001B[33mf\u001B[39m\u001B[33m\"\"\"\u001B[39m\n\u001B[32m     35\u001B[39m \u001B[33m한국 상장 기업 \u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstock\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\u001B[33m과 관련된 소식이 제공됩니다.\u001B[39m\n\u001B[32m     36\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m     70\u001B[39m \n\u001B[32m     71\u001B[39m \u001B[33m\"\"\"\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m73\u001B[39m \tresponse = \u001B[43mclient\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmodels\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgenerate_content\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     74\u001B[39m \u001B[43m\t\t\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mgemini-2.5-flash\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     75\u001B[39m \u001B[43m\t\t\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtypes\u001B[49m\u001B[43m.\u001B[49m\u001B[43mGenerateContentConfig\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     76\u001B[39m \u001B[43m\t\t\t\u001B[49m\u001B[43msystem_instruction\u001B[49m\u001B[43m=\u001B[49m\u001B[43msystem_prompt\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     77\u001B[39m \u001B[43m\t\t\u001B[49m\u001B[43mcontents\u001B[49m\u001B[43m=\u001B[49m\u001B[43muser_prompt\u001B[49m\n\u001B[32m     78\u001B[39m \u001B[43m\t\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     80\u001B[39m \t\u001B[38;5;28;01mreturn\u001B[39;00m response.text\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Graduation_Project\\Code_Of_DL\\S3CE\\.venv\\Lib\\site-packages\\google\\genai\\models.py:5898\u001B[39m, in \u001B[36mModels.generate_content\u001B[39m\u001B[34m(self, model, contents, config)\u001B[39m\n\u001B[32m   5896\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m remaining_remote_calls_afc > \u001B[32m0\u001B[39m:\n\u001B[32m   5897\u001B[39m   i += \u001B[32m1\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m5898\u001B[39m   response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_generate_content\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   5899\u001B[39m \u001B[43m      \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontents\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcontents\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m=\u001B[49m\u001B[43mparsed_config\u001B[49m\n\u001B[32m   5900\u001B[39m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   5901\u001B[39m   logger.info(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mAFC remote call \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m is done.\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m   5902\u001B[39m   remaining_remote_calls_afc -= \u001B[32m1\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Graduation_Project\\Code_Of_DL\\S3CE\\.venv\\Lib\\site-packages\\google\\genai\\models.py:4838\u001B[39m, in \u001B[36mModels._generate_content\u001B[39m\u001B[34m(self, model, contents, config)\u001B[39m\n\u001B[32m   4835\u001B[39m request_dict = _common.convert_to_dict(request_dict)\n\u001B[32m   4836\u001B[39m request_dict = _common.encode_unserializable_types(request_dict)\n\u001B[32m-> \u001B[39m\u001B[32m4838\u001B[39m response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_api_client\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   4839\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mpost\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequest_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhttp_options\u001B[49m\n\u001B[32m   4840\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   4842\u001B[39m response_dict = \u001B[33m'\u001B[39m\u001B[33m'\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m response.body \u001B[38;5;28;01melse\u001B[39;00m json.loads(response.body)\n\u001B[32m   4844\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._api_client.vertexai:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Graduation_Project\\Code_Of_DL\\S3CE\\.venv\\Lib\\site-packages\\google\\genai\\_api_client.py:1077\u001B[39m, in \u001B[36mBaseApiClient.request\u001B[39m\u001B[34m(self, http_method, path, request_dict, http_options)\u001B[39m\n\u001B[32m   1067\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mrequest\u001B[39m(\n\u001B[32m   1068\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   1069\u001B[39m     http_method: \u001B[38;5;28mstr\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1072\u001B[39m     http_options: Optional[HttpOptionsOrDict] = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1073\u001B[39m ) -> SdkHttpResponse:\n\u001B[32m   1074\u001B[39m   http_request = \u001B[38;5;28mself\u001B[39m._build_request(\n\u001B[32m   1075\u001B[39m       http_method, path, request_dict, http_options\n\u001B[32m   1076\u001B[39m   )\n\u001B[32m-> \u001B[39m\u001B[32m1077\u001B[39m   response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhttp_request\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m   1078\u001B[39m   response_body = (\n\u001B[32m   1079\u001B[39m       response.response_stream[\u001B[32m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m response.response_stream \u001B[38;5;28;01melse\u001B[39;00m \u001B[33m'\u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m   1080\u001B[39m   )\n\u001B[32m   1081\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m SdkHttpResponse(headers=response.headers, body=response_body)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Graduation_Project\\Code_Of_DL\\S3CE\\.venv\\Lib\\site-packages\\google\\genai\\_api_client.py:968\u001B[39m, in \u001B[36mBaseApiClient._request\u001B[39m\u001B[34m(self, http_request, stream)\u001B[39m\n\u001B[32m    963\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_request\u001B[39m(\n\u001B[32m    964\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    965\u001B[39m     http_request: HttpRequest,\n\u001B[32m    966\u001B[39m     stream: \u001B[38;5;28mbool\u001B[39m = \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m    967\u001B[39m ) -> HttpResponse:\n\u001B[32m--> \u001B[39m\u001B[32m968\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_retry\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_request_once\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhttp_request\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Graduation_Project\\Code_Of_DL\\S3CE\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:475\u001B[39m, in \u001B[36mRetrying.__call__\u001B[39m\u001B[34m(self, fn, *args, **kwargs)\u001B[39m\n\u001B[32m    473\u001B[39m retry_state = RetryCallState(retry_object=\u001B[38;5;28mself\u001B[39m, fn=fn, args=args, kwargs=kwargs)\n\u001B[32m    474\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m475\u001B[39m     do = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43miter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mretry_state\u001B[49m\u001B[43m=\u001B[49m\u001B[43mretry_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    476\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(do, DoAttempt):\n\u001B[32m    477\u001B[39m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Graduation_Project\\Code_Of_DL\\S3CE\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:376\u001B[39m, in \u001B[36mBaseRetrying.iter\u001B[39m\u001B[34m(self, retry_state)\u001B[39m\n\u001B[32m    374\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    375\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m action \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.iter_state.actions:\n\u001B[32m--> \u001B[39m\u001B[32m376\u001B[39m     result = \u001B[43maction\u001B[49m\u001B[43m(\u001B[49m\u001B[43mretry_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    377\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Graduation_Project\\Code_Of_DL\\S3CE\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:398\u001B[39m, in \u001B[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001B[39m\u001B[34m(rs)\u001B[39m\n\u001B[32m    396\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_post_retry_check_actions\u001B[39m(\u001B[38;5;28mself\u001B[39m, retry_state: \u001B[33m\"\u001B[39m\u001B[33mRetryCallState\u001B[39m\u001B[33m\"\u001B[39m) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    397\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m.iter_state.is_explicit_retry \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m.iter_state.retry_run_result):\n\u001B[32m--> \u001B[39m\u001B[32m398\u001B[39m         \u001B[38;5;28mself\u001B[39m._add_action_func(\u001B[38;5;28;01mlambda\u001B[39;00m rs: \u001B[43mrs\u001B[49m\u001B[43m.\u001B[49m\u001B[43moutcome\u001B[49m\u001B[43m.\u001B[49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m    399\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[32m    401\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.after \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:449\u001B[39m, in \u001B[36mFuture.result\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m    447\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m CancelledError()\n\u001B[32m    448\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._state == FINISHED:\n\u001B[32m--> \u001B[39m\u001B[32m449\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m__get_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    451\u001B[39m \u001B[38;5;28mself\u001B[39m._condition.wait(timeout)\n\u001B[32m    453\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._state \u001B[38;5;129;01min\u001B[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:401\u001B[39m, in \u001B[36mFuture.__get_result\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    399\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._exception:\n\u001B[32m    400\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m401\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m._exception\n\u001B[32m    402\u001B[39m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    403\u001B[39m         \u001B[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001B[39;00m\n\u001B[32m    404\u001B[39m         \u001B[38;5;28mself\u001B[39m = \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Graduation_Project\\Code_Of_DL\\S3CE\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:478\u001B[39m, in \u001B[36mRetrying.__call__\u001B[39m\u001B[34m(self, fn, *args, **kwargs)\u001B[39m\n\u001B[32m    476\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(do, DoAttempt):\n\u001B[32m    477\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m478\u001B[39m         result = \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    479\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m:  \u001B[38;5;66;03m# noqa: B902\u001B[39;00m\n\u001B[32m    480\u001B[39m         retry_state.set_exception(sys.exc_info())  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Graduation_Project\\Code_Of_DL\\S3CE\\.venv\\Lib\\site-packages\\google\\genai\\_api_client.py:951\u001B[39m, in \u001B[36mBaseApiClient._request_once\u001B[39m\u001B[34m(self, http_request, stream)\u001B[39m\n\u001B[32m    947\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m HttpResponse(\n\u001B[32m    948\u001B[39m       response.headers, response \u001B[38;5;28;01mif\u001B[39;00m stream \u001B[38;5;28;01melse\u001B[39;00m [response.text]\n\u001B[32m    949\u001B[39m   )\n\u001B[32m    950\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m951\u001B[39m   response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_httpx_client\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    952\u001B[39m \u001B[43m      \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhttp_request\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    953\u001B[39m \u001B[43m      \u001B[49m\u001B[43murl\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhttp_request\u001B[49m\u001B[43m.\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    954\u001B[39m \u001B[43m      \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhttp_request\u001B[49m\u001B[43m.\u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    955\u001B[39m \u001B[43m      \u001B[49m\u001B[43mcontent\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    956\u001B[39m \u001B[43m      \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhttp_request\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    957\u001B[39m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    958\u001B[39m   errors.APIError.raise_for_response(response)\n\u001B[32m    959\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m HttpResponse(\n\u001B[32m    960\u001B[39m       response.headers, response \u001B[38;5;28;01mif\u001B[39;00m stream \u001B[38;5;28;01melse\u001B[39;00m [response.text]\n\u001B[32m    961\u001B[39m   )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpx\\_client.py:825\u001B[39m, in \u001B[36mClient.request\u001B[39m\u001B[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001B[39m\n\u001B[32m    810\u001B[39m     warnings.warn(message, \u001B[38;5;167;01mDeprecationWarning\u001B[39;00m, stacklevel=\u001B[32m2\u001B[39m)\n\u001B[32m    812\u001B[39m request = \u001B[38;5;28mself\u001B[39m.build_request(\n\u001B[32m    813\u001B[39m     method=method,\n\u001B[32m    814\u001B[39m     url=url,\n\u001B[32m   (...)\u001B[39m\u001B[32m    823\u001B[39m     extensions=extensions,\n\u001B[32m    824\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m825\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mauth\u001B[49m\u001B[43m=\u001B[49m\u001B[43mauth\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpx\\_client.py:914\u001B[39m, in \u001B[36mClient.send\u001B[39m\u001B[34m(self, request, stream, auth, follow_redirects)\u001B[39m\n\u001B[32m    910\u001B[39m \u001B[38;5;28mself\u001B[39m._set_timeout(request)\n\u001B[32m    912\u001B[39m auth = \u001B[38;5;28mself\u001B[39m._build_request_auth(request, auth)\n\u001B[32m--> \u001B[39m\u001B[32m914\u001B[39m response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_send_handling_auth\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    915\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    916\u001B[39m \u001B[43m    \u001B[49m\u001B[43mauth\u001B[49m\u001B[43m=\u001B[49m\u001B[43mauth\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    917\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    918\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhistory\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    919\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    920\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    921\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m stream:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpx\\_client.py:942\u001B[39m, in \u001B[36mClient._send_handling_auth\u001B[39m\u001B[34m(self, request, auth, follow_redirects, history)\u001B[39m\n\u001B[32m    939\u001B[39m request = \u001B[38;5;28mnext\u001B[39m(auth_flow)\n\u001B[32m    941\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m942\u001B[39m     response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_send_handling_redirects\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    943\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    944\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    945\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhistory\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhistory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    946\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    947\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    948\u001B[39m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpx\\_client.py:979\u001B[39m, in \u001B[36mClient._send_handling_redirects\u001B[39m\u001B[34m(self, request, follow_redirects, history)\u001B[39m\n\u001B[32m    976\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._event_hooks[\u001B[33m\"\u001B[39m\u001B[33mrequest\u001B[39m\u001B[33m\"\u001B[39m]:\n\u001B[32m    977\u001B[39m     hook(request)\n\u001B[32m--> \u001B[39m\u001B[32m979\u001B[39m response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_send_single_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    980\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    981\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._event_hooks[\u001B[33m\"\u001B[39m\u001B[33mresponse\u001B[39m\u001B[33m\"\u001B[39m]:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpx\\_client.py:1014\u001B[39m, in \u001B[36mClient._send_single_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m   1009\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m   1010\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mAttempted to send an async request with a sync Client instance.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1011\u001B[39m     )\n\u001B[32m   1013\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request=request):\n\u001B[32m-> \u001B[39m\u001B[32m1014\u001B[39m     response = \u001B[43mtransport\u001B[49m\u001B[43m.\u001B[49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1016\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response.stream, SyncByteStream)\n\u001B[32m   1018\u001B[39m response.request = request\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpx\\_transports\\default.py:250\u001B[39m, in \u001B[36mHTTPTransport.handle_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    237\u001B[39m req = httpcore.Request(\n\u001B[32m    238\u001B[39m     method=request.method,\n\u001B[32m    239\u001B[39m     url=httpcore.URL(\n\u001B[32m   (...)\u001B[39m\u001B[32m    247\u001B[39m     extensions=request.extensions,\n\u001B[32m    248\u001B[39m )\n\u001B[32m    249\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m map_httpcore_exceptions():\n\u001B[32m--> \u001B[39m\u001B[32m250\u001B[39m     resp = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_pool\u001B[49m\u001B[43m.\u001B[49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    252\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(resp.stream, typing.Iterable)\n\u001B[32m    254\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m Response(\n\u001B[32m    255\u001B[39m     status_code=resp.status,\n\u001B[32m    256\u001B[39m     headers=resp.headers,\n\u001B[32m    257\u001B[39m     stream=ResponseStream(resp.stream),\n\u001B[32m    258\u001B[39m     extensions=resp.extensions,\n\u001B[32m    259\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001B[39m, in \u001B[36mConnectionPool.handle_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    253\u001B[39m         closing = \u001B[38;5;28mself\u001B[39m._assign_requests_to_connections()\n\u001B[32m    255\u001B[39m     \u001B[38;5;28mself\u001B[39m._close_connections(closing)\n\u001B[32m--> \u001B[39m\u001B[32m256\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m exc \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    258\u001B[39m \u001B[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001B[39;00m\n\u001B[32m    259\u001B[39m \u001B[38;5;66;03m# the point at which the response is closed.\u001B[39;00m\n\u001B[32m    260\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response.stream, typing.Iterable)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001B[39m, in \u001B[36mConnectionPool.handle_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    232\u001B[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001B[32m    234\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    235\u001B[39m     \u001B[38;5;66;03m# Send the request on the assigned connection.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m236\u001B[39m     response = \u001B[43mconnection\u001B[49m\u001B[43m.\u001B[49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    237\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpool_request\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\n\u001B[32m    238\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    239\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m ConnectionNotAvailable:\n\u001B[32m    240\u001B[39m     \u001B[38;5;66;03m# In some cases a connection may initially be available to\u001B[39;00m\n\u001B[32m    241\u001B[39m     \u001B[38;5;66;03m# handle a request, but then become unavailable.\u001B[39;00m\n\u001B[32m    242\u001B[39m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[32m    243\u001B[39m     \u001B[38;5;66;03m# In this case we clear the connection and try again.\u001B[39;00m\n\u001B[32m    244\u001B[39m     pool_request.clear_connection()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001B[39m, in \u001B[36mHTTPConnection.handle_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    100\u001B[39m     \u001B[38;5;28mself\u001B[39m._connect_failed = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m    101\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m exc\n\u001B[32m--> \u001B[39m\u001B[32m103\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_connection\u001B[49m\u001B[43m.\u001B[49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpcore\\_sync\\http11.py:136\u001B[39m, in \u001B[36mHTTP11Connection.handle_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    134\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m Trace(\u001B[33m\"\u001B[39m\u001B[33mresponse_closed\u001B[39m\u001B[33m\"\u001B[39m, logger, request) \u001B[38;5;28;01mas\u001B[39;00m trace:\n\u001B[32m    135\u001B[39m         \u001B[38;5;28mself\u001B[39m._response_closed()\n\u001B[32m--> \u001B[39m\u001B[32m136\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m exc\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpcore\\_sync\\http11.py:106\u001B[39m, in \u001B[36mHTTP11Connection.handle_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m     95\u001B[39m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[32m     97\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m Trace(\n\u001B[32m     98\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mreceive_response_headers\u001B[39m\u001B[33m\"\u001B[39m, logger, request, kwargs\n\u001B[32m     99\u001B[39m ) \u001B[38;5;28;01mas\u001B[39;00m trace:\n\u001B[32m    100\u001B[39m     (\n\u001B[32m    101\u001B[39m         http_version,\n\u001B[32m    102\u001B[39m         status,\n\u001B[32m    103\u001B[39m         reason_phrase,\n\u001B[32m    104\u001B[39m         headers,\n\u001B[32m    105\u001B[39m         trailing_data,\n\u001B[32m--> \u001B[39m\u001B[32m106\u001B[39m     ) = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_receive_response_headers\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    107\u001B[39m     trace.return_value = (\n\u001B[32m    108\u001B[39m         http_version,\n\u001B[32m    109\u001B[39m         status,\n\u001B[32m    110\u001B[39m         reason_phrase,\n\u001B[32m    111\u001B[39m         headers,\n\u001B[32m    112\u001B[39m     )\n\u001B[32m    114\u001B[39m network_stream = \u001B[38;5;28mself\u001B[39m._network_stream\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpcore\\_sync\\http11.py:177\u001B[39m, in \u001B[36mHTTP11Connection._receive_response_headers\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    174\u001B[39m timeout = timeouts.get(\u001B[33m\"\u001B[39m\u001B[33mread\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m    176\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m177\u001B[39m     event = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_receive_event\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    178\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(event, h11.Response):\n\u001B[32m    179\u001B[39m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001B[39m, in \u001B[36mHTTP11Connection._receive_event\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m    214\u001B[39m     event = \u001B[38;5;28mself\u001B[39m._h11_state.next_event()\n\u001B[32m    216\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m event \u001B[38;5;129;01mis\u001B[39;00m h11.NEED_DATA:\n\u001B[32m--> \u001B[39m\u001B[32m217\u001B[39m     data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_network_stream\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    218\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mREAD_NUM_BYTES\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\n\u001B[32m    219\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    221\u001B[39m     \u001B[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001B[39;00m\n\u001B[32m    222\u001B[39m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[32m    223\u001B[39m     \u001B[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    227\u001B[39m     \u001B[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001B[39;00m\n\u001B[32m    228\u001B[39m     \u001B[38;5;66;03m# it as a ConnectError.\u001B[39;00m\n\u001B[32m    229\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m data == \u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpcore\\_backends\\sync.py:128\u001B[39m, in \u001B[36mSyncStream.read\u001B[39m\u001B[34m(self, max_bytes, timeout)\u001B[39m\n\u001B[32m    126\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m map_exceptions(exc_map):\n\u001B[32m    127\u001B[39m     \u001B[38;5;28mself\u001B[39m._sock.settimeout(timeout)\n\u001B[32m--> \u001B[39m\u001B[32m128\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sock\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrecv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmax_bytes\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1295\u001B[39m, in \u001B[36mSSLSocket.recv\u001B[39m\u001B[34m(self, buflen, flags)\u001B[39m\n\u001B[32m   1291\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m flags != \u001B[32m0\u001B[39m:\n\u001B[32m   1292\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   1293\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mnon-zero flags not allowed in calls to recv() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m\"\u001B[39m %\n\u001B[32m   1294\u001B[39m             \u001B[38;5;28mself\u001B[39m.\u001B[34m__class__\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1295\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbuflen\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1296\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1297\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m().recv(buflen, flags)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1168\u001B[39m, in \u001B[36mSSLSocket.read\u001B[39m\u001B[34m(self, len, buffer)\u001B[39m\n\u001B[32m   1166\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sslobj.read(\u001B[38;5;28mlen\u001B[39m, buffer)\n\u001B[32m   1167\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1168\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sslobj\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m   1169\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m SSLError \u001B[38;5;28;01mas\u001B[39;00m x:\n\u001B[32m   1170\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m x.args[\u001B[32m0\u001B[39m] == SSL_ERROR_EOF \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.suppress_ragged_eofs:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "9684e553-90a5-401e-bbd4-ff1bde25fb2b",
   "metadata": {},
   "source": [
    "### LLM으로 영상자막요약 + 기사자막요약을 통해 등락 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "05e394fd-4a0c-4b3e-9976-76fcd09e4996",
   "metadata": {},
   "source": [
    "from predict_stock_outlook import predict_market_from_mix\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "llm_dir = 'predict_mix_gem' # preprocessed_data/llm/ 하위폴더명 (llm predict 저장될 곳)\n",
    "\n",
    "df_v = pd.read_csv('data_kr/video/뉴스 영상 수집본.csv', encoding='utf-8')\n",
    "df_v.rename(columns={'url':'v_url', 'upload_dt':'v_upload_dt'}, inplace=True)\n",
    "df_v = df_v[[\"year\",\"quarter\",\"month\",\"week\",\"code\",\"name\",\"sector\",\"after\",\"before\",\"v_url\",\"v_upload_dt\"]]\n",
    "\n",
    "df_a = pd.read_csv('data_kr/video/뉴스 기사 수집본.csv', encoding='utf-8')\n",
    "df_a.rename(columns={'url':'a_url', 'upload_dt':'a_upload_dt'}, inplace=True)\n",
    "df_a = df_a[[\"a_url\",\"a_upload_dt\"]]\n",
    "\n",
    "df = pd.concat([df_v, df_a], axis=1)\n",
    "\n",
    "for code in df[\"code\"].unique():\n",
    "    df_ = df[df[\"code\"] == code].reset_index(drop=True)\n",
    "\n",
    "    for row in tqdm(df_.itertuples(), total=len(df_), desc=f\"{code}LLM predicting\"):\n",
    "        if pd.isna(row.v_url) or pd.isna(row.a_url) :\n",
    "            continue\n",
    "        \n",
    "        code = str(row.code).zfill(6)\n",
    "        name = row.name\n",
    "        v_summary_dir = f'data_kr/video/script/{row.sector}/{code}/'\n",
    "        t_summary_dir = f'data_kr/video/text/{row.sector}/{code}/'\n",
    "        predict_dir = f'preprocessed_data/llm/{llm_dir}/{row.sector}/{code}/'\n",
    "        os.makedirs(predict_dir, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            filename = f'{row.year}-{row.quarter}-{str(row.month).zfill(2)}-{row.week}.txt'\n",
    "            stock = f'{name}({code})'\n",
    "            with open(f'{v_summary_dir}{filename}', \"r\", encoding=\"utf-8\") as file:\n",
    "                script = file.read()\n",
    "            with open(f'{t_summary_dir}{filename}', \"r\", encoding=\"utf-8\") as file:\n",
    "                article = file.read()\n",
    " \n",
    "            data = predict_market_from_mix(article, script, f'{name}({code})')\n",
    "            \n",
    "            with open(f'{predict_dir}{filename}', \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(data)\n",
    "            \n",
    "            with open(f'preprocessed_data/llm/{llm_dir}/log.txt', \"a\", encoding=\"utf-8\") as log_file:\n",
    "                timestamp = datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S]\")\n",
    "                log_file.write(f\"{timestamp} predict completed: {predict_dir}{filename}\\n\")\n",
    "        except Exception as e:\n",
    "            with open(f'preprocessed_data/llm/{llm_dir}/log.txt', \"a\", encoding=\"utf-8\") as log_file:\n",
    "                timestamp = datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S]\")\n",
    "                log_file.write(f\"{timestamp} predict error: {predict_dir}{filename}\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0a3a9a1b-c4ca-473d-917e-86af4d5e011c",
   "metadata": {},
   "source": [
    "### 기사 예측 결과 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "81c5e488-346e-4161-8b26-ee4ab9d49bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120LLM predicting: 100%|███████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 6100.98it/s]\n",
      "150LLM predicting: 100%|███████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 9762.54it/s]\n",
      "660LLM predicting: 100%|███████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 9780.40it/s]\n",
      "1120LLM predicting: 100%|██████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 6554.96it/s]\n",
      "3490LLM predicting: 100%|██████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 6959.18it/s]\n",
      "3550LLM predicting: 100%|██████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 9396.11it/s]\n",
      "3570LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 12940.49it/s]\n",
      "4710LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 13488.85it/s]\n",
      "5930LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 10176.72it/s]\n",
      "6260LLM predicting: 100%|██████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 8688.44it/s]\n",
      "6400LLM predicting: 100%|██████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 6802.44it/s]\n",
      "8060LLM predicting: 100%|██████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 6729.73it/s]\n",
      "9150LLM predicting: 100%|██████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 6978.53it/s]\n",
      "10120LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 7236.27it/s]\n",
      "11070LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 6473.38it/s]\n",
      "11200LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 6452.58it/s]\n",
      "18260LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 8623.38it/s]\n",
      "20150LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 6513.39it/s]\n",
      "25540LLM predicting: 100%|████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 15362.37it/s]\n",
      "29530LLM predicting: 100%|████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 20075.97it/s]\n",
      "34220LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 7425.54it/s]\n",
      "34730LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 7614.13it/s]\n",
      "42700LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 8045.04it/s]\n",
      "47050LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 6242.53it/s]\n",
      "47810LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 7419.17it/s]\n",
      "51600LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 6514.03it/s]\n",
      "66570LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 6603.22it/s]\n",
      "86280LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 7480.86it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "llm_dir = 'predict_text_gem' # preprocessed_data/llm/ 하위폴더명 (llm predict 저장될 곳)\n",
    "\n",
    "df=pd.read_csv('./data_kr/video/뉴스 기사 수집본.csv', encoding='utf-8')\n",
    "\n",
    "for code in df[\"code\"].unique():\n",
    "    df_ = df[df[\"code\"] == code].reset_index(drop=True)\n",
    "    score_list = []\n",
    "\n",
    "    for row in tqdm(df_.itertuples(), total=len(df_), desc=f\"{code}LLM predicting\"):\n",
    "        if pd.isna(row.url) or row.url == '':\n",
    "            score_list.append(None)\n",
    "            continue\n",
    "        \n",
    "        code = str(row.code).zfill(6)\t\n",
    "        name = row.name\n",
    "        predict_dir = f'preprocessed_data/llm/{llm_dir}/{row.sector}/{code}/'\n",
    "        os.makedirs(predict_dir, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            filename = f'{row.year}-{row.quarter}-{str(row.month).zfill(2)}-{row.week}.txt'\n",
    " \n",
    "            with open(f'{predict_dir}{filename}', 'r', encoding='utf-8') as f:\n",
    "                # json.load() 함수를 사용하여 파일의 내용을 파이썬 딕셔너리로 불러옵니다.\n",
    "                score = int(f.read())\n",
    "\n",
    "            score_list.append(score)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'error: {e}, file:{filename}')\n",
    "            score_list.append(0)\n",
    "            \n",
    "    df_predict = df_.copy()\n",
    "    df_predict[\"score\"] = score_list\n",
    "    df_predict = df_predict[[\"year\", \"quarter\", \"month\", \"week\", \"code\", \"name\", \"sector\", \"upload_dt\", \"score\"]]\n",
    "    df_predict.to_csv(f\"{predict_dir}{code}.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95054901-f78d-4671-87aa-9e6931a13628",
   "metadata": {},
   "source": [
    "### 영상 예측 결과 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b0b1cdce-09e6-4828-8434-465f7dff3442",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120LLM predicting: 100%|███████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 5416.56it/s]\n",
      "150LLM predicting: 100%|███████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 5343.40it/s]\n",
      "660LLM predicting: 100%|███████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 5132.62it/s]\n",
      "1120LLM predicting: 100%|██████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 8306.60it/s]\n",
      "3490LLM predicting: 100%|██████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 5458.58it/s]\n",
      "3550LLM predicting: 100%|██████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 5257.21it/s]\n",
      "3570LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 21223.29it/s]\n",
      "4710LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 32702.87it/s]\n",
      "5930LLM predicting: 100%|██████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 5011.41it/s]\n",
      "6260LLM predicting: 100%|██████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 9059.45it/s]\n",
      "6400LLM predicting: 100%|██████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 5882.21it/s]\n",
      "8060LLM predicting: 100%|██████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 5063.28it/s]\n",
      "9150LLM predicting: 100%|██████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 6441.36it/s]\n",
      "10120LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 6554.91it/s]\n",
      "11070LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 7563.72it/s]\n",
      "11200LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 8047.84it/s]\n",
      "18260LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 6596.44it/s]\n",
      "20150LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 7276.27it/s]\n",
      "25540LLM predicting: 100%|████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 38365.98it/s]\n",
      "29530LLM predicting: 100%|███████████████████████████████████████████████████████| 204/204 [00:00<00:00, 112067.85it/s]\n",
      "34220LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 5823.68it/s]\n",
      "34730LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 5407.83it/s]\n",
      "42700LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 7773.72it/s]\n",
      "47050LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 6937.68it/s]\n",
      "47810LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 5621.95it/s]\n",
      "51600LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 7615.01it/s]\n",
      "66570LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 5444.55it/s]\n",
      "86280LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 9483.80it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "llm_dir = 'predict_video_gem' # preprocessed_data/llm/ 하위폴더명 (llm predict 저장될 곳)\n",
    "\n",
    "df = pd.read_csv('./data_kr/video/뉴스 영상 수집본.csv', encoding='utf-8')\n",
    "\n",
    "for code in df[\"code\"].unique():\n",
    "    df_ = df[df[\"code\"] == code].reset_index(drop=True)\n",
    "    score_list = []\n",
    "    \n",
    "    for row in tqdm(df_.itertuples(), total=len(df_), desc=f\"{code}LLM predicting\"):\n",
    "        if pd.isna(row.url) or row.url == '':\n",
    "            score_list.append(None)\n",
    "            continue\n",
    "        \n",
    "        code = str(row.code).zfill(6)\t\n",
    "        name = row.name\n",
    "        predict_dir = f'preprocessed_data/llm/{llm_dir}/{row.sector}/{code}/'\n",
    "        os.makedirs(predict_dir, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            filename = f'{row.year}-{row.quarter}-{str(row.month).zfill(2)}-{row.week}.txt'\n",
    " \n",
    "            with open(f'{predict_dir}{filename}', 'r', encoding='utf-8') as f:\n",
    "                # json.load() 함수를 사용하여 파일의 내용을 파이썬 딕셔너리로 불러옵니다.\n",
    "                score = int(f.read())\n",
    "\n",
    "            score_list.append(score)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'error: {e}, file:{filename}')\n",
    "            score_list.append(0)\n",
    "            \n",
    "    df_predict = df_.copy()\n",
    "    df_predict[\"score\"] = score_list\n",
    "    df_predict = df_predict[[\"year\", \"quarter\", \"month\", \"week\", \"code\", \"name\", \"sector\", \"upload_dt\", \"score\"]]\n",
    "    df_predict.to_csv(f\"{predict_dir}{code}.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62be3001-6d4b-4a19-850f-604b6a9662a6",
   "metadata": {},
   "source": [
    "### 영상+기사 예측 결과 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "79d33f16-ee3b-4595-b61d-de7db9e728c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120LLM predicting: 100%|███████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 7016.07it/s]\n",
      "150LLM predicting: 100%|███████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 9830.96it/s]\n",
      "660LLM predicting: 100%|███████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 9465.02it/s]\n",
      "1120LLM predicting: 100%|██████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 9264.86it/s]\n",
      "3490LLM predicting: 100%|██████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 6868.95it/s]\n",
      "3550LLM predicting: 100%|██████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 9026.19it/s]\n",
      "3570LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 36462.88it/s]\n",
      "4710LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 69192.79it/s]\n",
      "5930LLM predicting: 100%|██████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 9096.53it/s]\n",
      "6260LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 12469.22it/s]\n",
      "6400LLM predicting: 100%|██████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 6967.11it/s]\n",
      "8060LLM predicting: 100%|██████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 6131.28it/s]\n",
      "9150LLM predicting: 100%|██████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 7408.12it/s]\n",
      "10120LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 8396.68it/s]\n",
      "11070LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 7334.21it/s]\n",
      "11200LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 8999.52it/s]\n",
      "18260LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 9507.30it/s]\n",
      "20150LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 8043.83it/s]\n",
      "25540LLM predicting: 100%|████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 58665.62it/s]\n",
      "29530LLM predicting: 100%|███████████████████████████████████████████████████████| 204/204 [00:00<00:00, 148265.12it/s]\n",
      "34220LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 8151.03it/s]\n",
      "34730LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 7317.46it/s]\n",
      "42700LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 9333.38it/s]\n",
      "47050LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 7655.83it/s]\n",
      "47810LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 7697.08it/s]\n",
      "51600LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 9880.92it/s]\n",
      "66570LLM predicting: 100%|█████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 7411.39it/s]\n",
      "86280LLM predicting: 100%|████████████████████████████████████████████████████████| 204/204 [00:00<00:00, 13503.11it/s]\n"
     ]
    }
   ],
   "source": [
    "from predict_stock_outlook import predict_market_from_mix\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "llm_dir = 'predict_mix_gem' # preprocessed_data/llm/ 하위폴더명 (llm predict 저장될 곳)\n",
    "\n",
    "df_v = pd.read_csv('data_kr/video/뉴스 영상 수집본.csv', encoding='utf-8')\n",
    "df_v.rename(columns={'url':'v_url', 'upload_dt':'v_upload_dt'}, inplace=True)\n",
    "df_v = df_v[[\"year\",\"quarter\",\"month\",\"week\",\"code\",\"name\",\"sector\",\"after\",\"before\",\"v_url\",\"v_upload_dt\"]]\n",
    "\n",
    "df_a = pd.read_csv('data_kr/video/뉴스 기사 수집본.csv', encoding='utf-8')\n",
    "df_a.rename(columns={'url':'a_url', 'upload_dt':'a_upload_dt'}, inplace=True)\n",
    "df_a = df_a[[\"a_url\",\"a_upload_dt\"]]\n",
    "\n",
    "df = pd.concat([df_v, df_a], axis=1)\n",
    "\n",
    "for code in df[\"code\"].unique():\n",
    "    df_ = df[df[\"code\"] == code].reset_index(drop=True)\n",
    "    score_list = []\n",
    "    uploaddt_list = []\n",
    "\n",
    "    for row in tqdm(df_.itertuples(), total=len(df_), desc=f\"{code}LLM predicting\"):\n",
    "        if pd.isna(row.v_url) or pd.isna(row.a_url) :\n",
    "            score_list.append(None)\n",
    "            uploaddt_list.append(None)\n",
    "            continue\n",
    "        \n",
    "        code = str(row.code).zfill(6)\n",
    "        name = row.name\n",
    "        predict_dir = f'preprocessed_data/llm/{llm_dir}/{row.sector}/{code}/'\n",
    "        os.makedirs(predict_dir, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            filename = f'{row.year}-{row.quarter}-{str(row.month).zfill(2)}-{row.week}.txt'\n",
    "            \n",
    "            with open(f'{predict_dir}{filename}', 'r', encoding='utf-8') as f:\n",
    "                # json.load() 함수를 사용하여 파일의 내용을 파이썬 딕셔너리로 불러옵니다.\n",
    "                score = int(f.read().splitlines()[0])\n",
    "\n",
    "            score_list.append(score)\n",
    "            if not pd.isna(row.v_upload_dt) and not pd.isna(row.a_upload_dt):\n",
    "                uploaddt = row.v_upload_dt if row.v_upload_dt > row.a_upload_dt else row.a_upload_dt\n",
    "                uploaddt_list.append(uploaddt)\n",
    "            else:\n",
    "                uploaddt_list.append(None)\n",
    "                \n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'error: {e}, file:{filename}')\n",
    "            score_list.append(0)\n",
    "            \n",
    "            \n",
    "    df_predict = df_.copy()\n",
    "    df_predict[\"score\"] = score_list\n",
    "    df_predict['upload_dt'] = uploaddt_list\n",
    "    df_predict = df_predict[[\"year\", \"quarter\", \"month\", \"week\", \"code\", \"name\", \"sector\", \"upload_dt\", \"score\"]]\n",
    "    df_predict.to_csv(f\"{predict_dir}{code}.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08186f40-9496-4e66-9322-c5098757571d",
   "metadata": {},
   "source": [
    "## 예측 결과 정리 concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b8db379c-233c-48ec-a740-d938881d3694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "# 현재 폴더를 Path 객체로 만듭니다.\n",
    "datas=['video', 'text', 'mix']\n",
    "llm_dirs=[f'predict_{data}_gem' for data in datas]\n",
    "\n",
    "for data, llm_dir in zip(datas, llm_dirs):\n",
    "    path_list = []\n",
    "    for sector in [\"산업재\", \"정보기술\"]:\n",
    "        root_path = Path(f'./preprocessed_data/llm/{llm_dir}/{sector}')\n",
    "        \n",
    "        # rglob('*')는 재귀적으로 모든(*) 파일과 폴더를 찾습니다.\n",
    "        for path in root_path.rglob('*.csv'):\n",
    "            # path는 파일 또는 폴더에 대한 Path 객체입니다.\n",
    "            path_list.append(path)\n",
    "    df_list = [pd.read_csv(f'{path}', encoding='utf-8') for path in path_list]\n",
    "    df = pd.concat(df_list, axis=0)\n",
    "    df = df.sort_values(by=['code', 'year', 'quarter', 'month', 'week'], ascending=True)\n",
    "    df.to_csv(f'./preprocessed_data/llm/{llm_dir}/predict.csv', index=False, encoding='utf-8')\n",
    "    df.to_csv(f'./preprocessed_data/llm/predict_total/{llm_dir}.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebf8014-2226-421d-b3e7-552eb0776904",
   "metadata": {},
   "source": [
    "## mix에 video or text 단독예측도 추가하여 predict_total 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "54019e35-233a-4065-898d-8f5244976108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "## 파일경로 알아서 바꾸기\n",
    "\n",
    "df_v = pd.read_csv('preprocessed_data/llm/predict_total/predict_video_gem.csv', encoding='utf-8')\n",
    "df_t = pd.read_csv('preprocessed_data/llm/predict_total/predict_text_gem.csv', encoding='utf-8')\n",
    "df_m = pd.read_csv('preprocessed_data/llm/predict_total/predict_mix_gem.csv', encoding='utf-8')\n",
    "\n",
    "df_total = df_m.copy()\n",
    "df_total[pd.isna(df_total)] = df_v[pd.isna(df_total)]\n",
    "df_total[pd.isna(df_total)] = df_t[pd.isna(df_total)]\n",
    "\n",
    "df_total.to_csv('preprocessed_data/llm/predict_total/predict_total_gem.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a07fb5d-bd32-4a5e-bc31-4a1fc0de3100",
   "metadata": {},
   "source": [
    "## LLM 정확도 측정 (업로드일과 일주일 마감일 가격 비교) & 정확도 지표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "33a9ceff-bdc7-4ffb-9cfb-7ee6f19325f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 5712/5712 [00:25<00:00, 225.87it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5712/5712 [00:25<00:00, 226.92it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5712/5712 [00:23<00:00, 240.79it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5712/5712 [00:25<00:00, 227.33it/s]\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta, datetime\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "def get_pred_accuracy():\n",
    "    scores = dict()\n",
    "    \n",
    "    plt.figure(figsize=(24, 5))\n",
    "    \n",
    "    datas = ['video', 'text', 'mix', 'total']\n",
    "    llm_dirs=[f'predict_{data}_gem' for data in datas]\n",
    "    for idx, (data, llm_dir) in enumerate(zip(datas, llm_dirs)):\n",
    "        df = pd.read_csv(f'preprocessed_data/llm/predict_total/{llm_dir}.csv', encoding='utf-8')\n",
    "        df_d = pd.read_csv(f'data_kr/video/뉴스 기사 수집본.csv', encoding='utf-8')[['after', 'before']]\n",
    "        df = pd.concat([df, df_d], axis=1)\n",
    "        df[['upload_dt', 'after', 'before']] = df[['upload_dt', 'after', 'before']].apply(pd.to_datetime)\n",
    "        \n",
    "        ### LLM 예측에 실제 등락 라벨 추가 ###\n",
    "        df[\"code\"] = df[\"code\"].astype(str).str.zfill(6)\n",
    "        \n",
    "        price_upload = [] # 업로드 당일 종가\n",
    "        price_end = [] # 공시 당일 종가\n",
    "        for row in tqdm(df.itertuples(), total=len(df)):\n",
    "            df_price = pd.read_csv(f\"data_kr/price/{row.code}.csv\")\n",
    "            df_price['날짜'] = pd.to_datetime(df_price['날짜'])\n",
    "            ### 업로드 날짜 직전 종가\n",
    "            price_upload.append(df_price.loc[df_price[\"날짜\"] < row.upload_dt, \"종가\"].iloc[-1] if not pd.isna(row.upload_dt) else None)\n",
    "            ### 데이터 업로드 주간 마지막 날 종가\n",
    "            # price_end.append(df_price.loc[df_price[\"날짜\"] < row.before, \"종가\"].iloc[-1] if not pd.isna(row.before) else None)\n",
    "            ### 데이터 업로드 다음 날 종가\n",
    "            price_end.append(df_price.loc[df_price[\"날짜\"] > row.upload_dt, \"종가\"].iloc[0] if not pd.isna(row.upload_dt) else None)\n",
    "        \n",
    "        df[\"price_upload\"] = price_upload\n",
    "        df[\"price_end\"] = price_end\n",
    "    \n",
    "        def check_change(row):\n",
    "            rate = (row[\"price_end\"] / row[\"price_upload\"] - 1) * 100\n",
    "            if rate > 0:\n",
    "                return \"상승\"\n",
    "            else:\n",
    "                return \"하락\"\n",
    "        df[\"label\"] = df.apply(check_change, axis=1)\n",
    "\n",
    "        ###############################################################\n",
    "        ### 유효한 데이터만 남기기 ###\n",
    "        valid_bool = ~(pd.isna(df['score']) | (df['score'] == 0))\n",
    "        prediction = df[valid_bool]['score']\n",
    "        label = df[valid_bool]['label']\n",
    "        \n",
    "        mapping = {1: '상승', 0: '하락', -1: '하락'}\n",
    "        prediction = [mapping[l] for l in prediction]\n",
    "\n",
    "        accuracy = accuracy_score(label, prediction)\n",
    "        precision = precision_score(label, prediction, pos_label='상승', zero_division=0)\n",
    "        recall = recall_score(label, prediction, pos_label='상승', zero_division=0)\n",
    "        f1 = f1_score(label, prediction, pos_label='상승', zero_division=0)\n",
    "\n",
    "        score_dict = {'accuracy': accuracy, 'precision':precision, 'recall':recall, 'f1':f1}\n",
    "        scores[data] = score_dict\n",
    "        \n",
    "        # 혼동 행렬\n",
    "        labels = [\"상승\", \"하락\"]\n",
    "        cm = confusion_matrix(label, prediction, labels=labels)\n",
    "        \n",
    "        # 한글 폰트 설정 (Windows, Mac, Linux 환경에 맞게 설정)\n",
    "        # 윈도우\n",
    "        plt.rc('font', family='Malgun Gothic')\n",
    "    \n",
    "        # 시각화\n",
    "        plt.subplot(1, 4, idx + 1)  # 1행 3열 중 i번째 위치에 subplot을 생성\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "        plt.xlabel('Prediction')\n",
    "        plt.ylabel('Label')\n",
    "        plt.title(f'{data} Accuracy Confusion Matrix')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"preprocessed_data/llm/predict_total/predict_accuracy_after1_gem.png\")\n",
    "    plt.close()\n",
    "    pd.DataFrame(scores).to_csv(f\"preprocessed_data/llm/predict_total/predict_accuracy_after1_gem.csv\")\n",
    "\n",
    "get_pred_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcbfe49-8fc9-4fd5-b766-5c39bff46c11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s3ce_kernel",
   "language": "python",
   "name": "s3ce"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
