from dotenv import load_dotenv
import os
from pathlib import Path
from datetime import datetime, timedelta

import torch
from googleapiclient.discovery import build
import yt_dlp
import whisper
import re
from tqdm import tqdm

import google.generativeai as genai
from langchain.chat_models import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage

import pandas as pd

load_dotenv()  # .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
YOUTUBE_API_KEY = os.getenv("YOUTUBE_API_KEY")

# whisper ëª¨ë¸ ë¡œë“œ (base, small, medium, large ì¤‘ ì„ íƒ ê°€ëŠ¥)
device = "cuda" if torch.cuda.is_available() else "cpu"
whisper_model = whisper.load_model("medium").to(device)

# gemini ëª¨ë¸ ë¡œë“œ
genai.configure(api_key=GEMINI_API_KEY)
gemini_model = genai.GenerativeModel("gemini-2.0-flash")

# GPT ëª¨ë¸ ë¡œë“œ
gpt_model = ChatOpenAI(temperature=0, model="gpt-4o", openai_api_key=OPENAI_API_KEY)

# YOUTUBE ë¹Œë“œ
youtube = build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)

# -----------------------------
# ìœ íŠœë¸Œ ì±„ë„ëª…ìœ¼ë¡œ ì±„ë„ID ì¶”ì¶œ
# ì…ë ¥ ì˜ˆ: "í•œêµ­ê²½ì œTV" ë˜ëŠ” "@wowtv"
# ì¶œë ¥: ì±„ë„ ID
# -----------------------------
def get_channel_id(channel_name):
	res = youtube.search().list(
		q=channel_name,
		type="channel",
		part="snippet",
		maxResults=1
	).execute()

	# ê²°ê³¼ì—ì„œ ì±„ë„ ID ì¶”ì¶œ
	if res.get("items"):
		channel_id = res["items"][0]["snippet"]["channelId"]
		print("ğŸ“º ì±„ë„ ID:", channel_id)
	else:
		print("ì±„ë„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

	return channel_id

# -----------------------------
# ì±„ë„ IDë¡œ ì—…ë¡œë“œ playlist ID ì–»ê¸°
# ì…ë ¥: channel id
# ì¶œë ¥: playlist id
# -----------------------------
def get_uploads_playlist_id(channel_id):
    res = youtube.channels().list(
        part="contentDetails",
        id=channel_id
    ).execute()
    return res["items"][0]["contentDetails"]["relatedPlaylists"]["uploads"]

# -----------------------------
# playlist IDë¡œ í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ ë‚´ ëª¨ë“  video IDì™€ ì—…ë¡œë“œ ë‚ ì§œ ì–»ê¸°
# ì…ë ¥: playlist id
# ì¶œë ¥: type: íŠœí”Œ ë¦¬ìŠ¤íŠ¸  ex) [(id1, date1, ì¡°íšŒìˆ˜1), (id2, date2, ì¡°íšŒìˆ˜2) ...]
# -----------------------------
def get_video_datas_from_playlist(playlist_id):
    video_list = []
    next_page_token = None
    video_id_date_pairs = []

    # 1. playlistItems APIë¡œ video ID + ì—…ë¡œë“œ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°
    while True:
        res = youtube.playlistItems().list(
            part="snippet",
            playlistId=playlist_id,
            maxResults=50,
            pageToken=next_page_token
        ).execute()

        for item in res["items"]:
            snippet = item["snippet"]
            video_id = snippet["resourceId"]["videoId"]
            published_at = snippet["publishedAt"]
            video_id_date_pairs.append((video_id, published_at))

        next_page_token = res.get("nextPageToken")
        if not next_page_token:
            break

    # 2. video IDë¡œ ì¡°íšŒìˆ˜ ê°€ì ¸ì˜¤ê¸°
    for i in range(0, len(video_id_date_pairs), 50):
        batch = video_id_date_pairs[i:i+50]
        ids_only = [vid for vid, _ in batch]

        res = youtube.videos().list(
            part="statistics",
            id=",".join(ids_only)
        ).execute()

        stats = {item["id"]: int(item["statistics"].get("viewCount", 0)) for item in res["items"]}

        # 3. íŠœí”Œë¡œ ì €ì¥: (video_id, published_at, view_count)
        for video_id, published_at in batch:
            view_count = stats.get(video_id, 0)
            video_list.append((video_id, published_at, view_count))

    return video_list

# -----------------------------
# video idsë¡œ ì—…ë¡œë“œ ë‚ ì§œ í•„í„°ë§í•˜ê¸°
# ì…ë ¥: video_datas(type íŠœí”Œ ë¦¬ìŠ¤íŠ¸), ì‹œì‘ë‚ ì§œ, ëë‚ ì§œ ("2001-04-30" í˜•ì‹ìœ¼ë¡œ ì…ë ¥)
# ì¶œë ¥: í•„í„°ë§ ëœ video_datas(type íŠœí”Œ ë¦¬ìŠ¤íŠ¸)
# -----------------------------
def filter_by_date(video_datas, start_date, end_date):
    # ë¬¸ìì—´ â†’ datetime ê°ì²´ë¡œ ë³€í™˜
    start_dt = datetime.strptime(start_date + "T00:00:00Z", "%Y-%m-%dT%H:%M:%SZ")
    end_dt = datetime.strptime(end_date + "T00:00:00Z", "%Y-%m-%dT%H:%M:%SZ")

    filtered = []
    for video_id, published_at, view_count in video_datas:
        pub_date = datetime.strptime(published_at, "%Y-%m-%dT%H:%M:%SZ")
        if start_dt <= pub_date < end_dt:
            filtered.append((video_id, published_at, view_count))

    return filtered
    
# -----------------------------
# ì¡°íšŒìˆ˜ ê¸°ì¤€ í•„í„°ë§
# ì…ë ¥: video_datas(type íŠœí”Œ ë¦¬ìŠ¤íŠ¸), ìµœì†Œì¡°íšŒìˆ˜
# ì¶œë ¥: video_datas(type íŠœí”Œ ë¦¬ìŠ¤íŠ¸)
# -----------------------------
def filter_videos_by_view_count(video_tuples, min_views=0, max_views=float("inf")):
    return [
        (video_id, published_at, view_count)
        for video_id, published_at, view_count in video_tuples
        if min_views <= view_count <= max_views
    ]
    
# -----------------------------
# ì±„ë„ì˜ìƒì„ ë‚ ì§œ, ì¡°íšŒìˆ˜ ê¸°ì¤€ í•„í„°ë§
# ì…ë ¥: channel_id, ê¸°ê°„, ìµœì†Œì¡°íšŒìˆ˜
# ì¶œë ¥: video_datas(type íŠœí”Œ ë¦¬ìŠ¤íŠ¸)
# -----------------------------
def get_filtered_videos_by_channel(channel_id, start_date, end_date, min_views=0):
    video_results = []
    next_page_token = None

    # ë‚ ì§œ ë¬¸ìì—´ â†’ datetime ê°ì²´ â†’ ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    start_iso = (datetime.strptime(start_date, "%Y-%m-%d") - timedelta(days=7)).isoformat("T") + "Z"
    end_iso = datetime.strptime(end_date, "%Y-%m-%d").isoformat("T") + "Z"

    # 1. search().list() â†’ video ID + publishedAt
    temp_videos = []
    while True:
        res = youtube.search().list(
            part="snippet",
            channelId=channel_id,
            publishedAfter=start_iso,
            publishedBefore=end_iso,
            maxResults=50,
            pageToken=next_page_token,
            type="video",
            order="date"
        ).execute()

        for item in res["items"]:
            video_id = item["id"]["videoId"]
            published_at = item["snippet"]["publishedAt"]
            temp_videos.append((video_id, published_at))

        next_page_token = res.get("nextPageToken")
        if not next_page_token:
            break

    # 2. videos().list() â†’ ì¡°íšŒìˆ˜ ë¶™ì´ê¸°
    for i in range(0, len(temp_videos), 50):
        batch = temp_videos[i:i+50]
        ids = [vid for vid, _ in batch]

        res = youtube.videos().list(
            part="statistics",
            id=",".join(ids)
        ).execute()

        stats = {
            item["id"]: int(item["statistics"].get("viewCount", 0))
            for item in res["items"]
        }

        for video_id, published_at in batch:
            view_count = stats.get(video_id, 0)
            if view_count >= min_views:
                video_results.append((video_id, published_at, view_count))

    return video_results

# -----------------------------
# ìœ íŠœë¸Œ ì˜ìƒ ê²€ìƒ‰
# date ì´ì „ì˜ query ê²€ìƒ‰ ê²°ê³¼ë“¤ë§Œ ë³´ì—¬ì¤Œ 
# dateëŠ” '2025-04-11' í˜•ì‹ìœ¼ë¡œ ì…ë ¥
# -----------------------------
def search_videos(query, date):
	date += 'T00:00:00Z' # ISO 8601 í˜•ì‹ìœ¼ë¡œ ë³€ê²½

	# Step 1: ê²€ìƒ‰
	search_res = youtube.search().list(
		q=query,
		part='snippet',
		type='video',
		maxResults=50,
		order='date',
		publishedBefore=date
	).execute()
	
	# Step 2: videoId ìˆ˜ì§‘
	video_ids = [item['id']['videoId'] for item in search_res['items']]
	if not video_ids:
		print("ì¡°ê±´ì— ë§ëŠ” ì˜ìƒ ì—†ìŒ")

	return video_ids

# -----------------------------
# ìœ íŠœë¸Œ ë§í¬ or idì—ì„œ ìŒì„±íŒŒì¼ ì¶”ì¶œ
# INPUT: method(link or id), youtube id or link, ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ê²½ë¡œ
# -----------------------------
def extract_video_audio(method, video_id, audio_dir):
	os.makedirs('./audio', exist_ok=True)
	
	if method == "link":
		url = video_id
	elif method == "id":
		url = "https://www.youtube.com/watch?v=" + video_id
	else:
		print('methodì¸ìë¡œ link or idë¥¼ ì…ë ¥í•˜ì„¸ìš”')
		return False

	ydl_opts = {
		'format': 'bestaudio/best',
		'outtmpl': f'{audio_dir}.%(ext)s',
		'postprocessors': [{
			'key': 'FFmpegExtractAudio',
			'preferredcodec': 'mp3',
		}]
	}

	try:
		with yt_dlp.YoutubeDL(ydl_opts) as ydl:
			ydl.download([url])
		print("ìŒì„±íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì„±ê³µ")
		return True
	except:
		print("ìŒì„±íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
		return False

# -----------------------------
# ìŒì„±íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
# INPUT: youtube id, ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ê²½ë¡œ
# -----------------------------
def audio2text(audio_dir):
	# ìŒì„± íŒŒì¼ STT ìˆ˜í–‰
	result = whisper_model.transcribe(f'{audio_dir}.mp3', language="ko")  # wav, mp4 ë“±ë„ OK

	# í…ìŠ¤íŠ¸ ì¶œë ¥
	print(result["text"])

	return result["text"]

# -----------------------------
# ìë§‰ ì „ì²˜ë¦¬ í•¨ìˆ˜
# -----------------------------
def clean_srt(srt_text: str) -> str:
	srt_text = re.sub(r"(ì¢€|ê·¸ëƒ¥|ë­ë„ê¹Œ|ê·¸ëŸ¬ë‹ˆê¹Œ|ì•„ë‹ˆ|ì•½ê°„|ë­”ê°€|ë­ëƒë©´ìš”)", "", srt_text)
	return srt_text


# ------------------------
# GPT-4o ìš”ì•½
# ------------------------
def summarize_text(text: str, stock: str) -> str:
	system_prompt = """
ë„ˆëŠ” ê²½ì œ ì „ë¬¸ ë‰´ìŠ¤ ë¶„ì„ AIì•¼. ì‚¬ìš©ìê°€ ì§€ì •í•œ ì¢…ëª©(íšŒì‚¬ëª…)ê³¼ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ì •ë³´ë§Œ ì„ íƒí•´ í•µì‹¬ì ìœ¼ë¡œ ìš”ì•½í•´.
ì‚¬ì‹¤ ê¸°ë°˜ìœ¼ë¡œ ìš”ì•½í•˜ê³ , ê°ì„±ì´ë‚˜ ì¶”ë¡ ì´ í•„ìš”í•œ ê²½ìš°ì—ëŠ” ì¤‘ë¦½ì ìœ¼ë¡œ í‘œí˜„í•´.
"""

	user_prompt = f"""
ë‹¤ìŒì€ ê²½ì œ ë‰´ìŠ¤ ê¸°ì‚¬ì…ë‹ˆë‹¤.

ì´ ê¸°ì‚¬ì—ì„œ **í•œêµ­ ìƒì¥ ê¸°ì—… "{stock}"**ê³¼ ê´€ë ¨ëœ ë‚´ìš©ë§Œ ê³¨ë¼ ìš”ì•½í•´ ì£¼ì„¸ìš”.

ìš”ì•½ ê¸°ì¤€:
- "{stock}"ì´ ì–¸ê¸‰ëœ ë¶€ë¶„ ì¤‘ì‹¬
- ê´€ë ¨ ì‚¬ì—…, ì‹¤ì , ì£¼ê°€, ì‹œì¥ ë°˜ì‘, ê²½ìŸì‚¬ì™€ì˜ ì—°ê´€ì„±
- ì •ë¶€ ì •ì±…, ì‚°ì—… íŠ¸ë Œë“œ ë“± ì™¸ë¶€ ìš”ì¸ ì¤‘ ê´€ë ¨ ìˆëŠ” ë¶€ë¶„
- ë¶€ì •ì /ê¸ì •ì  ë…¼ì¡°ë„ ê°„ë‹¨íˆ ì–¸ê¸‰ (ìˆëŠ” ê²½ìš°)

í˜•ì‹ì€ ê°„ê²°í•œ ë¬¸ì¥ ë˜ëŠ” Bullet Point í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.

ê¸°ì‚¬ ì „ë¬¸:
{text}
"""

	response = gpt_model([
		SystemMessage(content=system_prompt.strip()),
		HumanMessage(content=user_prompt.strip())
	])
	return response.content.strip()



# ------------------------
# GPT-4o ë“±ë½ ì˜ˆì¸¡
# ------------------------
def predict_market_from_summary(summary: str, stock: str) -> str:
	system_prompt = """
ë„ˆëŠ” ê²½ì œ ë‰´ìŠ¤ ìš”ì•½ì„ ë°”íƒ•ìœ¼ë¡œ ì£¼ì‹ ì¢…ëª©ì˜ ë‹¨ê¸° ë“±ë½ ê°€ëŠ¥ì„±ì„ íŒë‹¨í•˜ëŠ” ë¶„ì„ AIì•¼.
ì£¼ì˜: ì˜¤ì§ ì‚¬ìš©ìì—ê²Œ ì£¼ì–´ì§„ ìš”ì•½ë³¸ë§Œ ë³´ê³  íŒë‹¨í•´ì•¼ í•˜ë©°, ì™¸ë¶€ ì§€ì‹ì´ë‚˜ ê¸°ì‚¬ ì›ë¬¸ì€ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆ.
ì ìˆ˜ëŠ” -2 ~ +2 ì‚¬ì´ë¡œ íŒë‹¨í•˜ê³ , ë…¼ì¡°ì— ëŒ€í•œ ê·¼ê±°ë„ ê°„ë‹¨íˆ ì‘ì„±í•´.
"""

	user_prompt = f"""
ë‹¤ìŒì€ í•œêµ­ ìƒì¥ ê¸°ì—… "{stock}"ê³¼ ê´€ë ¨ëœ ë‰´ìŠ¤ ìš”ì•½ì…ë‹ˆë‹¤.

ìš”ì•½ë³¸ì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ "{stock}"ì˜ ë‹¨ê¸° ì£¼ê°€ ë“±ë½ ì „ë§ì„ ì ìˆ˜ë¡œ í‰ê°€í•´ ì£¼ì„¸ìš”.

â—ï¸**ìš”ì•½ë¬¸ì— ë‚˜íƒ€ë‚œ ì •ë³´ë§Œì„ ê·¼ê±°ë¡œ íŒë‹¨í•´ì•¼ í•˜ë©°, ê¸°ì‚¬ ì›ë¬¸ì´ë‚˜ ë°°ê²½ì§€ì‹ì€ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.**

ì¶œë ¥ í˜•ì‹:
- ë…¼ì¡° íŒë‹¨: ê¸ì •ì  / ë¶€ì •ì  / ì¤‘ë¦½
- íŒë‹¨ ê·¼ê±°: (ë…¼ì¡° íŒë‹¨ì˜ ê·¼ê±°ë¥¼ ì‘ì„±)
- ë“±ë½ ì „ë§ ì ìˆ˜ (ìˆ«ìë§Œ):  
  - +2: ê°•í•œ ìƒìŠ¹  
  - +1: ë‹¤ì†Œ ìƒìŠ¹  
  -  0: ì¤‘ë¦½ / ì˜í–¥ ì—†ìŒ  
  - -1: ë‹¤ì†Œ í•˜ë½  
  - -2: ê°•í•œ í•˜ë½  
  
ì¶œë ¥ ì˜ˆì‹œ:
- ë…¼ì¡° íŒë‹¨: ê¸ì •ì 
- íŒë‹¨ ê·¼ê±°: ê¸°ì‚¬ ìš”ì•½ì—ì„œ í•´ë‹¹ ê¸°ì—…ì´ ë¯¸êµ­ ëŒ€í˜• ì „ê¸°ì°¨ ì—…ì²´ì™€ ì‹ ê·œ ë°°í„°ë¦¬ ê³µê¸‰ ê³„ì•½ì„ ì²´ê²°í–ˆê³ , ìˆ˜ì¶œ í™•ëŒ€ì™€ ì‹¤ì  ê°œì„ ì— ëŒ€í•œ ê¸°ëŒ€ê°ì´ ì–¸ê¸‰ë˜ì–´ ê¸ì •ì ì¸ ë…¼ì¡°ë¡œ íŒë‹¨ë¨
- ë“±ë½ ì „ë§ ì ìˆ˜: +1

ìš”ì•½ë³¸:
{summary}
"""

	response = gpt_model([
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_prompt)
    ])
	return response.content.strip()


# ------------------------
# ì•ì˜ ëª¨ë“  í•¨ìˆ˜ë¥¼ ì´ìš©í•œ ìµœì¢… í•¨ìˆ˜
#
# ì£¼ì‹ê³¼ ë‚ ì§œ ì…ë ¥í•˜ë©´ ì£¼ê°€ì „ë§ ì˜ˆì¸¡
# dateëŠ” '2025-04-11' í˜•ì‹ìœ¼ë¡œ ì…ë ¥
# ------------------------
def predict_market(stock: str, date: str) -> str:
	SEARCH_QUERY = f'{stock} ì£¼ê°€ì „ë§'
	BEFORE_DATE = date
	print("SEARCH_QUERY:", SEARCH_QUERY)
	video_id = search_videos(SEARCH_QUERY, BEFORE_DATE)
	if video_id == None:
		return "middle"

	# ìœ íŠœë¸Œ ì˜ìƒ ìŒì„± ì¶”ì¶œ
	audio_dir = f'audio/{stock}_{BEFORE_DATE}'
	if not extract_video_audio(video_id, audio_dir):
		return "middle"
	text = audio2text(audio_dir)

	# ìë§‰ ì „ì²˜ë¦¬
	cleaned = clean_srt(text)
	print("ì „ì²˜ë¦¬:\n", cleaned)

	# ì „ì²˜ë¦¬ ìë§‰ ìš”ì•½
	summary = summarize_text(cleaned)

	# ì£¼ì‹ ë“±ë½ ì˜ˆì¸¡
	prediction = predict_market_from_summary(summary, stock)

	print("\nğŸ“„ GEMINI ìš”ì•½ ê²°ê³¼:\n", summary)
	print("\nğŸ“ˆ GPT-4 ì˜ˆì¸¡ ê²°ê³¼:\n\n", prediction)

	if prediction == 'ì˜¤ë¥¼ ê°€ëŠ¥ì„± ìˆìŒ':
		return 'up'
	elif prediction == 'ì˜¤ë¥¼ ê°€ëŠ¥ì„± ë‚®ìŒ':
		return 'down'
	else:
		return 'middle'




# ------------------------
# ëª¨ë“  ë¶„ê¸°ì˜ ë²”ìœ„ êµ¬í•˜ê¸°
# disclosure_date_range.csv íŒŒì¼ ìƒì„±
# ------------------------
def get_disclosure_range():
    # ëª¨ë“  ì¢…ëª©ì˜ ëª¨ë“  ë¶„ê¸° ê³µì‹œì¼ì„ í•˜ë‚˜ì˜ íŒŒì¼ë¡œ
	root_path = Path('./data_kr/merged')
	all_symbols_disclosure = pd.DataFrame()
	for file_path in root_path.rglob("*.csv"):
		df_ = pd.read_csv(file_path)
		df_ = df_[["code", "name", "year", "quarter", "disclosure_date"]]
		all_symbols_disclosure = pd.concat([all_symbols_disclosure, df_])


	years = [2015] + ([y for y in range(2016, 2025) for _ in range(4)])
	quarters = ["Q4"] + ([q for _ in range(2016, 2025) for q in ["Q1", "Q2", "Q3", "Q4"]])
	df_disclosure = pd.DataFrame({
		"year": years,
		"quarter": quarters,
		"min_disclosure_date": [None] * len(years),
		"max_disclosure_date": [None] * len(years)
	})

	for i, row in enumerate(df_disclosure.itertuples()):
		disclosures = all_symbols_disclosure[(all_symbols_disclosure["year"] == row.year) & (all_symbols_disclosure["quarter"] == row.quarter)]["disclosure_date"]
		df_disclosure.loc[i, "min_disclosure_date"] = disclosures.min()
		df_disclosure.loc[i, "max_disclosure_date"] = disclosures.max()

	os.makedirs('./data_kr/audio', exist_ok=True)
	df_disclosure.to_csv("./data_kr/audio/disclosure_date_range.csv", index=False)
 
# ------------------------
# disclosure rangeë¥¼ ë§Œì¡±í•˜ê³  ì¡°íšŒìˆ˜ê°€ min_view_cnt ì´ìƒì¸ video id êµ¬í•˜ê¸°
# channel_name: str
# min_view_cnt: int
# data_kr/audoi/ì— ì—°ë„-ë¶„ê¸°.csv íŒŒì¼ ìƒì„±
# ------------------------
def get_video_datas(channel_name, min_view_cnt):
	channel_id = get_channel_id(channel_name) 
	dir = f'data_kr/audio/{channel_name}'
	os.makedirs(dir, exist_ok=True)
 
	df_disclosure = pd.read_csv('data_kr/audio/disclosure_date_range.csv')
	for row in df_disclosure.itertuples():
		start = datetime.strptime(row.min_disclosure_date, "%Y-%m-%d")
		start -= timedelta(days=7)
		start = start.strftime("%Y-%m-%d")
		end = row.max_disclosure_date

		video_datas = get_filtered_videos_by_channel(channel_id, start, end, min_view_cnt)
		year_quarter = f'{row.year}-{row.quarter}'
		os.makedirs(f'{dir}/{year_quarter}', exist_ok=True)
		pd.DataFrame(video_datas, columns=['video_id', 'published_at', 'view_count']).to_csv(f'{dir}/{year_quarter}/{year_quarter}.csv', index=False)
    
if __name__ == "__main__":
	# df = pd.read_csv("data_kr/video/ìë£Œ ìˆ˜ì§‘ ìµœì¢…ë³¸.csv")
	
	# ### ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ###
	# for row in df.itertuples():
	# 	if pd.isna(row.url) or row.url == '' or row.category != "video":
	# 		continue

	# 	code = str(row.code).zfill(6)
	# 	audio_dir = f'data_kr/video/audio/{row.sector}/{code}/'
	# 	text_dir = f'data_kr/video/text/{row.sector}/{code}/'
	# 	os.makedirs(audio_dir, exist_ok=True)

	# 	if audio_dir + f'{row.year}-{row.quarter}' != 'data_kr/video/audio/ì‚°ì—…ì¬/003490/2016-Q2':
	# 		continue
		
	# 	if extract_video_audio("link", row.url, audio_dir + f'{row.year}-{row.quarter}'):
	# 		with open('data_kr/video/log.txt', "a", encoding="utf-8") as log_file:
	# 			timestamp = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
	# 			log_file.write(f"{timestamp} audio download completed: {audio_dir + f'{row.year}-{row.quarter}'}\n")
	# 	else:
	# 		with open('data_kr/video/log.txt', "a", encoding="utf-8") as log_file:
	# 			timestamp = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
	# 			log_file.write(f"{timestamp} audio download error: {audio_dir + f'{row.year}-{row.quarter}'}\n")
	
	# ### í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ ###
	# for row in df.itertuples():
	# 	if pd.isna(row.url) or row.url == '' or row.category != "video":
	# 		continue

	# 	code = str(row.code).zfill(6)
	# 	audio_dir = f'data_kr/video/audio/{row.sector}/{code}/'
	# 	text_dir = f'data_kr/video/text/{row.sector}/{code}/'
	# 	os.makedirs(text_dir, exist_ok=True)
		
	# 	if audio_dir + f'{row.year}-{row.quarter}' != 'data_kr/video/audio/ì‚°ì—…ì¬/003490/2016-Q2':
	# 		continue
		
	# 	try:
	# 		text = audio2text(audio_dir + f'{row.year}-{row.quarter}')
	# 		with open(text_dir + f'{row.year}-{row.quarter}.txt', "w", encoding="utf-8") as f:
	# 			f.write(text)
	# 		with open('data_kr/video/log.txt', "a", encoding="utf-8") as log_file:
	# 			timestamp = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
	# 			log_file.write(f"{timestamp} whisper completed: {text_dir + f'{row.year}-{row.quarter}'}\n")
	# 	except Exception as e:
	# 		with open('data_kr/video/log.txt', "a", encoding="utf-8") as log_file:
	# 			timestamp = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
	# 			log_file.write(f"{timestamp} whisper error: {text_dir + f'{row.year}-{row.quarter}'}\n")

	# ### í…ìŠ¤íŠ¸ token ìˆ˜ í™•ì¸  ###
	# import tiktoken
	# # ì˜ˆ: GPT-4ìš© ì¸ì½”ë” ë¶ˆëŸ¬ì˜¤ê¸°
	# encoding = tiktoken.encoding_for_model("gpt-4o")
	# total_tokens = 0

	# for row in tqdm(df.itertuples(), total=len(df), desc="checking tokens"):
	# 	if pd.isna(row.url) or row.url == '':
	# 		continue

	# 	code = str(row.code).zfill(6)
	# 	text_dir = f'data_kr/video/text/{row.sector}/{code}/'
	# 	os.makedirs(text_dir, exist_ok=True)
		
	# 	try:
	# 		filename = f'{row.year}-{row.quarter}-{str(row.month).zfill(2)}-{row.week}.txt'
	# 		with open(text_dir + filename, "r", encoding="utf-8") as file:
	# 			text = file.read()
	# 			system_prompt = """
	# 		ë„ˆëŠ” ê²½ì œ ì „ë¬¸ ë‰´ìŠ¤ ë¶„ì„ AIì•¼. ì‚¬ìš©ìê°€ ì§€ì •í•œ ì¢…ëª©(íšŒì‚¬ëª…)ê³¼ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ì •ë³´ë§Œ ì„ íƒí•´ í•µì‹¬ì ìœ¼ë¡œ ìš”ì•½í•´.
	# 		ì‚¬ì‹¤ ê¸°ë°˜ìœ¼ë¡œ ìš”ì•½í•˜ê³ , ê°ì„±ì´ë‚˜ ì¶”ë¡ ì´ í•„ìš”í•œ ê²½ìš°ì—ëŠ” ì¤‘ë¦½ì ìœ¼ë¡œ í‘œí˜„í•´.
	# 		"""

	# 			user_prompt = f"""
	# 		ë‹¤ìŒì€ ê²½ì œ ë‰´ìŠ¤ ê¸°ì‚¬ì…ë‹ˆë‹¤.

	# 		ì´ ê¸°ì‚¬ì—ì„œ **í•œêµ­ ìƒì¥ ê¸°ì—… "{code}"**ê³¼ ê´€ë ¨ëœ ë‚´ìš©ë§Œ ê³¨ë¼ ìš”ì•½í•´ ì£¼ì„¸ìš”.

	# 		ìš”ì•½ ê¸°ì¤€:
	# 		- "{code}"ì´ ì–¸ê¸‰ëœ ë¶€ë¶„ ì¤‘ì‹¬
	# 		- ê´€ë ¨ ì‚¬ì—…, ì‹¤ì , ì£¼ê°€, ì‹œì¥ ë°˜ì‘, ê²½ìŸì‚¬ì™€ì˜ ì—°ê´€ì„±
	# 		- ì •ë¶€ ì •ì±…, ì‚°ì—… íŠ¸ë Œë“œ ë“± ì™¸ë¶€ ìš”ì¸ ì¤‘ ê´€ë ¨ ìˆëŠ” ë¶€ë¶„
	# 		- ë¶€ì •ì /ê¸ì •ì  ë…¼ì¡°ë„ ê°„ë‹¨íˆ ì–¸ê¸‰ (ìˆëŠ” ê²½ìš°)

	# 		í˜•ì‹ì€ ê°„ê²°í•œ ë¬¸ì¥ ë˜ëŠ” Bullet Point í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.

	# 		ê¸°ì‚¬ ì „ë¬¸:
	# 		{text}
	# 		"""
	# 			total_prompt = system_prompt + user_prompt
	# 			tokens = encoding.encode(total_prompt)
	# 			total_tokens += len(tokens)
	# 		with open('data_kr/video/num_token.txt', "a", encoding="utf-8") as log_file:
	# 			timestamp = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
	# 			log_file.write(f"{text_dir}{filename} len: {len(text)}, token: {len(tokens)}\n")
	# 	except Exception as e:
	# 		with open('data_kr/video/num_token.txt', "a", encoding="utf-8") as log_file:
	# 			timestamp = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
	# 			log_file.write(f"{text_dir}{filename} error: " + e + "\n")

	# with open('data_kr/video/num_token.txt', "a", encoding="utf-8") as log_file:
	# 	log_file.write(f"total tokens: {total_tokens}\n")

	# ### LLMìœ¼ë¡œ ì˜ìƒ ìë§‰ ìš”ì•½ ###
	# for row in tqdm(df.itertuples(), total=len(df), desc="LLM summarizing"):
	# 	if pd.isna(row.url) or row.url == '':
	# 		continue

	# 	code = str(row.code).zfill(6)
	# 	name = row.name
	# 	text_dir = f'data_kr/video/text/{row.sector}/{code}/'
	# 	summary_dir = f'preprocessed_data/llm/summary/{row.sector}/{code}/'
	# 	os.makedirs(summary_dir, exist_ok=True)
			
	# 	try:
	# 		filename = f'{row.year}-{row.quarter}-{str(row.month).zfill(2)}-{row.week}.txt'
	# 		stock = f'{name}({code})'
	# 		with open(text_dir + filename, "r", encoding="utf-8") as file:
	# 			text = file.read()
	# 		summary = summarize_text(text, stock)
	# 		with open(summary_dir + filename, "w", encoding="utf-8") as f:
	# 			f.write(summary)
	# 		with open('preprocessed_data/llm/summary/log.txt', "a", encoding="utf-8") as log_file:
	# 			timestamp = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
	# 			log_file.write(f"{timestamp} summary completed: {summary_dir + filename}\n")
	# 	except Exception as e:
	# 		with open('preprocessed_data/llm/summary/log.txt', "a", encoding="utf-8") as log_file:
	# 			timestamp = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
	# 			log_file.write(f"{timestamp} summary error: {summary_dir + filename}\t error: {e}\n")

	### LLMìœ¼ë¡œ ê¸°ì‚¬ ìë§‰ìš”ì•½ì„ í†µí•´ ë“±ë½ ì˜ˆì¸¡ ###
	df = pd.read_csv('data_kr/video/ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘ë³¸.csv', encoding='utf-8')
	for code in df["code"].unique():
		df_ = df[df["code"] == code].reset_index(drop=True)
		
		predict_list = []
		reason_list = []

		for row in tqdm(df_.itertuples(), total=len(df_), desc=f"{code}LLM predicting"):
			if pd.isna(row.url) or row.url == '':
				predict_list.append(None)
				reason_list.append(None)
				continue
			
			code = str(row.code).zfill(6)	
			name = row.name
			summary_dir = f'preprocessed_data/llm/summary_text/{row.sector}/{code}/'
			predict_dir = f'preprocessed_data/llm/predict_text/{row.sector}/{code}/'
			os.makedirs(predict_dir, exist_ok=True)

			try:
				filename = f'{row.year}-{row.quarter}-{str(row.month).zfill(2)}-{row.week}.txt'
				stock = f'{name}({code})'
				with open(f'{summary_dir}{filename}', "r", encoding="utf-8") as file:
					summary = file.read()
				data = predict_market_from_summary(summary, f'{name}({code})')
				
				with open(f'{predict_dir}{filename}', "w", encoding="utf-8") as file:
					file.write(data)
     
				predict = data.split('\n')[0].split(':')[1].strip()
				reason = data.split('\n')[1].split(':')[1].strip()
				predict_list.append(predict)
				reason_list.append(reason)
				
				with open('preprocessed_data/llm/predict_text/log.txt', "a", encoding="utf-8") as log_file:
					timestamp = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
					log_file.write(f"{timestamp} predict completed: {predict_dir}{filename}\n")
			except Exception as e:
				with open('preprocessed_data/llm/predict_text/log.txt', "a", encoding="utf-8") as log_file:
					timestamp = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
					log_file.write(f"{timestamp} predict error: {predict_dir}{filename}\n")  
  
	### LLMìœ¼ë¡œ ì˜ìƒ ìë§‰ìš”ì•½ì„ í†µí•´ ë“±ë½ ì˜ˆì¸¡ ###
	df = pd.read_csv('data_kr/video/ë‰´ìŠ¤ ì˜ìƒ ìˆ˜ì§‘ë³¸.csv', encoding='utf-8')
	for code in df["code"].unique():
		df_ = df[df["code"] == code].reset_index(drop=True)
		
		predict_list = []
		reason_list = []

		for row in tqdm(df_.itertuples(), total=len(df_), desc=f"{code}LLM predicting"):
			if pd.isna(row.url) or row.url == '':
				predict_list.append(None)
				reason_list.append(None)
				continue
			
			code = str(row.code).zfill(6)	
			name = row.name
			summary_dir = f'preprocessed_data/llm/summary_video/{row.sector}/{code}/'
			predict_dir = f'preprocessed_data/llm/predict_video/{row.sector}/{code}/'
			os.makedirs(predict_dir, exist_ok=True)

			try:
				filename = f'{row.year}-{row.quarter}-{str(row.month).zfill(2)}-{row.week}.txt'
				stock = f'{name}({code})'
				with open(f'{summary_dir}{filename}', "r", encoding="utf-8") as file:
					summary = file.read()
				data = predict_market_from_summary(summary, f'{name}({code})')
				
				with open(f'{predict_dir}{filename}', "w", encoding="utf-8") as file:
					file.write(data)
     
				predict = data.split('\n')[0].split(':')[1].strip()
				reason = data.split('\n')[1].split(':')[1].strip()
				predict_list.append(predict)
				reason_list.append(reason)
				
				with open('preprocessed_data/llm/predict_video/log.txt', "a", encoding="utf-8") as log_file:
					timestamp = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
					log_file.write(f"{timestamp} predict completed: {predict_dir}{filename}\n")
			except Exception as e:
				with open('preprocessed_data/llm/predict_video/log.txt', "a", encoding="utf-8") as log_file:
					timestamp = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
					log_file.write(f"{timestamp} predict error: {predict_dir}{filename}\n")
     
    ##### ê¸°ì‚¬ ì˜ˆì¸¡ ê²°ê³¼ ì •ë¦¬ #####
	df=pd.read_csv('./data_kr/video/ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘ë³¸.csv', encoding='utf-8')
	for code in df["code"].unique():
		df_ = df[df["code"] == code].reset_index(drop=True)
		predict_list = []
		reason_list = []
		score_list = []

		for row in tqdm(df_.itertuples(), total=len(df_), desc=f"{code}LLM predicting"):
			if pd.isna(row.url) or row.url == '':
				predict_list.append(None)
				reason_list.append(None)
				score_list.append(None)
				continue
			
			code = str(row.code).zfill(6)	
			name = row.name
			predict_dir = f'preprocessed_data/llm/predict_text/{row.sector}/{code}/'
			os.makedirs(predict_dir, exist_ok=True)

			try:
				filename = f'{row.year}-{row.quarter}-{str(row.month).zfill(2)}-{row.week}.txt'
				stock = f'{name}({code})'
				with open(f'{predict_dir}{filename}', "r", encoding="utf-8") as file:
					data = file.read()
					
				predict = data.split('\n')[0].split(':')[1].strip()
				reason = data.split('\n')[1].split(':')[1].strip()
				score = data.split('\n')[2].split(':')[1].strip()
				predict_list.append(predict)
				reason_list.append(reason)
				score_list.append(int(score))
				
			except Exception as e:
				predict_list.append("ë¶ˆê°€ëŠ¥")
				reason_list.append("ê´€ë ¨ ì—†ìŒ")
				score_list.append(0)
				
		df_predict = df_.copy()
		df_predict["prediction"] = predict_list
		df_predict["reason"] = reason_list
		df_predict["score"] = score_list
		df_predict = df_predict[["year", "quarter", "month", "week", "code", "name", "sector", "upload_dt", "prediction", "reason", "score"]]
		df_predict.to_csv(f"{predict_dir}{code}.csv", index=False, encoding="utf-8")
     
     
	##### ì˜ìƒ ì˜ˆì¸¡ ê²°ê³¼ ì •ë¦¬ #####
	df=pd.read_csv('./data_kr/video/ë‰´ìŠ¤ ì˜ìƒ ìˆ˜ì§‘ë³¸.csv', encoding='utf-8')
	for code in df["code"].unique():
		df_ = df[df["code"] == code].reset_index(drop=True)
		predict_list = []
		reason_list = []
		score_list = []

		for row in tqdm(df_.itertuples(), total=len(df_), desc=f"{code}LLM predicting"):
			if pd.isna(row.url) or row.url == '':
				predict_list.append(None)
				reason_list.append(None)
				score_list.append(None)
				continue
			
			code = str(row.code).zfill(6)	
			name = row.name
			predict_dir = f'preprocessed_data/llm/predict_video/{row.sector}/{code}/'
			os.makedirs(predict_dir, exist_ok=True)

			try:
				filename = f'{row.year}-{row.quarter}-{str(row.month).zfill(2)}-{row.week}.txt'
				stock = f'{name}({code})'
				with open(f'{predict_dir}{filename}', "r", encoding="utf-8") as file:
					data = file.read()
					
				predict = data.split('\n')[0].split(':')[1].strip()
				reason = data.split('\n')[1].split(':')[1].strip()
				score = data.split('\n')[2].split(':')[1].strip()
				predict_list.append(predict)
				reason_list.append(reason)
				score_list.append(int(score))
				
			except Exception as e:
				predict_list.append("ë¶ˆê°€ëŠ¥")
				reason_list.append("ê´€ë ¨ ì—†ìŒ")
				score_list.append(0)
				
		df_predict = df_.copy()
		df_predict["prediction"] = predict_list
		df_predict["reason"] = reason_list
		df_predict["score"] = score_list
		df_predict = df_predict[["year", "quarter", "month", "week", "code", "name", "sector", "upload_dt", "prediction", "reason", "score"]]
		df_predict.to_csv(f"{predict_dir}{code}.csv", index=False, encoding="utf-8")