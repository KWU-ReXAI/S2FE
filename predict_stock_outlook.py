from dotenv import load_dotenv
import os
from pathlib import Path
from datetime import datetime, timedelta

import torch
from googleapiclient.discovery import build
import yt_dlp
import whisper
import re
from tqdm import tqdm

import google.generativeai as genai
from langchain.chat_models import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage

import pandas as pd

load_dotenv()  # .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
YOUTUBE_API_KEY = os.getenv("YOUTUBE_API_KEY")

# whisper ëª¨ë¸ ë¡œë“œ (base, small, medium, large ì¤‘ ì„ íƒ ê°€ëŠ¥)
device = "cuda" if torch.cuda.is_available() else "cpu"
whisper_model = whisper.load_model("medium").to(device)

# gemini ëª¨ë¸ ë¡œë“œ
genai.configure(api_key=GEMINI_API_KEY)
gemini_model = genai.GenerativeModel("gemini-2.0-flash")

# GPT ëª¨ë¸ ë¡œë“œ
gpt_model = ChatOpenAI(temperature=0, model="gpt-4o", openai_api_key=OPENAI_API_KEY)

# YOUTUBE ë¹Œë“œ
youtube = build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)

# -----------------------------
# ìœ íŠœë¸Œ ì±„ë„ëª…ìœ¼ë¡œ ì±„ë„ID ì¶”ì¶œ
# ì…ë ¥ ì˜ˆ: "í•œêµ­ê²½ì œTV" ë˜ëŠ” "@wowtv"
# ì¶œë ¥: ì±„ë„ ID
# -----------------------------
def get_channel_id(channel_name):
	res = youtube.search().list(
		q=channel_name,
		type="channel",
		part="snippet",
		maxResults=1
	).execute()

	# ê²°ê³¼ì—ì„œ ì±„ë„ ID ì¶”ì¶œ
	if res.get("items"):
		channel_id = res["items"][0]["snippet"]["channelId"]
		print("ğŸ“º ì±„ë„ ID:", channel_id)
	else:
		print("ì±„ë„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

	return channel_id

# -----------------------------
# ì±„ë„ IDë¡œ ì—…ë¡œë“œ playlist ID ì–»ê¸°
# ì…ë ¥: channel id
# ì¶œë ¥: playlist id
# -----------------------------
def get_uploads_playlist_id(channel_id):
    res = youtube.channels().list(
        part="contentDetails",
        id=channel_id
    ).execute()
    return res["items"][0]["contentDetails"]["relatedPlaylists"]["uploads"]

# -----------------------------
# playlist IDë¡œ í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ ë‚´ ëª¨ë“  video IDì™€ ì—…ë¡œë“œ ë‚ ì§œ ì–»ê¸°
# ì…ë ¥: playlist id
# ì¶œë ¥: type: íŠœí”Œ ë¦¬ìŠ¤íŠ¸  ex) [(id1, date1, ì¡°íšŒìˆ˜1), (id2, date2, ì¡°íšŒìˆ˜2) ...]
# -----------------------------
def get_video_datas_from_playlist(playlist_id):
    video_list = []
    next_page_token = None
    video_id_date_pairs = []

    # 1. playlistItems APIë¡œ video ID + ì—…ë¡œë“œ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°
    while True:
        res = youtube.playlistItems().list(
            part="snippet",
            playlistId=playlist_id,
            maxResults=50,
            pageToken=next_page_token
        ).execute()

        for item in res["items"]:
            snippet = item["snippet"]
            video_id = snippet["resourceId"]["videoId"]
            published_at = snippet["publishedAt"]
            video_id_date_pairs.append((video_id, published_at))

        next_page_token = res.get("nextPageToken")
        if not next_page_token:
            break

    # 2. video IDë¡œ ì¡°íšŒìˆ˜ ê°€ì ¸ì˜¤ê¸°
    for i in range(0, len(video_id_date_pairs), 50):
        batch = video_id_date_pairs[i:i+50]
        ids_only = [vid for vid, _ in batch]

        res = youtube.videos().list(
            part="statistics",
            id=",".join(ids_only)
        ).execute()

        stats = {item["id"]: int(item["statistics"].get("viewCount", 0)) for item in res["items"]}

        # 3. íŠœí”Œë¡œ ì €ì¥: (video_id, published_at, view_count)
        for video_id, published_at in batch:
            view_count = stats.get(video_id, 0)
            video_list.append((video_id, published_at, view_count))

    return video_list

# -----------------------------
# video idsë¡œ ì—…ë¡œë“œ ë‚ ì§œ í•„í„°ë§í•˜ê¸°
# ì…ë ¥: video_datas(type íŠœí”Œ ë¦¬ìŠ¤íŠ¸), ì‹œì‘ë‚ ì§œ, ëë‚ ì§œ ("2001-04-30" í˜•ì‹ìœ¼ë¡œ ì…ë ¥)
# ì¶œë ¥: í•„í„°ë§ ëœ video_datas(type íŠœí”Œ ë¦¬ìŠ¤íŠ¸)
# -----------------------------
def filter_by_date(video_datas, start_date, end_date):
    # ë¬¸ìì—´ â†’ datetime ê°ì²´ë¡œ ë³€í™˜
    start_dt = datetime.strptime(start_date + "T00:00:00Z", "%Y-%m-%dT%H:%M:%SZ")
    end_dt = datetime.strptime(end_date + "T00:00:00Z", "%Y-%m-%dT%H:%M:%SZ")

    filtered = []
    for video_id, published_at, view_count in video_datas:
        pub_date = datetime.strptime(published_at, "%Y-%m-%dT%H:%M:%SZ")
        if start_dt <= pub_date < end_dt:
            filtered.append((video_id, published_at, view_count))

    return filtered
    
# -----------------------------
# ì¡°íšŒìˆ˜ ê¸°ì¤€ í•„í„°ë§
# ì…ë ¥: video_datas(type íŠœí”Œ ë¦¬ìŠ¤íŠ¸), ìµœì†Œì¡°íšŒìˆ˜
# ì¶œë ¥: video_datas(type íŠœí”Œ ë¦¬ìŠ¤íŠ¸)
# -----------------------------
def filter_videos_by_view_count(video_tuples, min_views=0, max_views=float("inf")):
    return [
        (video_id, published_at, view_count)
        for video_id, published_at, view_count in video_tuples
        if min_views <= view_count <= max_views
    ]
    
# -----------------------------
# ì±„ë„ì˜ìƒì„ ë‚ ì§œ, ì¡°íšŒìˆ˜ ê¸°ì¤€ í•„í„°ë§
# ì…ë ¥: channel_id, ê¸°ê°„, ìµœì†Œì¡°íšŒìˆ˜
# ì¶œë ¥: video_datas(type íŠœí”Œ ë¦¬ìŠ¤íŠ¸)
# -----------------------------
def get_filtered_videos_by_channel(channel_id, start_date, end_date, min_views=0):
    video_results = []
    next_page_token = None

    # ë‚ ì§œ ë¬¸ìì—´ â†’ datetime ê°ì²´ â†’ ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    start_iso = (datetime.strptime(start_date, "%Y-%m-%d") - timedelta(days=7)).isoformat("T") + "Z"
    end_iso = datetime.strptime(end_date, "%Y-%m-%d").isoformat("T") + "Z"

    # 1. search().list() â†’ video ID + publishedAt
    temp_videos = []
    while True:
        res = youtube.search().list(
            part="snippet",
            channelId=channel_id,
            publishedAfter=start_iso,
            publishedBefore=end_iso,
            maxResults=50,
            pageToken=next_page_token,
            type="video",
            order="date"
        ).execute()

        for item in res["items"]:
            video_id = item["id"]["videoId"]
            published_at = item["snippet"]["publishedAt"]
            temp_videos.append((video_id, published_at))

        next_page_token = res.get("nextPageToken")
        if not next_page_token:
            break

    # 2. videos().list() â†’ ì¡°íšŒìˆ˜ ë¶™ì´ê¸°
    for i in range(0, len(temp_videos), 50):
        batch = temp_videos[i:i+50]
        ids = [vid for vid, _ in batch]

        res = youtube.videos().list(
            part="statistics",
            id=",".join(ids)
        ).execute()

        stats = {
            item["id"]: int(item["statistics"].get("viewCount", 0))
            for item in res["items"]
        }

        for video_id, published_at in batch:
            view_count = stats.get(video_id, 0)
            if view_count >= min_views:
                video_results.append((video_id, published_at, view_count))

    return video_results

# -----------------------------
# ìœ íŠœë¸Œ ì˜ìƒ ê²€ìƒ‰
# date ì´ì „ì˜ query ê²€ìƒ‰ ê²°ê³¼ë“¤ë§Œ ë³´ì—¬ì¤Œ 
# dateëŠ” '2025-04-11' í˜•ì‹ìœ¼ë¡œ ì…ë ¥
# -----------------------------
def search_videos(query, date):
	date += 'T00:00:00Z' # ISO 8601 í˜•ì‹ìœ¼ë¡œ ë³€ê²½

	# Step 1: ê²€ìƒ‰
	search_res = youtube.search().list(
		q=query,
		part='snippet',
		type='video',
		maxResults=50,
		order='date',
		publishedBefore=date
	).execute()
	
	# Step 2: videoId ìˆ˜ì§‘
	video_ids = [item['id']['videoId'] for item in search_res['items']]
	if not video_ids:
		print("ì¡°ê±´ì— ë§ëŠ” ì˜ìƒ ì—†ìŒ")

	return video_ids

# -----------------------------
# ìœ íŠœë¸Œ ë§í¬ or idì—ì„œ ìŒì„±íŒŒì¼ ì¶”ì¶œ
# INPUT: method(link or id), youtube id or link, ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ê²½ë¡œ
# -----------------------------
def extract_video_audio(method, video_id, audio_dir):
	os.makedirs('./audio', exist_ok=True)
	
	if method == "link":
		url = video_id
	elif method == "id":
		url = "https://www.youtube.com/watch?v=" + video_id
	else:
		print('methodì¸ìë¡œ link or idë¥¼ ì…ë ¥í•˜ì„¸ìš”')
		return False

	ydl_opts = {
		'format': 'bestaudio/best',
		'outtmpl': f'{audio_dir}.%(ext)s',
		'postprocessors': [{
			'key': 'FFmpegExtractAudio',
			'preferredcodec': 'mp3',
		}]
	}

	try:
		with yt_dlp.YoutubeDL(ydl_opts) as ydl:
			ydl.download([url])
		print("ìŒì„±íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì„±ê³µ")
		return True
	except:
		print("ìŒì„±íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
		return False

# -----------------------------
# ìŒì„±íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
# INPUT: youtube id, ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ê²½ë¡œ
# -----------------------------
def audio2text(audio_dir):
	# ìŒì„± íŒŒì¼ STT ìˆ˜í–‰
	result = whisper_model.transcribe(f'{audio_dir}.mp3', language="ko")  # wav, mp4 ë“±ë„ OK

	# í…ìŠ¤íŠ¸ ì¶œë ¥
	print(result["text"])

	return result["text"]

# -----------------------------
# ìë§‰ ì „ì²˜ë¦¬ í•¨ìˆ˜
# -----------------------------
def clean_srt(srt_text: str) -> str:
	srt_text = re.sub(r"(ì¢€|ê·¸ëƒ¥|ë­ë„ê¹Œ|ê·¸ëŸ¬ë‹ˆê¹Œ|ì•„ë‹ˆ|ì•½ê°„|ë­”ê°€|ë­ëƒë©´ìš”)", "", srt_text)
	return srt_text


# ------------------------
# GPT-4o ìš”ì•½
# ------------------------
def summarize_text(text: str, stock: str) -> str:
	system_prompt = """
ë„ˆëŠ” ê²½ì œ ì „ë¬¸ ë‰´ìŠ¤ ë¶„ì„ AIì•¼. ì‚¬ìš©ìê°€ ì§€ì •í•œ ì¢…ëª©(íšŒì‚¬ëª…)ê³¼ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ì •ë³´ë§Œ ì„ íƒí•´ í•µì‹¬ì ìœ¼ë¡œ ìš”ì•½í•´.
ì‚¬ì‹¤ ê¸°ë°˜ìœ¼ë¡œ ìš”ì•½í•˜ê³ , ê°ì„±ì´ë‚˜ ì¶”ë¡ ì´ í•„ìš”í•œ ê²½ìš°ì—ëŠ” ì¤‘ë¦½ì ìœ¼ë¡œ í‘œí˜„í•´.
"""

	user_prompt = f"""
ë‹¤ìŒì€ ê²½ì œ ë‰´ìŠ¤ ê¸°ì‚¬ì…ë‹ˆë‹¤.

ì´ ê¸°ì‚¬ì—ì„œ **í•œêµ­ ìƒì¥ ê¸°ì—… "{stock}"**ê³¼ ê´€ë ¨ëœ ë‚´ìš©ë§Œ ê³¨ë¼ ìš”ì•½í•´ ì£¼ì„¸ìš”.

ìš”ì•½ ê¸°ì¤€:
- "{stock}"ì´ ì–¸ê¸‰ëœ ë¶€ë¶„ ì¤‘ì‹¬
- ê´€ë ¨ ì‚¬ì—…, ì‹¤ì , ì£¼ê°€, ì‹œì¥ ë°˜ì‘, ê²½ìŸì‚¬ì™€ì˜ ì—°ê´€ì„±
- ì •ë¶€ ì •ì±…, ì‚°ì—… íŠ¸ë Œë“œ ë“± ì™¸ë¶€ ìš”ì¸ ì¤‘ ê´€ë ¨ ìˆëŠ” ë¶€ë¶„
- ë¶€ì •ì /ê¸ì •ì  ë…¼ì¡°ë„ ê°„ë‹¨íˆ ì–¸ê¸‰ (ìˆëŠ” ê²½ìš°)

í˜•ì‹ì€ ê°„ê²°í•œ ë¬¸ì¥ ë˜ëŠ” Bullet Point í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.

ê¸°ì‚¬ ì „ë¬¸:
{text}
"""

	response = gpt_model([
		SystemMessage(content=system_prompt.strip()),
		HumanMessage(content=user_prompt.strip())
	])
	return response.content.strip()



# ------------------------
# GPT-4o ë“±ë½ ì˜ˆì¸¡
# ------------------------
def predict_market_from_summary(summary: str, stock: str) -> str:
	system_prompt = """
ë„ˆëŠ” ê²½ì œ ë‰´ìŠ¤ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì£¼ì‹ ì¢…ëª©ì˜ ë‹¨ê¸° ë“±ë½ ê°€ëŠ¥ì„±ì„ íŒë‹¨í•˜ëŠ” ë¶„ì„ AIì…ë‹ˆë‹¤.
"""

	user_prompt = f"""
ë‹¤ìŒì€ í•œêµ­ ìƒì¥ ê¸°ì—… "{stock}"ê³¼ ê´€ë ¨ëœ ë‰´ìŠ¤ì…ë‹ˆë‹¤.

ë‰´ìŠ¤ì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ "{stock}"ì˜ ë‹¨ê¸° ì£¼ê°€ ë“±ë½ ì „ë§ì„ ë¶„ì„í•˜ê³ , ê²°ê³¼ë¥¼ **ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜**í•´ ì£¼ì„¸ìš”.

---
**[ì¤‘ìš” ê·œì¹™]**
1.  â—ï¸**ë‚´ìš©ì— ë‚˜íƒ€ë‚œ ì •ë³´ë§Œì„ ê·¼ê±°ë¡œ íŒë‹¨í•´ì•¼ í•˜ë©°, ê¸°ì‚¬ ì›ë¬¸ì´ë‚˜ ë°°ê²½ì§€ì‹ì€ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.**
2.  â—ï¸**ë‚´ìš©ê³¼ '{stock}'ì˜ ê´€ë ¨ì„±ì„ ë¨¼ì € íŒë‹¨í•˜ì„¸ìš”.**
    -   **ë‚´ìš©ì´ ê¸°ì—…ê³¼ ì§ì ‘ì ì¸ ê´€ë ¨ì´ ì—†ë‹¤ë©´**, ì£¼ê°€ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì´ ì—†ë‹¤ê³  íŒë‹¨í•˜ì—¬ **'ì¤‘ë¦½'ìœ¼ë¡œ ê²°ë¡  ë‚´ë¦¬ê³  ì ìˆ˜ëŠ” '0'ìœ¼ë¡œ ë¶€ì—¬**í•˜ì„¸ìš”.
    -   **ë‚´ìš©ì´ ê¸°ì—…ê³¼ ê´€ë ¨ì´ ìˆë‹¤ë©´**,  ë‚´ìš©ì˜ ë…¼ì¡°ì— ë”°ë¼ **"ë§¤ìš° ê¸ì •", "ê¸ì •", "ë¶€ì •", "ë§¤ìš° ë¶€ì •"** ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤. (ì´ ë‹¨ê³„ì—ì„œ ì˜í–¥ë ¥ì´ ë¶ˆë¶„ëª…í•˜ë‹¤ëŠ” ì´ìœ ë¡œ ì¤‘ë¦½ìœ¼ë¡œ íŒë‹¨í•˜ì§€ ë§ˆì„¸ìš”.) ê¸ì •ì ì¸ ì†Œì‹(ì‹¤ì  ê°œì„ , ì‹ ê·œ ìˆ˜ì£¼ ë“±)ì€ ìƒìŠ¹ ìš”ì¸ìœ¼ë¡œ, ë¶€ì •ì ì¸ ì†Œì‹(ì‹¤ì  ì•…í™”, ë²•ì  ë¦¬ìŠ¤í¬ ë“±)ì€ í•˜ë½ ìš”ì¸ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.
3. ì¶œë ¥ì€ ë°˜ë“œì‹œ JSON ê°ì²´ {{}}ë¡œ ì´ë£¨ì–´ì ¸ì•¼í•˜ë©°, JSON md markersë¡œ ê°ì‹¸ì§€ ë§ˆì„¸ìš”.
---

**[JSON ì¶œë ¥ í˜•ì‹ ë° í‚¤ ì„¤ëª…]**
-   `sentiment`: (String) ë…¼ì¡° íŒë‹¨ ê²°ê³¼. "ë§¤ìš° ê¸ì •", "ê¸ì •", "ì¤‘ë¦½", "ë¶€ì •", "ë§¤ìš° ë¶€ì •" ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.
-   `reasoning`: (String) íŒë‹¨ì˜ ê·¼ê±°ê°€ ë˜ëŠ” í•µì‹¬ ë‚´ìš©ì„ ì°¾ì•„ ê°„ê²°í•˜ê²Œ ì‘ì„±í•©ë‹ˆë‹¤.
-   `score`: (Integer) ë‹¨ê¸° ë“±ë½ ì „ë§ ì ìˆ˜. ì•„ë˜ ë²”ìœ„ ë‚´ì˜ ì •ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤.
	-   `+2`: ê°•í•œ ìƒìŠ¹
	-   `+1`: ë‹¤ì†Œ ìƒìŠ¹
	-   `0`: ì¤‘ë¦½
	-   `-1`: ë‹¤ì†Œ í•˜ë½
	-   `-2`: ê°•í•œ í•˜ë½

**[ì¶œë ¥ ì˜ˆì‹œ]**
{{
	"sentiment": "ê¸ì •ì ",
	"reasoning": "ê¸°ì‚¬ì—ì„œ í•´ë‹¹ ê¸°ì—…ì´ ë¯¸êµ­ ëŒ€í˜• ì „ê¸°ì°¨ ì—…ì²´ì™€ ì‹ ê·œ ë°°í„°ë¦¬ ê³µê¸‰ ê³„ì•½ì„ ì²´ê²°í–ˆê³ , ìˆ˜ì¶œ í™•ëŒ€ì™€ ì‹¤ì  ê°œì„ ì— ëŒ€í•œ ê¸°ëŒ€ê°ì´ ì–¸ê¸‰ë˜ì–´ ê¸ì •ì ì¸ ë…¼ì¡°ë¡œ íŒë‹¨ë¨.",
	"score": 1
}}

[ìš”ì•½ë³¸]
{summary}
"""

	response = gpt_model([
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_prompt)
    ])
	return response.content.strip()

# ------------------------
# GPT-4o ë“±ë½ ì˜ˆì¸¡
# ------------------------
def predict_market_from_mix(news_article: str, video_script:str, stock: str) -> str:
	system_prompt = """
ë„ˆëŠ” ì£¼ì–´ì§„ ë‰´ìŠ¤ ê¸°ì‚¬ì™€ ê²½ì œ ì˜ìƒ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬, íŠ¹ì • ì£¼ì‹ ì¢…ëª©ì˜ ë‹¨ê¸° ë“±ë½ ê°€ëŠ¥ì„±ì„ íŒë‹¨í•˜ëŠ” ë‹¤ì¤‘ ì •ë³´ ë¶„ì„ AIì…ë‹ˆë‹¤.
"""

	user_prompt = f"""
ë‹¤ìŒì€ í•œêµ­ ìƒì¥ ê¸°ì—… "{stock}"ê³¼ ê´€ë ¨ëœ **ë‰´ìŠ¤ ê¸°ì‚¬ì™€ ê²½ì œ ì˜ìƒ ìŠ¤í¬ë¦½íŠ¸**ì…ë‹ˆë‹¤.

**ì œê³µëœ ë‘ ê°€ì§€ ì½˜í…ì¸ ì˜ ë‚´ìš©ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„**í•˜ì—¬ "{stock}"ì˜ ë‹¨ê¸° ì£¼ê°€ ë“±ë½ ì „ë§ì„ íŒë‹¨í•˜ê³ , ê·¸ ê²°ê³¼ë¥¼ **ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜**í•´ ì£¼ì„¸ìš”.

---
**[ì…ë ¥ ì •ë³´]**
* `stock`: (String) ë¶„ì„í•  í•œêµ­ ìƒì¥ ê¸°ì—…ì˜ ì´ë¦„.
* `news_article`: (String) ë¶„ì„í•  ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸.
* `video_script`: (String) ë¶„ì„í•  ê²½ì œ ì˜ìƒì˜ ìŠ¤í¬ë¦½íŠ¸/ìš”ì•½ë³¸.
---

**[ì¤‘ìš” ê·œì¹™]**
1.  â—ï¸**ì œê³µëœ ë‘ ê°€ì§€ ì½˜í…ì¸ (ë‰´ìŠ¤ ê¸°ì‚¬, ì˜ìƒ ìŠ¤í¬ë¦½íŠ¸)ì— ë‚˜íƒ€ë‚œ ì •ë³´ë§Œì„ ê·¼ê±°ë¡œ íŒë‹¨í•´ì•¼ í•˜ë©°,** ë°°ê²½ì§€ì‹ì´ë‚˜ ì™¸ë¶€ ì •ë³´ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
2.  â—ï¸**íŒë‹¨ì€ ì•„ë˜ 3ë‹¨ê³„ ê³¼ì •ì„ ì—„ê²©íˆ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤.**
    * **1ë‹¨ê³„: ê°œë³„ ì½˜í…ì¸  ë¶„ì„**
        * ê° ì½˜í…ì¸ (ë‰´ìŠ¤, ì˜ìƒ)ê°€ '{stock}'ê³¼ **ì§ì ‘ ê´€ë ¨ì´ ìˆëŠ”ì§€**ë¥¼ ë¨¼ì € í™•ì¸í•©ë‹ˆë‹¤.
        * **ê´€ë ¨ì´ ì—†ëŠ” ì½˜í…ì¸ **ì˜ ì…ì¥ì€ **'ì¤‘ë¦½(0ì )'** ìœ¼ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.
        * **ê´€ë ¨ì´ ìˆëŠ” ì½˜í…ì¸ **ëŠ” ë‚´ìš©ì˜ ë…¼ì¡°ì— ë”°ë¼ **'ê¸ì •(+)' ë˜ëŠ” 'ë¶€ì •(-)'** ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤. (ì´ ë‹¨ê³„ì—ì„œ ì˜í–¥ë ¥ì´ ë¶ˆë¶„ëª…í•˜ë‹¤ëŠ” ì´ìœ ë¡œ ì¤‘ë¦½ìœ¼ë¡œ íŒë‹¨í•˜ì§€ ë§ˆì„¸ìš”.)

    * **2ë‹¨ê³„: ì¢…í•© íŒë‹¨ ë° ìµœì¢… ê²°ë¡ **
        * **[ìƒì¶©] í•œìª½ì€ 'ê¸ì •', ë‹¤ë¥¸ ìª½ì€ 'ë¶€ì •'ì¼ ê²½ìš°:** ì´ê²ƒì´ 'ì¤‘ë¦½'ìœ¼ë¡œ íŒë‹¨í•˜ëŠ” **ìœ ì¼í•œ ì¡°ê±´**ì…ë‹ˆë‹¤. ìµœì¢… ê²°ê³¼ë¥¼ **'ì¤‘ë¦½'ìœ¼ë¡œ íŒë‹¨í•˜ê³  ì ìˆ˜ëŠ” '0'** ìœ¼ë¡œ ë¶€ì—¬í•˜ì„¸ìš”.
        * **[ì¼ì¹˜] ë‘˜ ë‹¤ 'ê¸ì •'ì´ê±°ë‚˜ ë‘˜ ë‹¤ 'ë¶€ì •'ì¼ ê²½ìš°:** í•´ë‹¹ ì…ì¥ì„ ì¢…í•©í•˜ì—¬ ìµœì¢… ê²°ê³¼ë¥¼ íŒë‹¨í•©ë‹ˆë‹¤. (ì˜ˆ: ê¸ì •+ê¸ì • = 'ê¸ì •' ë˜ëŠ” 'ë§¤ìš° ê¸ì •')
        * **[í¸ì¤‘] í•œìª½ë§Œ 'ê¸ì •'/'ë¶€ì •'ì´ê³  ë‹¤ë¥¸ ìª½ì€ 'ì¤‘ë¦½'ì¼ ê²½ìš°:** **ì˜ë¯¸ ìˆëŠ” ì…ì¥ì„ ê°€ì§„ ìª½(ê¸ì • ë˜ëŠ” ë¶€ì •)ì˜ ë…¼ì¡°**ë¥¼ ìµœì¢… ê²°ê³¼ë¡œ ë”°ë¦…ë‹ˆë‹¤.

    * **3ë‹¨ê³„: ê·¼ê±° ì‘ì„±**
        * `reasoning` í•­ëª©ì—ëŠ” ìœ„ 1, 2ë‹¨ê³„ì˜ ë¶„ì„ ê³¼ì •ì„ ë°”íƒ•ìœ¼ë¡œ **ì–´ë–»ê²Œ ìµœì¢… ê²°ë¡ ì— ë„ë‹¬í–ˆëŠ”ì§€** ëª…í™•íˆ ì„œìˆ í•´ì•¼ í•©ë‹ˆë‹¤. (ì˜ˆ: "ë‰´ìŠ¤ ê¸°ì‚¬ëŠ” ê¸ì •ì ì´ì—ˆìœ¼ë‚˜ ì˜ìƒì€ íšŒì‚¬ì™€ ë¬´ê´€í•œ ë‚´ìš©ìœ¼ë¡œ ì¤‘ë¦½ì´ë¯€ë¡œ, ì¢…í•©ì ì¸ ì „ë§ì€ ë‰´ìŠ¤ì˜ ë…¼ì¡°ë¥¼ ë”°ë¼ ê¸ì •ìœ¼ë¡œ íŒë‹¨í•¨.")
3.  ì¶œë ¥ì€ ë°˜ë“œì‹œ JSON ê°ì²´ {{}}ë¡œ ì´ë£¨ì–´ì ¸ì•¼í•˜ë©°, JSON md markersë¡œ ê°ì‹¸ì§€ ë§ˆì„¸ìš”.
---	

**[JSON ì¶œë ¥ í˜•ì‹ ë° í‚¤ ì„¤ëª…]**
-   `sentiment`: (String) ë…¼ì¡° íŒë‹¨ ê²°ê³¼. "ë§¤ìš° ê¸ì •", "ê¸ì •", "ì¤‘ë¦½", "ë¶€ì •", "ë§¤ìš° ë¶€ì •" ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.
-   `reasoning`: (String) íŒë‹¨ì˜ ê·¼ê±°ê°€ ë˜ëŠ” í•µì‹¬ ë‚´ìš©ì„ ì‘ì„±í•©ë‹ˆë‹¤. **ì–´ë–¤ ì½˜í…ì¸ (ë‰´ìŠ¤/ì˜ìƒ)ì—ì„œ ê·¼ê±°ë¥¼ ì°¾ì•˜ëŠ”ì§€ ëª…ì‹œí•˜ê±°ë‚˜, ë‘ ì •ë³´ë¥¼ ì–´ë–»ê²Œ ì¢…í•©í–ˆëŠ”ì§€**ë¥¼ ê°„ê²°í•˜ê²Œ ì„¤ëª…í•´ì•¼ í•©ë‹ˆë‹¤.
-   `score`: (Integer) ë‹¨ê¸° ë“±ë½ ì „ë§ ì ìˆ˜. ì•„ë˜ ë²”ìœ„ ë‚´ì˜ ì •ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤.
    -   `+2`: ê°•í•œ ìƒìŠ¹
    -   `+1`: ë‹¤ì†Œ ìƒìŠ¹
    -   `0`: ì¤‘ë¦½
    -   `-1`: ë‹¤ì†Œ í•˜ë½
    -   `-2`: ê°•í•œ í•˜ë½

**[ì¶œë ¥ ì˜ˆì‹œ]**
{{
    "sentiment": "ì¤‘ë¦½",
    "reasoning": "ë‰´ìŠ¤ ê¸°ì‚¬ëŠ” 2ë¶„ê¸° ì–´ë‹ ì„œí”„ë¼ì´ì¦ˆë¥¼ ê¸ì •ì ìœ¼ë¡œ ë³´ë„í–ˆìœ¼ë‚˜, ê²½ì œ ì˜ìƒì—ì„œëŠ” ì›ìì¬ ê°€ê²© ìƒìŠ¹ìœ¼ë¡œ ì¸í•œ í•˜ë°˜ê¸° ìˆ˜ìµì„± ì•…í™” ê°€ëŠ¥ì„±ì„ ê²½ê³ í–ˆìŠµë‹ˆë‹¤. ê¸ì •ì  ìš”ì¸ê³¼ ë¶€ì •ì  ìš”ì¸ì´ ìƒì¶©í•˜ë¯€ë¡œ ì¢…í•©ì ìœ¼ë¡œ ì¤‘ë¦½ìœ¼ë¡œ íŒë‹¨í•©ë‹ˆë‹¤.",
    "score": 0
}}

---
**[ë‰´ìŠ¤ ê¸°ì‚¬ ë‚´ìš©]**
{news_article}
---
**[ê²½ì œ ì˜ìƒ ë‚´ìš©]**
{video_script}
"""

	response = gpt_model([
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_prompt)
    ])
	return response.content.strip()

# ------------------------
# ì•ì˜ ëª¨ë“  í•¨ìˆ˜ë¥¼ ì´ìš©í•œ ìµœì¢… í•¨ìˆ˜
#
# ì£¼ì‹ê³¼ ë‚ ì§œ ì…ë ¥í•˜ë©´ ì£¼ê°€ì „ë§ ì˜ˆì¸¡
# dateëŠ” '2025-04-11' í˜•ì‹ìœ¼ë¡œ ì…ë ¥
# ------------------------
def predict_market(stock: str, date: str) -> str:
	SEARCH_QUERY = f'{stock} ì£¼ê°€ì „ë§'
	BEFORE_DATE = date
	print("SEARCH_QUERY:", SEARCH_QUERY)
	video_id = search_videos(SEARCH_QUERY, BEFORE_DATE)
	if video_id == None:
		return "middle"

	# ìœ íŠœë¸Œ ì˜ìƒ ìŒì„± ì¶”ì¶œ
	audio_dir = f'audio/{stock}_{BEFORE_DATE}'
	if not extract_video_audio(video_id, audio_dir):
		return "middle"
	text = audio2text(audio_dir)

	# ìë§‰ ì „ì²˜ë¦¬
	cleaned = clean_srt(text)
	print("ì „ì²˜ë¦¬:\n", cleaned)

	# ì „ì²˜ë¦¬ ìë§‰ ìš”ì•½
	summary = summarize_text(cleaned)

	# ì£¼ì‹ ë“±ë½ ì˜ˆì¸¡
	prediction = predict_market_from_summary(summary, stock)

	print("\nğŸ“„ GEMINI ìš”ì•½ ê²°ê³¼:\n", summary)
	print("\nğŸ“ˆ GPT-4 ì˜ˆì¸¡ ê²°ê³¼:\n\n", prediction)

	if prediction == 'ì˜¤ë¥¼ ê°€ëŠ¥ì„± ìˆìŒ':
		return 'up'
	elif prediction == 'ì˜¤ë¥¼ ê°€ëŠ¥ì„± ë‚®ìŒ':
		return 'down'
	else:
		return 'middle'




# ------------------------
# ëª¨ë“  ë¶„ê¸°ì˜ ë²”ìœ„ êµ¬í•˜ê¸°
# disclosure_date_range.csv íŒŒì¼ ìƒì„±
# ------------------------
def get_disclosure_range():
    # ëª¨ë“  ì¢…ëª©ì˜ ëª¨ë“  ë¶„ê¸° ê³µì‹œì¼ì„ í•˜ë‚˜ì˜ íŒŒì¼ë¡œ
	root_path = Path('./data_kr/merged')
	all_symbols_disclosure = pd.DataFrame()
	for file_path in root_path.rglob("*.csv"):
		df_ = pd.read_csv(file_path)
		df_ = df_[["code", "name", "year", "quarter", "disclosure_date"]]
		all_symbols_disclosure = pd.concat([all_symbols_disclosure, df_])


	years = [2015] + ([y for y in range(2016, 2025) for _ in range(4)])
	quarters = ["Q4"] + ([q for _ in range(2016, 2025) for q in ["Q1", "Q2", "Q3", "Q4"]])
	df_disclosure = pd.DataFrame({
		"year": years,
		"quarter": quarters,
		"min_disclosure_date": [None] * len(years),
		"max_disclosure_date": [None] * len(years)
	})

	for i, row in enumerate(df_disclosure.itertuples()):
		disclosures = all_symbols_disclosure[(all_symbols_disclosure["year"] == row.year) & (all_symbols_disclosure["quarter"] == row.quarter)]["disclosure_date"]
		df_disclosure.loc[i, "min_disclosure_date"] = disclosures.min()
		df_disclosure.loc[i, "max_disclosure_date"] = disclosures.max()

	os.makedirs('./data_kr/audio', exist_ok=True)
	df_disclosure.to_csv("./data_kr/audio/disclosure_date_range.csv", index=False)
 
# ------------------------
# disclosure rangeë¥¼ ë§Œì¡±í•˜ê³  ì¡°íšŒìˆ˜ê°€ min_view_cnt ì´ìƒì¸ video id êµ¬í•˜ê¸°
# channel_name: str
# min_view_cnt: int
# data_kr/audoi/ì— ì—°ë„-ë¶„ê¸°.csv íŒŒì¼ ìƒì„±
# ------------------------
def get_video_datas(channel_name, min_view_cnt):
	channel_id = get_channel_id(channel_name) 
	dir = f'data_kr/audio/{channel_name}'
	os.makedirs(dir, exist_ok=True)
 
	df_disclosure = pd.read_csv('data_kr/audio/disclosure_date_range.csv')
	for row in df_disclosure.itertuples():
		start = datetime.strptime(row.min_disclosure_date, "%Y-%m-%d")
		start -= timedelta(days=7)
		start = start.strftime("%Y-%m-%d")
		end = row.max_disclosure_date

		video_datas = get_filtered_videos_by_channel(channel_id, start, end, min_view_cnt)
		year_quarter = f'{row.year}-{row.quarter}'
		os.makedirs(f'{dir}/{year_quarter}', exist_ok=True)
		pd.DataFrame(video_datas, columns=['video_id', 'published_at', 'view_count']).to_csv(f'{dir}/{year_quarter}/{year_quarter}.csv', index=False)
