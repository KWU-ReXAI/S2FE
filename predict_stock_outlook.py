from dotenv import load_dotenv
import os
from pathlib import Path
from datetime import datetime, timedelta

import torch
from googleapiclient.discovery import build
import yt_dlp
import whisper
import re
from tqdm import tqdm

import google.generativeai as genai
from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage

import pandas as pd

load_dotenv()  # .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
YOUTUBE_API_KEY = os.getenv("YOUTUBE_API_KEY")

# whisper ëª¨ë¸ ë¡œë“œ (base, small, medium, large ì¤‘ ì„ íƒ ê°€ëŠ¥)
device = "cuda" if torch.cuda.is_available() else "cpu"
whisper_model = whisper.load_model("medium").to(device)

# gemini ëª¨ë¸ ë¡œë“œ
genai.configure(api_key=GEMINI_API_KEY)
gemini_model = genai.GenerativeModel("gemini-2.0-flash")

# GPT ëª¨ë¸ ë¡œë“œ
gpt_model = ChatOpenAI(temperature=0, model="gpt-4o", openai_api_key=OPENAI_API_KEY)

# YOUTUBE ë¹Œë“œ
youtube = build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)

# -----------------------------
# ìœ íŠœë¸Œ ì±„ë„ëª…ìœ¼ë¡œ ì±„ë„ID ì¶”ì¶œ
# ì…ë ¥ ì˜ˆ: "í•œêµ­ê²½ì œTV" ë˜ëŠ” "@wowtv"
# ì¶œë ¥: ì±„ë„ ID
# -----------------------------
def get_channel_id(channel_name):
	res = youtube.search().list(
		q=channel_name,
		type="channel",
		part="snippet",
		maxResults=1
	).execute()

	# ê²°ê³¼ì—ì„œ ì±„ë„ ID ì¶”ì¶œ
	if res.get("items"):
		channel_id = res["items"][0]["snippet"]["channelId"]
		print("ğŸ“º ì±„ë„ ID:", channel_id)
	else:
		print("ì±„ë„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

	return channel_id

# -----------------------------
# ì±„ë„ IDë¡œ ì—…ë¡œë“œ playlist ID ì–»ê¸°
# ì…ë ¥: channel id
# ì¶œë ¥: playlist id
# -----------------------------
def get_uploads_playlist_id(channel_id):
    res = youtube.channels().list(
        part="contentDetails",
        id=channel_id
    ).execute()
    return res["items"][0]["contentDetails"]["relatedPlaylists"]["uploads"]

# -----------------------------
# playlist IDë¡œ í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ ë‚´ ëª¨ë“  video IDì™€ ì—…ë¡œë“œ ë‚ ì§œ ì–»ê¸°
# ì…ë ¥: playlist id
# ì¶œë ¥: type: íŠœí”Œ ë¦¬ìŠ¤íŠ¸  ex) [(id1, date1, ì¡°íšŒìˆ˜1), (id2, date2, ì¡°íšŒìˆ˜2) ...]
# -----------------------------
def get_video_datas_from_playlist(playlist_id):
    video_list = []
    next_page_token = None
    video_id_date_pairs = []

    # 1. playlistItems APIë¡œ video ID + ì—…ë¡œë“œ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°
    while True:
        res = youtube.playlistItems().list(
            part="snippet",
            playlistId=playlist_id,
            maxResults=50,
            pageToken=next_page_token
        ).execute()

        for item in res["items"]:
            snippet = item["snippet"]
            video_id = snippet["resourceId"]["videoId"]
            published_at = snippet["publishedAt"]
            video_id_date_pairs.append((video_id, published_at))

        next_page_token = res.get("nextPageToken")
        if not next_page_token:
            break

    # 2. video IDë¡œ ì¡°íšŒìˆ˜ ê°€ì ¸ì˜¤ê¸°
    for i in range(0, len(video_id_date_pairs), 50):
        batch = video_id_date_pairs[i:i+50]
        ids_only = [vid for vid, _ in batch]

        res = youtube.videos().list(
            part="statistics",
            id=",".join(ids_only)
        ).execute()

        stats = {item["id"]: int(item["statistics"].get("viewCount", 0)) for item in res["items"]}

        # 3. íŠœí”Œë¡œ ì €ì¥: (video_id, published_at, view_count)
        for video_id, published_at in batch:
            view_count = stats.get(video_id, 0)
            video_list.append((video_id, published_at, view_count))

    return video_list

# -----------------------------
# video idsë¡œ ì—…ë¡œë“œ ë‚ ì§œ í•„í„°ë§í•˜ê¸°
# ì…ë ¥: video_datas(type íŠœí”Œ ë¦¬ìŠ¤íŠ¸), ì‹œì‘ë‚ ì§œ, ëë‚ ì§œ ("2001-04-30" í˜•ì‹ìœ¼ë¡œ ì…ë ¥)
# ì¶œë ¥: í•„í„°ë§ ëœ video_datas(type íŠœí”Œ ë¦¬ìŠ¤íŠ¸)
# -----------------------------
def filter_by_date(video_datas, start_date, end_date):
    # ë¬¸ìì—´ â†’ datetime ê°ì²´ë¡œ ë³€í™˜
    start_dt = datetime.strptime(start_date + "T00:00:00Z", "%Y-%m-%dT%H:%M:%SZ")
    end_dt = datetime.strptime(end_date + "T00:00:00Z", "%Y-%m-%dT%H:%M:%SZ")

    filtered = []
    for video_id, published_at, view_count in video_datas:
        pub_date = datetime.strptime(published_at, "%Y-%m-%dT%H:%M:%SZ")
        if start_dt <= pub_date < end_dt:
            filtered.append((video_id, published_at, view_count))

    return filtered
    
# -----------------------------
# ì¡°íšŒìˆ˜ ê¸°ì¤€ í•„í„°ë§
# ì…ë ¥: video_datas(type íŠœí”Œ ë¦¬ìŠ¤íŠ¸), ìµœì†Œì¡°íšŒìˆ˜
# ì¶œë ¥: video_datas(type íŠœí”Œ ë¦¬ìŠ¤íŠ¸)
# -----------------------------
def filter_videos_by_view_count(video_tuples, min_views=0, max_views=float("inf")):
    return [
        (video_id, published_at, view_count)
        for video_id, published_at, view_count in video_tuples
        if min_views <= view_count <= max_views
    ]
    
# -----------------------------
# ì±„ë„ì˜ìƒì„ ë‚ ì§œ, ì¡°íšŒìˆ˜ ê¸°ì¤€ í•„í„°ë§
# ì…ë ¥: channel_id, ê¸°ê°„, ìµœì†Œì¡°íšŒìˆ˜
# ì¶œë ¥: video_datas(type íŠœí”Œ ë¦¬ìŠ¤íŠ¸)
# -----------------------------
def get_filtered_videos_by_channel(channel_id, start_date, end_date, min_views=0):
    video_results = []
    next_page_token = None

    # ë‚ ì§œ ë¬¸ìì—´ â†’ datetime ê°ì²´ â†’ ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    start_iso = (datetime.strptime(start_date, "%Y-%m-%d") - timedelta(days=7)).isoformat("T") + "Z"
    end_iso = datetime.strptime(end_date, "%Y-%m-%d").isoformat("T") + "Z"

    # 1. search().list() â†’ video ID + publishedAt
    temp_videos = []
    while True:
        res = youtube.search().list(
            part="snippet",
            channelId=channel_id,
            publishedAfter=start_iso,
            publishedBefore=end_iso,
            maxResults=50,
            pageToken=next_page_token,
            type="video",
            order="date"
        ).execute()

        for item in res["items"]:
            video_id = item["id"]["videoId"]
            published_at = item["snippet"]["publishedAt"]
            temp_videos.append((video_id, published_at))

        next_page_token = res.get("nextPageToken")
        if not next_page_token:
            break

    # 2. videos().list() â†’ ì¡°íšŒìˆ˜ ë¶™ì´ê¸°
    for i in range(0, len(temp_videos), 50):
        batch = temp_videos[i:i+50]
        ids = [vid for vid, _ in batch]

        res = youtube.videos().list(
            part="statistics",
            id=",".join(ids)
        ).execute()

        stats = {
            item["id"]: int(item["statistics"].get("viewCount", 0))
            for item in res["items"]
        }

        for video_id, published_at in batch:
            view_count = stats.get(video_id, 0)
            if view_count >= min_views:
                video_results.append((video_id, published_at, view_count))

    return video_results

# -----------------------------
# ìœ íŠœë¸Œ ì˜ìƒ ê²€ìƒ‰
# date ì´ì „ì˜ query ê²€ìƒ‰ ê²°ê³¼ë“¤ë§Œ ë³´ì—¬ì¤Œ 
# dateëŠ” '2025-04-11' í˜•ì‹ìœ¼ë¡œ ì…ë ¥
# -----------------------------
def search_videos(query, date):
	date += 'T00:00:00Z' # ISO 8601 í˜•ì‹ìœ¼ë¡œ ë³€ê²½

	# Step 1: ê²€ìƒ‰
	search_res = youtube.search().list(
		q=query,
		part='snippet',
		type='video',
		maxResults=50,
		order='date',
		publishedBefore=date
	).execute()
	
	# Step 2: videoId ìˆ˜ì§‘
	video_ids = [item['id']['videoId'] for item in search_res['items']]
	if not video_ids:
		print("ì¡°ê±´ì— ë§ëŠ” ì˜ìƒ ì—†ìŒ")

	return video_ids

# -----------------------------
# ìœ íŠœë¸Œ ë§í¬ or idì—ì„œ ìŒì„±íŒŒì¼ ì¶”ì¶œ
# INPUT: method(link or id), youtube id or link, ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ê²½ë¡œ
# -----------------------------
def extract_video_audio(method, video_id, audio_dir):
	os.makedirs('./audio', exist_ok=True)
	
	if method == "link":
		url = video_id
	elif method == "id":
		url = "https://www.youtube.com/watch?v=" + video_id
	else:
		print('methodì¸ìë¡œ link or idë¥¼ ì…ë ¥í•˜ì„¸ìš”')
		return False

	ydl_opts = {
		'format': 'bestaudio/best',
		'outtmpl': f'{audio_dir}.%(ext)s',
		'postprocessors': [{
			'key': 'FFmpegExtractAudio',
			'preferredcodec': 'mp3',
		}]
	}

	try:
		with yt_dlp.YoutubeDL(ydl_opts) as ydl:
			ydl.download([url])
		print("ìŒì„±íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì„±ê³µ")
		return True
	except:
		print("ìŒì„±íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
		return False

# -----------------------------
# ìŒì„±íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
# INPUT: youtube id, ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ê²½ë¡œ
# -----------------------------
def audio2text(audio_dir):
	# ìŒì„± íŒŒì¼ STT ìˆ˜í–‰
	result = whisper_model.transcribe(f'{audio_dir}.mp3', language="ko")  # wav, mp4 ë“±ë„ OK

	# í…ìŠ¤íŠ¸ ì¶œë ¥
	print(result["text"])

	return result["text"]

# -----------------------------
# ìë§‰ ì „ì²˜ë¦¬ í•¨ìˆ˜
# -----------------------------
def clean_srt(srt_text: str) -> str:
	srt_text = re.sub(r"(ì¢€|ê·¸ëƒ¥|ë­ë„ê¹Œ|ê·¸ëŸ¬ë‹ˆê¹Œ|ì•„ë‹ˆ|ì•½ê°„|ë­”ê°€|ë­ëƒë©´ìš”)", "", srt_text)
	return srt_text


# ------------------------
# GPT-4o ìš”ì•½
# ------------------------
def summarize_text(text: str) -> str:
	prompt = f"""
The following is a transcript from a YouTube video about economics. Please summarize the content by extracting key economic ideas, arguments, data, and cited examples.  
Your summary should be **concise and well-organized**, capturing the **main points** and **logical structure** of the video.  
A **section-based summary** or **bullet-point outline** is preferred if appropriate.
Transcript: {text}
"""
	response = gpt_model([HumanMessage(content=prompt)])
	return response.content.strip()



# ------------------------
# GPT-4o ë“±ë½ ì˜ˆì¸¡
# ------------------------
def predict_market_from_summary(summary: str, stock: str) -> str:
	prompt = f"""
You are a financial analyst AI.

Based on the following summary of an economics-related YouTube video transcript, assess whether the stock of **{stock}** is likely to go up, down, or remain neutral based on the video's content. 

If the content has **no meaningful connection** to the stock or its industry, clearly state:  
"ì´ ì˜ìƒ ìš”ì•½ë³¸ì€ {stock} ì£¼ê°€ì™€ ê´€ë ¨ì´ ì—†ìŠµë‹ˆë‹¤."

Respond in Korean with a short but insightful analysis.

Summary:
{summary}
"""
	response = gpt_model([HumanMessage(content=prompt)])
	return response.content.strip()


# ------------------------
# ì•ì˜ ëª¨ë“  í•¨ìˆ˜ë¥¼ ì´ìš©í•œ ìµœì¢… í•¨ìˆ˜
#
# ì£¼ì‹ê³¼ ë‚ ì§œ ì…ë ¥í•˜ë©´ ì£¼ê°€ì „ë§ ì˜ˆì¸¡
# dateëŠ” '2025-04-11' í˜•ì‹ìœ¼ë¡œ ì…ë ¥
# ------------------------
def predict_market(stock: str, date: str) -> str:
	SEARCH_QUERY = f'{stock} ì£¼ê°€ì „ë§'
	BEFORE_DATE = date
	print("SEARCH_QUERY:", SEARCH_QUERY)
	video_id = search_videos(SEARCH_QUERY, BEFORE_DATE)
	if video_id == None:
		return "middle"

	# ìœ íŠœë¸Œ ì˜ìƒ ìŒì„± ì¶”ì¶œ
	audio_dir = f'audio/{stock}_{BEFORE_DATE}'
	if not extract_video_audio(video_id, audio_dir):
		return "middle"
	text = audio2text(audio_dir)

	# ìë§‰ ì „ì²˜ë¦¬
	cleaned = clean_srt(text)
	print("ì „ì²˜ë¦¬:\n", cleaned)

	# ì „ì²˜ë¦¬ ìë§‰ ìš”ì•½
	summary = summarize_text(cleaned)

	# ì£¼ì‹ ë“±ë½ ì˜ˆì¸¡
	prediction = predict_market_from_summary(summary, stock)

	print("\nğŸ“„ GEMINI ìš”ì•½ ê²°ê³¼:\n", summary)
	print("\nğŸ“ˆ GPT-4 ì˜ˆì¸¡ ê²°ê³¼:\n\n", prediction)

	if prediction == 'ì˜¤ë¥¼ ê°€ëŠ¥ì„± ìˆìŒ':
		return 'up'
	elif prediction == 'ì˜¤ë¥¼ ê°€ëŠ¥ì„± ë‚®ìŒ':
		return 'down'
	else:
		return 'middle'




# ------------------------
# ëª¨ë“  ë¶„ê¸°ì˜ ë²”ìœ„ êµ¬í•˜ê¸°
# disclosure_date_range.csv íŒŒì¼ ìƒì„±
# ------------------------
def get_disclosure_range():
    # ëª¨ë“  ì¢…ëª©ì˜ ëª¨ë“  ë¶„ê¸° ê³µì‹œì¼ì„ í•˜ë‚˜ì˜ íŒŒì¼ë¡œ
	root_path = Path('./data_kr/merged')
	all_symbols_disclosure = pd.DataFrame()
	for file_path in root_path.rglob("*.csv"):
		df_ = pd.read_csv(file_path)
		df_ = df_[["code", "name", "year", "quarter", "disclosure_date"]]
		all_symbols_disclosure = pd.concat([all_symbols_disclosure, df_])


	years = [2015] + ([y for y in range(2016, 2025) for _ in range(4)])
	quarters = ["Q4"] + ([q for _ in range(2016, 2025) for q in ["Q1", "Q2", "Q3", "Q4"]])
	df_disclosure = pd.DataFrame({
		"year": years,
		"quarter": quarters,
		"min_disclosure_date": [None] * len(years),
		"max_disclosure_date": [None] * len(years)
	})

	for i, row in enumerate(df_disclosure.itertuples()):
		disclosures = all_symbols_disclosure[(all_symbols_disclosure["year"] == row.year) & (all_symbols_disclosure["quarter"] == row.quarter)]["disclosure_date"]
		df_disclosure.loc[i, "min_disclosure_date"] = disclosures.min()
		df_disclosure.loc[i, "max_disclosure_date"] = disclosures.max()

	os.makedirs('./data_kr/audio', exist_ok=True)
	df_disclosure.to_csv("./data_kr/audio/disclosure_date_range.csv", index=False)
 
# ------------------------
# disclosure rangeë¥¼ ë§Œì¡±í•˜ê³  ì¡°íšŒìˆ˜ê°€ min_view_cnt ì´ìƒì¸ video id êµ¬í•˜ê¸°
# channel_name: str
# min_view_cnt: int
# data_kr/audoi/ì— ì—°ë„-ë¶„ê¸°.csv íŒŒì¼ ìƒì„±
# ------------------------
def get_video_datas(channel_name, min_view_cnt):
	channel_id = get_channel_id(channel_name) 
	dir = f'data_kr/audio/{channel_name}'
	os.makedirs(dir, exist_ok=True)
 
	df_disclosure = pd.read_csv('data_kr/audio/disclosure_date_range.csv')
	for row in df_disclosure.itertuples():
		start = datetime.strptime(row.min_disclosure_date, "%Y-%m-%d")
		start -= timedelta(days=7)
		start = start.strftime("%Y-%m-%d")
		end = row.max_disclosure_date

		video_datas = get_filtered_videos_by_channel(channel_id, start, end, min_view_cnt)
		year_quarter = f'{row.year}-{row.quarter}'
		os.makedirs(f'{dir}/{year_quarter}', exist_ok=True)
		pd.DataFrame(video_datas, columns=['video_id', 'published_at', 'view_count']).to_csv(f'{dir}/{year_quarter}/{year_quarter}.csv', index=False)
    
if __name__ == "__main__":    
    # ### ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ###
	# df = pd.read_csv('data_kr/video/ë™ì˜ìƒ ìˆ˜ì§‘ í†µí•©ë³¸.csv')
	# for row in df.itertuples():
	# 	if pd.isna(row.url) or row.url == '':
	# 		continue

	# 	code = str(row.code).zfill(6)
	# 	audio_dir = f'data_kr/video/audio/{row.sector}/{code}/'
	# 	text_dir = f'data_kr/video/text/{row.sector}/{code}/'
	# 	os.makedirs(audio_dir, exist_ok=True)

	# 	if extract_video_audio("link", row.url, audio_dir + f'{row.year}-{row.quarter}'):
	# 		with open('data_kr/video/log.txt', "a", encoding="utf-8") as log_file:
	# 			timestamp = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
	# 			log_file.write(f"{timestamp} audio download completed: {audio_dir + f'{row.year}-{row.quarter}'}\n")
	# 	else:
	# 		with open('data_kr/video/log.txt', "a", encoding="utf-8") as log_file:
	# 			timestamp = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
	# 			log_file.write(f"{timestamp} audio download error: {audio_dir + f'{row.year}-{row.quarter}'}\n")
	
	# ### í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ ###
	# df = pd.read_csv('data_kr/video/ë™ì˜ìƒ ìˆ˜ì§‘ í†µí•©ë³¸.csv')
	# for row in df.itertuples():
	# 	if pd.isna(row.url) or row.url == '':
	# 		continue

	# 	code = str(row.code).zfill(6)
	# 	audio_dir = f'data_kr/video/audio/{row.sector}/{code}/'
	# 	text_dir = f'data_kr/video/text/{row.sector}/{code}/'
	# 	os.makedirs(text_dir, exist_ok=True)
		
	# 	try:
	# 		text = audio2text(audio_dir + f'{row.year}-{row.quarter}')
	# 		with open(text_dir + f'{row.year}-{row.quarter}.txt', "w", encoding="utf-8") as f:
	# 			f.write(text)
	# 		with open('data_kr/video/log.txt', "a", encoding="utf-8") as log_file:
	# 			timestamp = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
	# 			log_file.write(f"{timestamp} whisper completed: {text_dir + f'{row.year}-{row.quarter}'}\n")
	# 	except Exception as e:
	# 		with open('data_kr/video/log.txt', "a", encoding="utf-8") as log_file:
	# 			timestamp = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
	# 			log_file.write(f"{timestamp} whisper error: {text_dir + f'{row.year}-{row.quarter}'}\n")

	# ### í…ìŠ¤íŠ¸ token ìˆ˜ í™•ì¸  ###
	# import tiktoken
	# # ì˜ˆ: GPT-4ìš© ì¸ì½”ë” ë¶ˆëŸ¬ì˜¤ê¸°
	# encoding = tiktoken.encoding_for_model("gpt-4")
	# total_tokens = 0

	# df = pd.read_csv('data_kr/video/ë™ì˜ìƒ ìˆ˜ì§‘ í†µí•©ë³¸.csv')
	# for row in df.itertuples():
	# 	if pd.isna(row.url) or row.url == '':
	# 		continue

	# 	code = str(row.code).zfill(6)
	# 	audio_dir = f'data_kr/video/audio/{row.sector}/{code}/'
	# 	text_dir = f'data_kr/video/text/{row.sector}/{code}/'
	# 	os.makedirs(text_dir, exist_ok=True)
		
	# 	try:
	# 		with open(text_dir + f'{row.year}-{row.quarter}.txt', "r", encoding="utf-8") as file:
	# 			text = file.read()
	# 			tokens = encoding.encode(text)
	# 			total_tokens += len(tokens)
	# 		with open('data_kr/video/num_token.txt', "a", encoding="utf-8") as log_file:
	# 			timestamp = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
	# 			log_file.write(f"{text_dir}{row.year}-{row.quarter} len: {len(text)}, token: {len(tokens)}\n")
	# 	except Exception as e:
	# 		with open('data_kr/video/num_token.txt', "a", encoding="utf-8") as log_file:
	# 			timestamp = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
	# 			log_file.write(f"{text_dir}" + f"{row.year}-{row.quarter} error: " + e + "\n")

	# with open('data_kr/video/num_token.txt', "a", encoding="utf-8") as log_file:
	# 	log_file.write(f"total tokens: {total_tokens}\n")

	# ### LLMìœ¼ë¡œ ì˜ìƒ ìë§‰ ìš”ì•½ ###
	# df = pd.read_csv('data_kr/video/ë™ì˜ìƒ ìˆ˜ì§‘ í†µí•©ë³¸.csv')
	# for row in tqdm(df.itertuples(), total=len(df), desc="LLM summarizing"):
	# 	if pd.isna(row.url) or row.url == '':
	# 		continue

	# 	code = str(row.code).zfill(6)
	# 	text_dir = f'data_kr/video/text/{row.sector}/{code}/'
	# 	summary_dir = f'preprocessed_data/llm/summary/{row.sector}/{code}/'
	# 	os.makedirs(summary_dir, exist_ok=True)

	# 	try:
	# 		with open(text_dir + f'{row.year}-{row.quarter}.txt', "r", encoding="utf-8") as file:
	# 			text = file.read()
	# 		summary = summarize_text(text)
	# 		with open(summary_dir + f'{row.year}-{row.quarter}.txt', "w", encoding="utf-8") as f:
	# 			f.write(summary)
	# 		with open('preprocessed_data/llm/summary/log.txt', "a", encoding="utf-8") as log_file:
	# 			timestamp = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
	# 			log_file.write(f"{timestamp} summary completed: {summary_dir + f'{row.year}-{row.quarter}'}\n")
	# 	except Exception as e:
	# 		with open('preprocessed_data/llm/summary/log.txt', "a", encoding="utf-8") as log_file:
	# 			timestamp = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
	# 			log_file.write(f"{timestamp} summary error: {summary_dir + f'{row.year}-{row.quarter}'}\n")

	### LLMìœ¼ë¡œ ìë§‰ìš”ì•½ì„ í†µí•´ ë“±ë½ ì˜ˆì¸¡ ###
	df = pd.read_csv('data_kr/video/ë™ì˜ìƒ ìˆ˜ì§‘ í†µí•©ë³¸.csv')
	for row in tqdm(df.itertuples(), total=len(df), desc="LLM predicting"):
		if pd.isna(row.url) or row.url == '':
			continue

		code = str(row.code).zfill(6)	
		name = row.name
		summary_dir = f'preprocessed_data/llm/summary/{row.sector}/{code}/'
		predict_dir = f'preprocessed_data/llm/predict/{row.sector}/{code}/'
		os.makedirs(summary_dir, exist_ok=True)

		try:
			with open(summary_dir + f'{row.year}-{row.quarter}.txt', "r", encoding="utf-8") as file:
				summary = file.read()
			predict = predict_market_from_summary(summary, f'{name}({code})')
			with open(predict_dir + f'{row.year}-{row.quarter}.txt', "w", encoding="utf-8") as f:
				f.write(predict)
			with open('preprocessed_data/llm/predict/log.txt', "a", encoding="utf-8") as log_file:
				timestamp = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
				log_file.write(f"{timestamp} predict completed: {predict_dir + f'{row.year}-{row.quarter}'}\n")
		except Exception as e:
			with open('preprocessed_data/llm/predict/log.txt', "a", encoding="utf-8") as log_file:
				timestamp = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
				log_file.write(f"{timestamp} predict error: {predict_dir + f'{row.year}-{row.quarter}'}\n")