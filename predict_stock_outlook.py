from dotenv import load_dotenv
import os
from pathlib import Path
from datetime import datetime, timedelta

from googleapiclient.discovery import build
import yt_dlp
import whisper
import re

import google.generativeai as genai
from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage

import pandas as pd

load_dotenv()  # .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
YOUTUBE_API_KEY = os.getenv("YOUTUBE_API_KEY")

# whisper ëª¨ë¸ ë¡œë“œ (base, small, medium, large ì¤‘ ì„ íƒ ê°€ëŠ¥)
whisper_model = whisper.load_model("small")

# gemini ëª¨ë¸ ë¡œë“œ
genai.configure(api_key=GEMINI_API_KEY)
gemini_model = genai.GenerativeModel("gemini-2.0-flash")

# GPT ëª¨ë¸ ë¡œë“œ
gpt_model = ChatOpenAI(temperature=0, model="gpt-4o", openai_api_key=OPENAI_API_KEY)

# YOUTUBE ë¹Œë“œ
youtube = build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)

# -----------------------------
# ìœ íŠœë¸Œ ì±„ë„ëª…ìœ¼ë¡œ ì±„ë„ID ì¶”ì¶œ
# ì…ë ¥ ì˜ˆ: "í•œêµ­ê²½ì œTV" ë˜ëŠ” "@wowtv"
# ì¶œë ¥: ì±„ë„ ID
# -----------------------------
def get_channel_id(channel_name):
	res = youtube.search().list(
		q=channel_name,
		type="channel",
		part="snippet",
		maxResults=1
	).execute()

	# ê²°ê³¼ì—ì„œ ì±„ë„ ID ì¶”ì¶œ
	if res.get("items"):
		channel_id = res["items"][0]["snippet"]["channelId"]
		print("ğŸ“º ì±„ë„ ID:", channel_id)
	else:
		print("ì±„ë„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

	return channel_id

# -----------------------------
# ì±„ë„ IDë¡œ ì—…ë¡œë“œ playlist ID ì–»ê¸°
# ì…ë ¥: channel id
# ì¶œë ¥: playlist id
# -----------------------------
def get_uploads_playlist_id(channel_id):
    res = youtube.channels().list(
        part="contentDetails",
        id=channel_id
    ).execute()
    return res["items"][0]["contentDetails"]["relatedPlaylists"]["uploads"]

# -----------------------------
# playlist IDë¡œ í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ ë‚´ ëª¨ë“  video IDì™€ ì—…ë¡œë“œ ë‚ ì§œ ì–»ê¸°
# ì…ë ¥: playlist id
# ì¶œë ¥: type: íŠœí”Œ ë¦¬ìŠ¤íŠ¸  ex) [(id1, date1, ì¡°íšŒìˆ˜1), (id2, date2, ì¡°íšŒìˆ˜2) ...]
# -----------------------------
def get_video_datas_from_playlist(playlist_id):
    video_list = []
    next_page_token = None
    video_id_date_pairs = []

    # 1. playlistItems APIë¡œ video ID + ì—…ë¡œë“œ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°
    while True:
        res = youtube.playlistItems().list(
            part="snippet",
            playlistId=playlist_id,
            maxResults=50,
            pageToken=next_page_token
        ).execute()

        for item in res["items"]:
            snippet = item["snippet"]
            video_id = snippet["resourceId"]["videoId"]
            published_at = snippet["publishedAt"]
            video_id_date_pairs.append((video_id, published_at))

        next_page_token = res.get("nextPageToken")
        if not next_page_token:
            break

    # 2. video IDë¡œ ì¡°íšŒìˆ˜ ê°€ì ¸ì˜¤ê¸°
    for i in range(0, len(video_id_date_pairs), 50):
        batch = video_id_date_pairs[i:i+50]
        ids_only = [vid for vid, _ in batch]

        res = youtube.videos().list(
            part="statistics",
            id=",".join(ids_only)
        ).execute()

        stats = {item["id"]: int(item["statistics"].get("viewCount", 0)) for item in res["items"]}

        # 3. íŠœí”Œë¡œ ì €ì¥: (video_id, published_at, view_count)
        for video_id, published_at in batch:
            view_count = stats.get(video_id, 0)
            video_list.append((video_id, published_at, view_count))

    return video_list

# -----------------------------
# video idsë¡œ ì—…ë¡œë“œ ë‚ ì§œ í•„í„°ë§í•˜ê¸°
# ì…ë ¥: video_datas(type íŠœí”Œ ë¦¬ìŠ¤íŠ¸), ì‹œì‘ë‚ ì§œ, ëë‚ ì§œ ("2001-04-30" í˜•ì‹ìœ¼ë¡œ ì…ë ¥)
# ì¶œë ¥: í•„í„°ë§ ëœ video_datas(type íŠœí”Œ ë¦¬ìŠ¤íŠ¸)
# -----------------------------
def filter_by_date(video_datas, start_date, end_date):
    # ë¬¸ìì—´ â†’ datetime ê°ì²´ë¡œ ë³€í™˜
    start_dt = datetime.strptime(start_date + "T00:00:00Z", "%Y-%m-%dT%H:%M:%SZ")
    end_dt = datetime.strptime(end_date + "T00:00:00Z", "%Y-%m-%dT%H:%M:%SZ")

    filtered = []
    for video_id, published_at, view_count in video_datas:
        pub_date = datetime.strptime(published_at, "%Y-%m-%dT%H:%M:%SZ")
        if start_dt <= pub_date < end_dt:
            filtered.append((video_id, published_at, view_count))

    return filtered
    
# -----------------------------
# ì¡°íšŒìˆ˜ ê¸°ì¤€ í•„í„°ë§
# ì…ë ¥: video_datas(type íŠœí”Œ ë¦¬ìŠ¤íŠ¸), ìµœì†Œì¡°íšŒìˆ˜
# ì¶œë ¥: video_datas(type íŠœí”Œ ë¦¬ìŠ¤íŠ¸)
# -----------------------------
def filter_videos_by_view_count(video_tuples, min_views=0, max_views=float("inf")):
    return [
        (video_id, published_at, view_count)
        for video_id, published_at, view_count in video_tuples
        if min_views <= view_count <= max_views
    ]
    
# -----------------------------
# ì±„ë„ì˜ìƒì„ ë‚ ì§œ, ì¡°íšŒìˆ˜ ê¸°ì¤€ í•„í„°ë§
# ì…ë ¥: channel_id, ê¸°ê°„, ìµœì†Œì¡°íšŒìˆ˜
# ì¶œë ¥: video_datas(type íŠœí”Œ ë¦¬ìŠ¤íŠ¸)
# -----------------------------
def get_filtered_videos_by_channel(channel_id, start_date, end_date, min_views=0):
    video_results = []
    next_page_token = None

    # ë‚ ì§œ ë¬¸ìì—´ â†’ datetime ê°ì²´ â†’ ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    start_iso = (datetime.strptime(start_date, "%Y-%m-%d") - timedelta(days=7)).isoformat("T") + "Z"
    end_iso = datetime.strptime(end_date, "%Y-%m-%d").isoformat("T") + "Z"

    # 1. search().list() â†’ video ID + publishedAt
    temp_videos = []
    while True:
        res = youtube.search().list(
            part="snippet",
            channelId=channel_id,
            publishedAfter=start_iso,
            publishedBefore=end_iso,
            maxResults=50,
            pageToken=next_page_token,
            type="video",
            order="date"
        ).execute()

        for item in res["items"]:
            video_id = item["id"]["videoId"]
            published_at = item["snippet"]["publishedAt"]
            temp_videos.append((video_id, published_at))

        next_page_token = res.get("nextPageToken")
        if not next_page_token:
            break

    # 2. videos().list() â†’ ì¡°íšŒìˆ˜ ë¶™ì´ê¸°
    for i in range(0, len(temp_videos), 50):
        batch = temp_videos[i:i+50]
        ids = [vid for vid, _ in batch]

        res = youtube.videos().list(
            part="statistics",
            id=",".join(ids)
        ).execute()

        stats = {
            item["id"]: int(item["statistics"].get("viewCount", 0))
            for item in res["items"]
        }

        for video_id, published_at in batch:
            view_count = stats.get(video_id, 0)
            if view_count >= min_views:
                video_results.append((video_id, published_at, view_count))

    return video_results

# -----------------------------
# ìœ íŠœë¸Œ ì˜ìƒ ê²€ìƒ‰
# date ì´ì „ì˜ query ê²€ìƒ‰ ê²°ê³¼ë“¤ë§Œ ë³´ì—¬ì¤Œ 
# dateëŠ” '2025-04-11' í˜•ì‹ìœ¼ë¡œ ì…ë ¥
# -----------------------------
def search_videos(query, date):
	date += 'T00:00:00Z' # ISO 8601 í˜•ì‹ìœ¼ë¡œ ë³€ê²½

	# Step 1: ê²€ìƒ‰
	search_res = youtube.search().list(
		q=query,
		part='snippet',
		type='video',
		maxResults=50,
		order='date',
		publishedBefore=date
	).execute()
	
	# Step 2: videoId ìˆ˜ì§‘
	video_ids = [item['id']['videoId'] for item in search_res['items']]
	if not video_ids:
		print("ì¡°ê±´ì— ë§ëŠ” ì˜ìƒ ì—†ìŒ")

	return video_ids

# -----------------------------
# ìœ íŠœë¸Œ idì—ì„œ ìŒì„±íŒŒì¼ ì¶”ì¶œ
# INPUT: youtube id, ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ê²½ë¡œ
# -----------------------------
def extract_video_audio(video_id, audio_dir):
	os.makedirs('./audio', exist_ok=True)
	
	url = "https://www.youtube.com/watch?v=" + video_id

	ydl_opts = {
		'format': 'bestaudio/best',
		'outtmpl': f'{audio_dir}.%(ext)s',
		'postprocessors': [{
			'key': 'FFmpegExtractAudio',
			'preferredcodec': 'mp3',
		}]
	}

	try:
		with yt_dlp.YoutubeDL(ydl_opts) as ydl:
			ydl.download([url])
		print("ìŒì„±íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì„±ê³µ")
		return True
	except:
		print("ìŒì„±íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
		return False

# -----------------------------
# ìŒì„±íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
# INPUT: youtube id, ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ê²½ë¡œ
# -----------------------------
def audio2text(audio_dir):
	# ìŒì„± íŒŒì¼ STT ìˆ˜í–‰
	result = whisper_model.transcribe(f'{audio_dir}.mp3')  # wav, mp4 ë“±ë„ OK

	# í…ìŠ¤íŠ¸ ì¶œë ¥
	print(result["text"])

	return result["text"]

# -----------------------------
# ìë§‰ ì „ì²˜ë¦¬ í•¨ìˆ˜
# -----------------------------
def clean_srt(srt_text: str) -> str:
	srt_text = re.sub(r"(ì¢€|ê·¸ëƒ¥|ë­ë„ê¹Œ|ê·¸ëŸ¬ë‹ˆê¹Œ|ì•„ë‹ˆ|ì•½ê°„|ë­”ê°€|ë­ëƒë©´ìš”)", "", srt_text)
	return srt_text


# ------------------------
# Gemini ìš”ì•½
# ------------------------
def summarize_with_gemini(text: str) -> str:
	prompt = f"""
ë‹¤ìŒì€ ê²½ì œ ê´€ë ¨ ì˜ìƒ ìë§‰ì…ë‹ˆë‹¤.
í•µì‹¬ ë‚´ìš© ìœ„ì£¼ë¡œ ê°„ê²°í•˜ê²Œ ë‰´ìŠ¤ ìŠ¤íƒ€ì¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:

{text}
"""
	response = gemini_model.generate_content(prompt)
	return response.text.strip()



# ------------------------
# GPT-4 ë“±ë½ ì˜ˆì¸¡
# ------------------------
def predict_market_from_summary(summary: str, stock: str) -> str:
	prompt = f"""
ì•„ë˜ëŠ” ê²½ì œ ë‰´ìŠ¤ì˜ ìš”ì•½ì…ë‹ˆë‹¤.

"{summary}"

ì´ ë‰´ìŠ¤ì˜ ë‚´ìš©ì´ ì£¼ì‹ ì¢…ëª© "{stock}"ì— ê¸ì •ì ì¸ ì˜í–¥ì„ ë¯¸ì¹  ê°€ëŠ¥ì„±ì´ ìˆì„ê¹Œìš”? ê·¸ë ‡ë‹¤ë©´ 'ì˜¤ë¥¼ ê°€ëŠ¥ì„± ìˆìŒ', ì•„ë‹ˆë¼ë©´ 'ì˜¤ë¥¼ ê°€ëŠ¥ì„± ë‚®ìŒ'ì´ë¼ê³ ë§Œ ë‹µí•´ ì£¼ì„¸ìš”.
"""
	response = gpt_model([HumanMessage(content=prompt)])
	return response.content.strip()


# ------------------------
# ì•ì˜ ëª¨ë“  í•¨ìˆ˜ë¥¼ ì´ìš©í•œ ìµœì¢… í•¨ìˆ˜
#
# ì£¼ì‹ê³¼ ë‚ ì§œ ì…ë ¥í•˜ë©´ ì£¼ê°€ì „ë§ ì˜ˆì¸¡
# dateëŠ” '2025-04-11' í˜•ì‹ìœ¼ë¡œ ì…ë ¥
# ------------------------
def predict_market(stock: str, date: str) -> str:
	SEARCH_QUERY = f'{stock} ì£¼ê°€ì „ë§'
	BEFORE_DATE = date
	print("SEARCH_QUERY:", SEARCH_QUERY)
	video_id = search_videos(SEARCH_QUERY, BEFORE_DATE)
	if video_id == None:
		return "middle"

	# ìœ íŠœë¸Œ ì˜ìƒ ìŒì„± ì¶”ì¶œ
	audio_dir = f'audio/{stock}_{BEFORE_DATE}'
	if not extract_video_audio(video_id, audio_dir):
		return "middle"
	text = audio2text(audio_dir)

	# ìë§‰ ì „ì²˜ë¦¬
	cleaned = clean_srt(text)
	print("ì „ì²˜ë¦¬:\n", cleaned)

	# ì „ì²˜ë¦¬ ìë§‰ ìš”ì•½
	summary = summarize_with_gemini(cleaned)

	# ì£¼ì‹ ë“±ë½ ì˜ˆì¸¡
	prediction = predict_market_from_summary(summary, stock)

	print("\nğŸ“„ GEMINI ìš”ì•½ ê²°ê³¼:\n", summary)
	print("\nğŸ“ˆ GPT-4 ì˜ˆì¸¡ ê²°ê³¼:\n\n", prediction)

	if prediction == 'ì˜¤ë¥¼ ê°€ëŠ¥ì„± ìˆìŒ':
		return 'up'
	elif prediction == 'ì˜¤ë¥¼ ê°€ëŠ¥ì„± ë‚®ìŒ':
		return 'down'
	else:
		return 'middle'




# ------------------------
# ëª¨ë“  ë¶„ê¸°ì˜ ë²”ìœ„ êµ¬í•˜ê¸°
# disclosure_date_range.csv íŒŒì¼ ìƒì„±
# ------------------------
def get_disclosure_range():
    # ëª¨ë“  ì¢…ëª©ì˜ ëª¨ë“  ë¶„ê¸° ê³µì‹œì¼ì„ í•˜ë‚˜ì˜ íŒŒì¼ë¡œ
	root_path = Path('./data_kr/merged')
	all_symbols_disclosure = pd.DataFrame()
	for file_path in root_path.rglob("*.csv"):
		df_ = pd.read_csv(file_path)
		df_ = df_[["code", "name", "year", "quarter", "disclosure_date"]]
		all_symbols_disclosure = pd.concat([all_symbols_disclosure, df_])


	years = [2015] + ([y for y in range(2016, 2025) for _ in range(4)])
	quarters = ["Q4"] + ([q for _ in range(2016, 2025) for q in ["Q1", "Q2", "Q3", "Q4"]])
	df_disclosure = pd.DataFrame({
		"year": years,
		"quarter": quarters,
		"min_disclosure_date": [None] * len(years),
		"max_disclosure_date": [None] * len(years)
	})

	for i, row in enumerate(df_disclosure.itertuples()):
		disclosures = all_symbols_disclosure[(all_symbols_disclosure["year"] == row.year) & (all_symbols_disclosure["quarter"] == row.quarter)]["disclosure_date"]
		df_disclosure.loc[i, "min_disclosure_date"] = disclosures.min()
		df_disclosure.loc[i, "max_disclosure_date"] = disclosures.max()

	os.makedirs('./data_kr/audio', exist_ok=True)
	df_disclosure.to_csv("./data_kr/audio/disclosure_date_range.csv", index=False)
 
# ------------------------
# disclosure rangeë¥¼ ë§Œì¡±í•˜ê³  ì¡°íšŒìˆ˜ê°€ min_view_cnt ì´ìƒì¸ video id êµ¬í•˜ê¸°
# channel_name: str
# min_view_cnt: int
# data_kr/audoi/ì— ì—°ë„-ë¶„ê¸°.csv íŒŒì¼ ìƒì„±
# ------------------------
def get_video_datas(channel_name, min_view_cnt):
	channel_id = get_channel_id(channel_name) 
	dir = f'data_kr/audio/{channel_name}'
	os.makedirs(dir, exist_ok=True)
 
	df_disclosure = pd.read_csv('data_kr/audio/disclosure_date_range.csv')
	for row in df_disclosure.itertuples():
		start = datetime.strptime(row.min_disclosure_date, "%Y-%m-%d")
		start -= timedelta(days=7)
		start = start.strftime("%Y-%m-%d")
		end = row.max_disclosure_date

		video_datas = get_filtered_videos_by_channel(channel_id, start, end, min_view_cnt)
		year_quarter = f'{row.year}-{row.quarter}'
		os.makedirs(f'{dir}/{year_quarter}', exist_ok=True)
		pd.DataFrame(video_datas, columns=['video_id', 'published_at', 'view_count']).to_csv(f'{dir}/{year_quarter}/{year_quarter}.csv', index=False)
    
if __name__ == "__main__":
	# get_disclosure_range()
 
	get_video_datas("í•œêµ­ê²½ì œTV", 1000)